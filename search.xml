<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>how to use blog</title>
    <url>/2022/05/09/blog-use/blog-use1/</url>
    <content><![CDATA[<h2 id="how-to-use-blog"><a href="#how-to-use-blog" class="headerlink" title="how to use blog"></a>how to use blog</h2><ol>
<li><p>在<em>source_posts</em>下新建一个md文档（可以使用模板功能但是要在外面用hexo新建？）</p>
</li>
<li><p>编辑好md后，在根目录下执行hexo clean -&gt; hexo g-&gt;hexo d</p>
</li>
</ol>
<hr>
<h2 id="md文档常用的格式"><a href="#md文档常用的格式" class="headerlink" title="md文档常用的格式"></a>md文档常用的格式</h2><ol>
<li><p>标题的使用<br>‘#’代表一级标题<br>‘##’代表二级标题<br>etc.</p>
</li>
<li><p>斜体文字的使用<br><em>斜体文本</em><br><strong>粗体文本</strong><br><strong><em>粗斜体文本</em></strong></p>
</li>
<li><p>‘<em>*</em>‘代表分割线  </p>
</li>
<li><p>带下划线文字<br><u>带下划线文本</u></p>
</li>
<li><p>列表嵌套</p>
<ol>
<li>在字列表前加四个空格</li>
</ol>
<ul>
<li>like this</li>
<li>or this</li>
</ul>
</li>
<li><blockquote>
<p>区块的使用</p>
</blockquote>
</li>
<li><p>markdown 代码</p>
</li>
</ol>
<p><code>printf()</code>函数  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> a:</span><br><span class="line">    <span class="built_in">print</span>(a) </span><br><span class="line"><span class="keyword">else</span>: </span><br><span class="line">    <span class="built_in">print</span>(b)</span><br></pre></td></tr></table></figure>
<ol>
<li>链接的使用</li>
</ol>
<blockquote>
<p><a href="链接地址">链接名称</a><br>或者<br>&lt;链接地址&gt;</p>
</blockquote>
<p>高级使用</p>
<blockquote>
<p>这个链接用 1 作为网址变量 <a href="http://www.google.com/">Google</a><br>这个链接用 runoob 作为网址变量 <a href="http://www.runoob.com/">Runoob</a><br>然后在文档的结尾为变量赋值（网址）</p>
</blockquote>
<ol>
<li>picture<br><img src="图片地址" alt="alt 属性文本"><br><img src="图片地址" alt="alt 属性文本" title="可选标题"><ul>
<li>开头一个感叹号 !</li>
<li>接着一个方括号，里面放上图片的替代文字</li>
<li>接着一个普通括号，里面放上图片的网址，最后还可以用引号包住并加上选择性的 ‘title’ 属性的文字。</li>
</ul>
</li>
</ol>
<p><strong>如果想改变图片的大小和位置要用css格式。like:</strong></p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;https://pic.imgdb.cn/item/627935a7094754312920d6d2.png&quot;</span> <span class="attr">width</span> = <span class="string">&quot;100%&quot;</span> <span class="attr">height</span> = <span class="string">&quot;25%&quot;</span> <span class="attr">alt</span>=<span class="string">&quot;图片名称&quot;</span> <span class="attr">align</span>=<span class="string">center</span> /&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">div</span> <span class="attr">align</span>=<span class="string">center</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">img</span> <span class="attr">src</span>=<span class="string">&quot;https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/whale_pytorch.jpg&quot;</span> <span class="attr">width</span>=<span class="string">&quot;250&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">div</span>&gt;</span></span><br></pre></td></tr></table></figure>
<ol>
<li>table<br>Markdown 制作表格使用 | 来分隔不同的单元格，使用 - 来分隔表头和其他行。</li>
</ol>
<p>语法格式如下：</p>
<blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>表头</th>
<th>表头</th>
</tr>
</thead>
<tbody>
<tr>
<td>单元格</td>
<td>单元格</td>
</tr>
<tr>
<td>单元格</td>
<td>单元格</td>
</tr>
</tbody>
</table>
</div>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">表头</th>
<th style="text-align:right">表头</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">单元格</td>
<td style="text-align:right">单元格</td>
</tr>
<tr>
<td style="text-align:left">单元格</td>
<td style="text-align:right">单元格</td>
</tr>
</tbody>
</table>
</div>
]]></content>
      <categories>
        <category>blog use</category>
      </categories>
      <tags>
        <tag>blog-use</tag>
      </tags>
  </entry>
  <entry>
    <title>how to create blog</title>
    <url>/2024/06/06/blog-use/blog-use2/</url>
    <content><![CDATA[<h2 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h2><h3 id="1-创建一篇新文章或新页面"><a href="#1-创建一篇新文章或新页面" class="headerlink" title="1.创建一篇新文章或新页面"></a>1.创建一篇新文章或新页面</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new [layout] &lt;title&gt;  </span><br></pre></td></tr></table></figure>
<h3 id="2-创建标签页"><a href="#2-创建标签页" class="headerlink" title="2.创建标签页"></a>2.创建标签页</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new page tags</span><br></pre></td></tr></table></figure>
<h3 id="3-创建分类页"><a href="#3-创建分类页" class="headerlink" title="3.创建分类页"></a>3.创建分类页</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hexo new page categories</span><br></pre></td></tr></table></figure>
<h3 id="4-通过npm安装包"><a href="#4-通过npm安装包" class="headerlink" title="4.通过npm安装包"></a>4.通过npm安装包</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">npm or cnpm install</span><br></pre></td></tr></table></figure>
<h2 id="一些使用时可能遇到的问题"><a href="#一些使用时可能遇到的问题" class="headerlink" title="一些使用时可能遇到的问题"></a>一些使用时可能遇到的问题</h2><h3 id="重新配置ssh公钥"><a href="#重新配置ssh公钥" class="headerlink" title="重新配置ssh公钥"></a>重新配置ssh公钥</h3><p>先在git上删除原本的公钥 -&gt;在bash窗口执行<code>ssh-keygen -t rsa -C &quot;邮箱地址”</code> -&gt;复制生存的公钥在git上重新添加 -&gt;<code>ssh -T git@github.com (验证是否成功)</code></p>
<h3 id="next主题更新方法"><a href="#next主题更新方法" class="headerlink" title="next主题更新方法"></a>next主题更新方法</h3><p> 1.在D:\新桌面\blog\blog\themes\next 运行<code>git pull</code><br> 2.（若提示commit your changes… ）运行git add (配置文件名)-&gt;<code>git commit -m My message</code></p>
]]></content>
      <categories>
        <category>blog use</category>
      </categories>
      <tags>
        <tag>blog-use</tag>
      </tags>
  </entry>
  <entry>
    <title>new-study-note1</title>
    <url>/2023/10/31/new-study-note/new-study-note1/</url>
    <content><![CDATA[<h2 id="type、dtype和astype的区别。"><a href="#type、dtype和astype的区别。" class="headerlink" title="type、dtype和astype的区别。"></a>type、dtype和astype的区别。</h2><p>1、type</p>
<p>返回的是数据结构类型，如list、dict、numpy.ndarray、pandas.core.frame.DataFrame等。</p>
<p>2、dtype</p>
<p>返回的是数据元素类型，如int、str、float等。</p>
<p>由于list、dict等可以包含不同的数据元素类型，因此不可调用dtype函数，np.array中要求所有元素都属于同一数据类型，因此可以调用dtype函数。</p>
<p>3、astype</p>
<p>改变数据元素类型如：float64 —&gt; int32，会将小数部分截断；string_ —&gt; float64 如果字符串数组表示的全是数字，也可以用astype转化为数值类型；如果字符串数组里不是以数字存储，则不能转换。</p>
<h2 id="用skleran-preprocessing进行数据预处理"><a href="#用skleran-preprocessing进行数据预处理" class="headerlink" title="用skleran.preprocessing进行数据预处理"></a>用skleran.preprocessing进行数据预处理</h2><p>sklearn.preprocessing中含有常用的标准化函数可对DF使用，例如preprocessing.MinMaxScaler()<br>正则化是将样本在向量空间模型上的一个转换，经常被使用在分类与聚类中。 preprocessing.Normalizer()<br>为类别特征编码。preprocessing.OneHotEncoder()<br>标准化也叫z-score。preprocessing.StandardScaler()</p>
<p><strong>使用案例</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">input_features = preprocessing.StandardScaler().fit_transform(features)</span><br><span class="line"></span><br><span class="line"><span class="keyword">or</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler                <span class="comment">#数据归一化</span></span><br><span class="line">data=[</span><br><span class="line">    [-<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line">    ,[-<span class="number">0.5</span>,<span class="number">6</span>]</span><br><span class="line">    ,[<span class="number">0</span>,<span class="number">10</span>]</span><br><span class="line">    ,[<span class="number">1</span>,<span class="number">18</span>]</span><br><span class="line">]</span><br><span class="line">scaler=MinMaxScaler(feature_range=[<span class="number">5</span>,<span class="number">10</span>])                     <span class="comment">#实例化（feature_range为范围）</span></span><br><span class="line">scaler_fit=scaler.fit(data)                                   <span class="comment">#模型，如果fit报错特征值过多无法计算可用partial_fit</span></span><br><span class="line">result=scaler_fit.transform(data)                             <span class="comment">#输出接口</span></span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(scaler.fit_transform(data))                             <span class="comment">#fit和transform的综合，等于将2和3步合并</span></span><br><span class="line"><span class="built_in">print</span>(scaler.inverse_transform(result))                       <span class="comment">#逆转归一化</span></span><br></pre></td></tr></table></figure>
<h2 id="exponential-smoothing"><a href="#exponential-smoothing" class="headerlink" title="exponential-smoothing"></a>exponential-smoothing</h2><p><strong>基本原理</strong>：指数平滑法是移动平均法中的一种，其特点在于给过去的观测值不一样的权重，即较近期观测值的权数比较远期观测值的权数要大。根据平滑次数不同，指数平滑法分为一次指数平滑法、二次指数平滑法和三次指数平滑法等。但它们的基本思想都是：预测值是以前观测值的加权和，且对不同的数据给予不同的权数，新数据给予较大的权数，旧数据给予较小的权数。</p>
<p><strong>计算公式</strong></p>
<div align=center>
<img src=https://pic.imgdb.cn/item/656571bec458853aefc587b0.png >
</div>

<p><strong>对于DF数据的快速实现</strong><br>NDFrame.ewm()快速计算指数加权移动平均数（Exponential Weighted Moving Average），用于计算时间序列数据的平均值。</p>
<blockquote>
<p>DataFrame.ewm(self, com=None, span=None, halflife=None, alpha=None, min_periods=0, adjust=True, ignore_na=False, axis=0)</p>
</blockquote>
<p><strong>com</strong>：根据质心指定衰减， α=1/(1+com), for com≥0</p>
<p><strong>span</strong> ：根据范围指定衰减， α=2/(span+1), for span≥1</p>
<p><strong>halflife</strong> ：根据半衰期指定衰减， α=1−exp(log(0.5)/halflife),forhalflife&gt;0</p>
<p><strong>alpha</strong>：直接指定平滑系数α， 0&lt;α≤1。</p>
<p><strong>min_periods</strong> ：窗口中具有值的最小观察数</p>
<p>用.mean()进行调用</p>
<p><strong>示例</strong><br><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="built_in">input</span> :</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;B&#x27;</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, np.nan, <span class="number">4</span>]&#125;)</span><br><span class="line"><span class="built_in">print</span>(df)</span><br><span class="line"><span class="built_in">print</span>(df.ewm(span=<span class="number">2</span>).mean())</span><br><span class="line"></span><br><span class="line">output:</span><br><span class="line">     B</span><br><span class="line"><span class="number">0</span>  <span class="number">0.0</span></span><br><span class="line"><span class="number">1</span>  <span class="number">1.0</span></span><br><span class="line"><span class="number">2</span>  <span class="number">2.0</span></span><br><span class="line"><span class="number">3</span>  NaN</span><br><span class="line"><span class="number">4</span>  <span class="number">4.0</span></span><br><span class="line">          B</span><br><span class="line"><span class="number">0</span>  <span class="number">0.000000</span></span><br><span class="line"><span class="number">1</span>  <span class="number">0.750000</span></span><br><span class="line"><span class="number">2</span>  <span class="number">1.615385</span></span><br><span class="line"><span class="number">3</span>  <span class="number">1.615385</span></span><br><span class="line"><span class="number">4</span>  <span class="number">3.670213</span></span><br></pre></td></tr></table></figure></p>
<h2 id="DF中的groupby"><a href="#DF中的groupby" class="headerlink" title="DF中的groupby"></a>DF中的groupby</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;key1&#x27;</span> : [<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;a&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;key2&#x27;</span> : [<span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;one&#x27;</span>, <span class="string">&#x27;two&#x27;</span>, <span class="string">&#x27;one&#x27;</span>],</span><br><span class="line">    <span class="string">&#x27;data1&#x27;</span> : np.random.randn(<span class="number">5</span>),</span><br><span class="line">    <span class="string">&#x27;data2&#x27;</span> : np.random.randn(<span class="number">5</span>)&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用groupby对DF按某一列分组DF.groupby(&#x27;keyname&#x27;),返回DataFrameGroupBy，可用list(grouped)进行查看</span></span><br><span class="line">grouped = df[<span class="string">&#x27;data1&#x27;</span>].groupby(df[<span class="string">&#x27;key1&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#GroupBy对象支持迭代操作，会产生一个由分组变量名和数据块组成的二元元组：</span></span><br><span class="line"><span class="keyword">for</span> (k1,k2), group <span class="keyword">in</span> df.groupby([<span class="string">&#x27;key1&#x27;</span>,<span class="string">&#x27;key2&#x27;</span>]):</span><br><span class="line">    <span class="built_in">print</span> k1,k2</span><br><span class="line">    <span class="built_in">print</span> group</span><br><span class="line"></span><br><span class="line"><span class="comment">#分组后常用操作包括：求和(sum)、平均值(mean)、计数(count)等等</span></span><br><span class="line">grouped.mean()</span><br><span class="line">df.groupby(<span class="string">&#x27;key1&#x27;</span>)[<span class="string">&#x27;data1&#x27;</span>].agg([<span class="string">&#x27;sum&#x27;</span>,<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;std&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#group后进行apply操作，在每一个分组内进行操作</span></span><br><span class="line">df.groupby(<span class="string">&#x27;key1&#x27;</span>)[<span class="string">&#x27;data1&#x27;</span>].apply(<span class="keyword">lambda</span> x: x.ewm(alpha=alpha).mean())</span><br><span class="line"></span><br><span class="line"><span class="comment">#把非标准形式转化为标准的 DataFrame 形式，利用的方法就是重置索引reset_index()</span></span><br><span class="line">df.groupby(<span class="string">&#x27;key1&#x27;</span>)[<span class="string">&#x27;data1&#x27;</span>].reset_index(level=<span class="number">0</span>, drop=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>study notes</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title>new-study-note3</title>
    <url>/2024/06/06/new-study-note/new-study-note3/</url>
    <content><![CDATA[<h2 id="tensorboard的使用"><a href="#tensorboard的使用" class="headerlink" title="tensorboard的使用"></a>tensorboard的使用</h2><h3 id="一、创建一个writer实例"><a href="#一、创建一个writer实例" class="headerlink" title="一、创建一个writer实例"></a>一、创建一个writer实例</h3><p>tensorboard的所有记录操作都是实例化了SummaryWriter这个对象，首先需要创建一个writer实例，有如下几种创建方式，如果创建writer时没有参数，默认文件在runs目录下，默认的子文件夹名字为该文件的创建日期，当然也可以手动设定子文件夹的名字</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> SummaryWriter </span><br><span class="line"></span><br><span class="line"><span class="comment"># SummaryWriter encapsulates everything </span></span><br><span class="line">writer = SummaryWriter()</span><br></pre></td></tr></table></figure>
<h3 id="二、添加数据形式"><a href="#二、添加数据形式" class="headerlink" title="二、添加数据形式"></a>二、添加数据形式</h3><ol>
<li>记录标量<br>记录一个标量<br><code>add_scalar(tag, scalar_value, global_step, walltime)</code></li>
</ol>
<ul>
<li>tag：数据标签</li>
<li>scalar_value：数据的值，即y轴的值</li>
<li>global_step：全局步数，即x轴的值</li>
<li>walltime：记录event的时间，可选</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">x = <span class="built_in">range</span>(<span class="number">100</span>) </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> x:     </span><br><span class="line">    time.sleep(<span class="number">0.1</span>)</span><br><span class="line">    writer.add_scalar(<span class="string">&#x27;y=2x&#x27;</span>, i * <span class="number">2</span>, i, walltime=time.time()) </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<p>记录多个标量<br><code>add_scalars(main_tag, scalar_dict, global_step, walltime)</code></p>
<ul>
<li>main_tag：数据标签</li>
<li>tag_scalar_dict：数据的值，即y轴的值</li>
<li>global_step：全局步数，即x轴的值</li>
<li>walltime：记录event的时间，可选</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorboardX <span class="keyword">import</span> </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">SummaryWriter writer = SummaryWriter() </span><br><span class="line">r = <span class="number">5</span> </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):     </span><br><span class="line">    writer.add_scalars(<span class="string">&#x27;run_14h&#x27;</span>, </span><br><span class="line">    				  &#123;<span class="string">&#x27;xsinx&#x27;</span>:i*np.sin(i/r),                                                                               </span><br><span class="line">					   <span class="string">&#x27;xcosx&#x27;</span>:i*np.cos(i/r),</span><br><span class="line">                       <span class="string">&#x27;tanx&#x27;</span>: np.tan(i/r)&#125;, </span><br><span class="line">                       i) </span><br><span class="line">writer.close() </span><br><span class="line"><span class="comment"># This call adds three values to the same scalar plot with the tag</span></span><br></pre></td></tr></table></figure>
<h3 id="三、运行tensorboard"><a href="#三、运行tensorboard" class="headerlink" title="三、运行tensorboard"></a>三、运行tensorboard</h3><p>当程序运行完成后会自动生成tensorboard文件和目录，在cmd终端使用<code>tensorboard --logdir=&lt;your_log_dir&gt;</code>运行日志进行可视化了</p>
<h2 id="Additive-Attention-和-Dot-product-Attention"><a href="#Additive-Attention-和-Dot-product-Attention" class="headerlink" title="Additive Attention 和 Dot-product Attention"></a>Additive Attention 和 Dot-product Attention</h2><p>additive attention 和 dot-product attention 是最常用的两种attention函数，都是用于在attention中计算两个向量之间的相关度，下面对这两个function进行简单的比较整理。</p>
<p><strong>计算原理</strong><br><code>additive attention</code>使用了一个有一个隐层的前馈神经网络，输入层是两个向量的横向拼接，输出层的激活函数是sigmoid表示二者的相关度，对每一对向量都需要进行一次计算，得到的结果再计算softmax得到attention相关度权重。<br>举个栗子<br>计算向量Q与向量K1，K2，… ，Kn的attention权重，先拼接Q-K1计算，再拼接Q-K2计算，…，直到所有的都计算出一个结果再套softmax。</p>
<p><code>dot-product attention</code>一般用矩阵运算，Q K V 分别是三个矩阵，均表示一组向量，dot-product attention想做的是如何用V中的向量表示Q，Q一般指的是要表示的目标，K要和Q建立联系，计算相关性，以计算出的相关性为权重，加权叠加矩阵V中的向量。下图是Transformer中用的dot-product attention，根号dk作用是缩放，一般的dot-product attention可以不用缩放。</p>
<p>这里Q和K的都是由维度为dk的向量组成的，V的维度为dv，最终生成的Q‘也是由维度为dv的向量组成的。<br>用同一个栗子，这里的矩阵Q 就是 1 × dk，K是 n × dk，V是 n × dv。</p>
<h2 id="argparse-ArgumentParser-用法解析"><a href="#argparse-ArgumentParser-用法解析" class="headerlink" title="argparse.ArgumentParser()用法解析"></a>argparse.ArgumentParser()用法解析</h2><ol>
<li>import argparse ；首先导入该模块</li>
<li>parser = argparse.ArgumentParser()；创建一个解析对象</li>
<li>parser.add_argument()；然后向该对象中添加要关注的命令行参数和选项，每一个add_argument方法对应一个要关注的参数或选项；</li>
<li>parser.parse_args()；调用parse_args()方法进行解析，解析成功之后即可使用。</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="comment">#设置可选参数（常用）</span></span><br><span class="line">parser = argparse.ArgumentParser(description=<span class="string">&#x27;姓名&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--family&#x27;</span>, defalut=<span class="string">&#x27;chen&#x27;</span>,<span class="built_in">type</span>=<span class="built_in">str</span>,<span class="built_in">help</span>=<span class="string">&#x27;姓&#x27;</span>)</span><br><span class="line">parser.add_argument(<span class="string">&#x27;--name&#x27;</span>, <span class="built_in">type</span>=<span class="built_in">str</span>,<span class="built_in">help</span>=<span class="string">&#x27;名&#x27;</span>)</span><br><span class="line">args = parser.parse_args()</span><br><span class="line"></span><br><span class="line"><span class="comment">#使用args.参数名即可调用对应参数</span></span><br><span class="line"><span class="built_in">print</span>(args.family)</span><br></pre></td></tr></table></figure>
<h2 id="Python-pandas-DataFrame-ewm函数方法的使用"><a href="#Python-pandas-DataFrame-ewm函数方法的使用" class="headerlink" title="Python pandas.DataFrame.ewm函数方法的使用"></a>Python pandas.DataFrame.ewm函数方法的使用</h2><p><code>DataFrame.ewm(self, com=None, span=None, halflife=None, alpha=None, min_periods=0, adjust=True, ignore_na=False, axis=0)</code><br><code>Returns:pandas.api.typing.ExponentialMovingWindow</code></p>
<ul>
<li>com ：  float，可选<br>根据质心指定衰减， α=1/(1+com), for com≥0。</li>
<li>span ：  float，可选<br>根据范围指定衰减， α=2/(span+1), for span≥1。</li>
<li>halflife ：  float，可选<br>根据半衰期指定衰减， α=1−exp(log(0.5)/halflife),forhalflife&gt;0。</li>
<li>alpha ：  float，可选<br>直接指定平滑系数α， 0&lt;α≤1。</li>
<li>min_periods ： int，默认0<br>窗口中具有值的最小观察数（否则结果为NA）。</li>
<li>adjust ： bool，默认为True<br>除以开始阶段的衰减调整因子，以解释相对权重的不平衡(将EWMA视为移动平均线)。</li>
<li>ignore_na ： bool，默认为False<br>计算权重时忽略缺失值；指定True可重现0.15.0之前的行为。</li>
<li>axis ： {0或’index’，1或’columns’}，默认0<br>要使用的轴。值0标识行，值1标识列。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(&#123;<span class="string">&#x27;B&#x27;</span>: [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, np.nan, <span class="number">4</span>]&#125;)</span><br><span class="line"></span><br><span class="line">df.ewm(com=<span class="number">1</span>).mean()</span><br></pre></td></tr></table></figure>
<h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h2><p>Additive Attention 和 Dot-product Attention<br><a href="https://www.zhihu.com/tardis/bd/art/366993073?source_id=1001">https://www.zhihu.com/tardis/bd/art/366993073?source_id=1001</a></p>
]]></content>
      <categories>
        <category>study notes</category>
      </categories>
  </entry>
  <entry>
    <title>GCN</title>
    <url>/2024/06/06/new-study-note/new-study-note2/</url>
    <content><![CDATA[<h2 id="什么是图"><a href="#什么是图" class="headerlink" title="什么是图"></a>什么是图</h2><p>A graph represents the relations (edges) between a collection of entities (nodes).</p>
<p>G=(V,E,U)<br>Vertex (or node) attributes<br>Edge (or link) attributes and directions<br>Global (or master node) attributes</p>
<h2 id="GCN"><a href="#GCN" class="headerlink" title="GCN"></a>GCN</h2><p>图卷积网络的本质就是提取图结构的空间特征，基于提取方式的不同可以分为：基于空间域的图网络（GraphSAGE，GAT，MPNN等）、基于谱域的图网络（Spectral CNN、ChebyNet、GCN等）。</p>
<blockquote>
<p>①基于空间的方法就是直接从图结构出发，聚合邻居节点的信息作为新的特征，不断的进行消息传递的过程。<br>②基于谱域的方法就是将原始数据转换至谱域中，利用图谱理论，引入滤波器进行滤波，在转换回时域的一个过程。</p>
</blockquote>
<h3 id="从空间域理解"><a href="#从空间域理解" class="headerlink" title="从空间域理解"></a>从空间域理解</h3><p>图网络的计算就是不断考虑邻居及自身信息的一个迭代过程，每进行一次迭代就是一次特征重组，下一层的特征为上一层特征的图卷积。<br>对于图网络的特征矩阵X</p>
<script type="math/tex; mode=display">
X^{l+1}=\widetilde{D}^{-1/2}\widetilde{A}\widetilde{D}^{-1/2}X^l</script><p>其中</p>
<script type="math/tex; mode=display">
考虑自身信息的邻接矩阵\widetilde{A}=A+I
\\D为度矩阵\widetilde{D}=D+I</script><p>度矩阵就是与该节点相邻节点的数据，因此就是邻接矩阵A每一行的求和组成的对角阵，</p>
<div align=center>
<img src=https://pic.imgdb.cn/item/656571bfc458853aefc588ef.png >
</div>

<div align=center>
<img src= https://pic.imgdb.cn/item/656571bfc458853aefc58c48.png>
</div>

<div align=center>
<img src= https://pic.imgdb.cn/item/656571bec458853aefc58738.png>
</div>

<p>基础的GCN信息传递的权重是通过度矩阵也既与节点相连的节点数得来的，GCN的权重还可以通过注意力机制获得称为GAT</p>
<div align=center>
<img src=https://pic.imgdb.cn/item/656571bfc458853aefc58a8a.png >
</div>

<h3 id="从频域理解"><a href="#从频域理解" class="headerlink" title="从频域理解"></a>从频域理解</h3><p>todo</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><h3 id="torch-geometric库用来构造GNN"><a href="#torch-geometric库用来构造GNN" class="headerlink" title="torch_geometric库用来构造GNN"></a>torch_geometric库用来构造GNN</h3><h4 id="实例化一个图"><a href="#实例化一个图" class="headerlink" title="实例化一个图"></a>实例化一个图</h4><p>图一般被用来建模和描述目标（节点）间成对的关系（边）。在Pytorch Gemometric（以后均简称pyg）中，一个图是由<em>torch_geometric.data.Data</em>的一个实例来描述的，<br>设此图有N个节点，每个节点有n个特征，M条边，每条边有m个特征，默认情况下拥有如下的属性：</p>
<p><strong>data.x</strong>: 节点的特征矩阵，形状：[N，n]<br><strong>data.edge_index</strong>: 用COO格式储存的图数据，形状：[2,M]（如不理解没事，后面我会在栗子中详细介绍），数据类型是torch.long<br>ex:[[0,1,2,3],[1,1,2,2]] 0,1,2,3分别与1,1,2,2相连<br><strong>data.edge_attr</strong>: 边特征矩阵，形状[M，m]<br><strong>data.y</strong>: 要训练的目标（可以是任意形状），如节点级目标[节点数，<em>]，图级目标[1,</em>]（寿命预测常用，用rul作为图的label）,此处<em>代表样本数量。<br><em>*data.pos</em></em>: 节点位置矩阵，形状：[N,num_dimensions],在有些图中，节点是具有坐标属性，比如3D点云，每个节点都是3维空间中的一个坐标，类似的也可以是其它维度的。<br>这些属性都不是必须属性，实际上Data对象并不局限于这些属性。例如：我们可以通过data.face来扩展它，用以保存形状为[3，num_Faces]（数据类型为torch.long）的三维网格中三角形的连通性。</p>
<h4 id="PyG的DataLoader"><a href="#PyG的DataLoader" class="headerlink" title="PyG的DataLoader"></a>PyG的DataLoader</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_scatter <span class="keyword">import</span> scatter_mean</span><br><span class="line"><span class="keyword">from</span> torch_geometric.datasets <span class="keyword">import</span> TUDataset</span><br><span class="line"><span class="keyword">from</span> torch_geometric.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line">dataset = TUDataset(root=<span class="string">&#x27;/tmp/ENZYMES&#x27;</span>, name=<span class="string">&#x27;ENZYMES&#x27;</span>, use_node_attr=<span class="literal">True</span>)</span><br><span class="line">loader = DataLoader(dataset, batch_size=<span class="number">32</span>, shuffle=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> loader:</span><br><span class="line">    data</span><br><span class="line">    &gt;&gt;&gt; Batch(batch=[<span class="number">1082</span>], edge_index=[<span class="number">2</span>, <span class="number">4066</span>], x=[<span class="number">1082</span>, <span class="number">21</span>], y=[<span class="number">32</span>])</span><br><span class="line"></span><br><span class="line">    data.num_graphs</span><br><span class="line">    &gt;&gt;&gt; <span class="number">32</span></span><br><span class="line"></span><br><span class="line">    x = scatter_mean(data.x, data.batch, dim=<span class="number">0</span>)</span><br><span class="line">    x.size()</span><br><span class="line">    &gt;&gt;&gt; torch.Size([<span class="number">32</span>, <span class="number">21</span>])</span><br></pre></td></tr></table></figure>
<p>torch_geometric.data.Batch 继承自 torch_geometric.data.Data 并且包含了一个额外的属性：batch.<br>batch 是一个列向量，它将整个图的所有节点映射到不同的批次中。</p>
<h4 id="基础的GCN"><a href="#基础的GCN" class="headerlink" title="基础的GCN"></a>基础的GCN</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch_geometric.nn <span class="keyword">import</span> GCNConv</span><br><span class="line"></span><br><span class="line">self.conv1 = GCNConv(num_node_features,num_hidden) <span class="comment">#实例化</span></span><br><span class="line">x = self.conv1(x, edge_index) <span class="comment">#前先传播</span></span><br></pre></td></tr></table></figure>
<h3 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h3><p>GCN空间和频域理解<br><a href="https://zhuanlan.zhihu.com/p/633419078?utm_id=0">https://zhuanlan.zhihu.com/p/633419078?utm_id=0</a></p>
<p>GCN原理<br><a href="https://distill.pub/2021/gnn-intro/">https://distill.pub/2021/gnn-intro/</a></p>
<p>PyG文档之二：快速入门<a href="https://blog.csdn.net/Yichar/article/details/107856411">https://blog.csdn.net/Yichar/article/details/107856411</a></p>
]]></content>
      <categories>
        <category>study notes</category>
      </categories>
  </entry>
  <entry>
    <title>new-study-note4</title>
    <url>/2024/06/06/new-study-note/new-study-note4/</url>
    <content><![CDATA[<h2 id="数据扩充"><a href="#数据扩充" class="headerlink" title="数据扩充"></a>数据扩充</h2><p>对于时间序列，当数据长度不满足于time_window时可以采用插值或填充首数据的方法获取数据（对于CMAPSS数据集）。</p>
<h3 id="插值：todo"><a href="#插值：todo" class="headerlink" title="插值：todo"></a>插值：todo</h3><h3 id="拟合"><a href="#拟合" class="headerlink" title="拟合"></a>拟合</h3><p>多项式拟合（Polynomial Fitting）：多项式拟合是一种基本的拟合方法，它使用多项式函数来逼近数据。多项式拟合可以通过最小二乘法（Least Squares Method）或使用多项式拟合函数（如<code>numpy.polyfit</code>）来实现。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例散点数据</span></span><br><span class="line">x_data = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line">y_data = np.array([<span class="number">1.2</span>, <span class="number">1.9</span>, <span class="number">3.2</span>, <span class="number">4.1</span>, <span class="number">5.5</span>, <span class="number">6.8</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 拟合多项式的阶数</span></span><br><span class="line">degree = <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多项式拟合</span></span><br><span class="line">coefficients = np.polyfit(x_data, y_data, degree)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据拟合系数生成拟合曲线的x值</span></span><br><span class="line">x_fit = np.linspace(<span class="built_in">min</span>(x_data), <span class="built_in">max</span>(x_data), <span class="number">100</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算拟合曲线的y值</span></span><br><span class="line">y_fit = np.polyval(coefficients, x_fit)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制原始散点和拟合曲线</span></span><br><span class="line">plt.scatter(x_data, y_data, label=<span class="string">&#x27;Data&#x27;</span>)</span><br><span class="line">plt.plot(x_fit, y_fit, <span class="string">&#x27;r&#x27;</span>, label=<span class="string">&#x27;Fit&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.xlabel(<span class="string">&#x27;x&#x27;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;y&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Polynomial Fitting&#x27;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#具体例子</span></span><br><span class="line"><span class="comment">#多项式拟合</span></span><br><span class="line">data = np.zeros((time_window, self.x.shape[<span class="number">1</span>]))</span><br><span class="line"><span class="comment">#对每一列数据单独计算</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(data.shape[<span class="number">1</span>]):</span><br><span class="line">    x_old = np.linspace(<span class="number">0</span>, <span class="built_in">len</span>(self.x)-<span class="number">1</span>, <span class="built_in">len</span>(self.x), dtype=np.float64)</span><br><span class="line">    <span class="comment">#用原来的数据通过polyfit求出参数</span></span><br><span class="line">    params = np.polyfit(x_old, self.x[:, j].flatten(), deg=<span class="number">1</span>)</span><br><span class="line">    k = params[<span class="number">0</span>]</span><br><span class="line">    b = params[<span class="number">1</span>]</span><br><span class="line">    x_new = np.linspace(<span class="number">0</span>, time_window-<span class="number">1</span>, time_window, dtype=np.float64)</span><br><span class="line">    <span class="comment">#用新的x和参数计算新的值</span></span><br><span class="line">    data[:, j] = (x_new * <span class="built_in">len</span>(self.x) / time_window * k + b)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<h3 id="other"><a href="#other" class="headerlink" title="other"></a>other</h3><p>直接用首第一个元素pad成time_window长度，对于ndarry可以直接用np.pad</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">data=np.pad(self.x, ((time_window - self.x.shape[0], 0), (0, 0)), &#x27;edge&#x27;)</span><br><span class="line">X=data[:, 5:-2]</span><br></pre></td></tr></table></figure>
<h2 id="轴承故障诊断-振动信号分析"><a href="#轴承故障诊断-振动信号分析" class="headerlink" title="轴承故障诊断-振动信号分析"></a>轴承故障诊断-振动信号分析</h2><h3 id="时域"><a href="#时域" class="headerlink" title="时域"></a>时域</h3><h4 id="时域波形"><a href="#时域波形" class="headerlink" title="时域波形"></a>时域波形</h4><h3 id="频域"><a href="#频域" class="headerlink" title="频域"></a>频域</h3><h4 id="FFT"><a href="#FFT" class="headerlink" title="FFT"></a>FFT</h4><p>通过频谱分析我们可以将信号从其原始的时间域（即随时间变化的形式）转换到频域（即按频率分布的形式）。<br>然而，传统的傅里叶变换有一个限制：它假设信号是平稳的，即其频率成分不会随时间改变。</p>
<h4 id="频谱"><a href="#频谱" class="headerlink" title="频谱"></a>频谱</h4><p>傅里叶谱（即频谱）表示：某一点频率上的幅值表示在整个信号里和在整个时间范围内，有一个含有此频率的三角函数组分。（横坐标为频率，纵坐标为幅值）</p>
<h4 id="包络谱"><a href="#包络谱" class="headerlink" title="包络谱"></a>包络谱</h4><p>包络谱：对信号进行hilbert变换之后，然后取极值，然后对取极值之后得到的一维数据取包络，对包络信号进行FFT变换得到的数据。（横坐标为频率，纵坐标为幅值）包络谱对冲击事件的故障比较敏感。包络谱图中各频率幅值的分布与的频谱图有所区别。频谱图中故障特征频率幅值较小，包络谱图中故障特征频率的幅值很高，窖易辨认。闪此，相对对于频谱分析，包络谱分析剔除了不必要的频率干扰，更能够凸显故障特征频率。根据包络谱图能更容易地对滚动轴承的故障种类进行判断。</p>
<p>频谱与包络谱的频率分布没有多大关系，①<strong>包络谱峰值较高的地方表示原始信号在该频率处有对应的频率分量</strong>；<strong>频谱峰值高的地方表示</strong>在整个信号里和在整个时间范围内，<strong>有一个含有此频率的三角函数组分</strong>。②频谱是直接对原信号做fft；包络谱是对原信号做hilbert变换之后的曲线取的包络线进行fft，得到的频域曲线理应不同。</p>
<h3 id="时频域"><a href="#时频域" class="headerlink" title="时频域"></a>时频域</h3><h4 id="STFT"><a href="#STFT" class="headerlink" title="STFT"></a>STFT</h4><p>对FFT进行加窗</p>
<p>在短时傅里叶变换（STFT）中，窗口函数及其大小选择是分析的关键。窗口函数决定了在任何给定时间点，信号的哪一部分被用于分析。窗口大小的选择直接影响了分析结果的时间分辨率和频率分辨率，这是进行有效STFT分析的最重要的权衡。</p>
<p>窗口大小的时间分辨率影响：时间分辨率与窗口的宽度密切相关。一个窄窗口提供较高的时间分辨率，因为它捕捉了信号在很短时间内的变化。这对于分析包含快速变化的瞬态事件，如敲击声或爆炸声，是非常有用的。然而，较小的窗口将限制频率分辨率，因为频率分析需要足够的周期来准确估计。</p>
<p>窗口大小的频率分辨率影响：频率分辨率与窗口的宽度呈反比。一个宽窗口覆盖了信号的较长时间段，提供了较高的频率分辨率。这是因为更多的周期可以在窗口内被分析，从而更准确地确定低频成分。但是，这会牺牲时间分辨率，因为窗口中的信号被假定在这段时间内是平稳的。</p>
<h4 id="CWT"><a href="#CWT" class="headerlink" title="CWT"></a>CWT</h4><p>高频部分具有较高的时间分辨率和较低的频率分辨率，而低频部分具有较高的频率分辨率和较低的时间分辨率，这就恰好解决了STFT的痛点。（PS：小波是复信号，上图中只画了实部投影）</p>
<h3 id="参考链接："><a href="#参考链接：" class="headerlink" title="参考链接："></a>参考链接：</h3><p>插值与拟合：<a href="https://blog.csdn.net/qq_45927003/article/details/120310513">https://blog.csdn.net/qq_45927003/article/details/120310513</a><br>python常见拟合方法：<a href="https://zhuanlan.zhihu.com/p/671063617">https://zhuanlan.zhihu.com/p/671063617</a></p>
<p>从傅里叶变换，到短时傅里叶变换，再到小波分析（CWT：<a href="https://zhuanlan.zhihu.com/p/589651368">https://zhuanlan.zhihu.com/p/589651368</a></p>
]]></content>
      <categories>
        <category>study notes</category>
      </categories>
  </entry>
  <entry>
    <title>new-study-note5</title>
    <url>/2024/06/06/new-study-note/new-study-note5/</url>
    <content><![CDATA[<h2 id="wandb"><a href="#wandb" class="headerlink" title="wandb"></a>wandb</h2><ol>
<li><strong>基本使用（使用wandb替换tensorboard）</strong></li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> wandb</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># start a new wandb run to track this script</span></span><br><span class="line">wandb.init(</span><br><span class="line">    <span class="comment"># set the wandb project where this run will be logged</span></span><br><span class="line">    project=<span class="string">&quot;my-awesome-project&quot;</span>,</span><br><span class="line">    <span class="comment">#记录项目名称</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># track hyperparameters and run metadata</span></span><br><span class="line">    config=&#123;</span><br><span class="line">    <span class="string">&quot;learning_rate&quot;</span>: <span class="number">0.02</span>,</span><br><span class="line">    <span class="string">&quot;architecture&quot;</span>: <span class="string">&quot;CNN&quot;</span>,</span><br><span class="line">    <span class="string">&quot;dataset&quot;</span>: <span class="string">&quot;CIFAR-100&quot;</span>,</span><br><span class="line">    <span class="string">&quot;epochs&quot;</span>: <span class="number">10</span>,</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">#记录参数</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># simulate training</span></span><br><span class="line">epochs = <span class="number">10</span></span><br><span class="line">offset = random.random() / <span class="number">5</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, epochs):</span><br><span class="line">    acc = <span class="number">1</span> - <span class="number">2</span> ** -epoch - random.random() / epoch - offset</span><br><span class="line">    loss = <span class="number">2</span> ** -epoch + random.random() / epoch + offset</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># log metrics to wandb</span></span><br><span class="line">    wandb.log(&#123;<span class="string">&quot;acc&quot;</span>: acc, <span class="string">&quot;loss&quot;</span>: loss&#125;) <span class="comment">#选择要记录的数据</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># [optional] finish the wandb run, necessary in notebooks</span></span><br><span class="line">wandb.finish()</span><br></pre></td></tr></table></figure>
<p>将要记录的数据一次性记录下来使他们有相同的步长，也可以设置wandb记录的横坐标</p>
<ol>
<li><strong>想要科学上网和wandb一起使用（离线使用）</strong></li>
</ol>
<p>需求： 一般情况下挂VPN会导致wandb初始化出错，不能连接到wandb服务器进行数据同步上传，因此可以采用离线上传数据。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> wandb</span><br><span class="line">os.environ[<span class="string">&quot;WANDB_API_KEY&quot;</span>] = <span class="string">&#x27;自己的API KEY&#x27;</span> </span><br><span class="line">os.environ[<span class="string">&quot;WANDB_MODE&quot;</span>] = <span class="string">&quot;offline&quot;</span></span><br></pre></td></tr></table></figure>
<p>在导包时加上如上语句，在训练后在根据提升上传数据到网站上。</p>
<blockquote>
<p>使用命令wandb sync 数据地址 上传数据</p>
</blockquote>
<p>官方文档：<a href="https://docs.wandb.ai/tutorials">https://docs.wandb.ai/tutorials</a></p>
<h2 id="使用小波包分解将一位时间数据转换为二维数据"><a href="#使用小波包分解将一位时间数据转换为二维数据" class="headerlink" title="使用小波包分解将一位时间数据转换为二维数据"></a>使用小波包分解将一位时间数据转换为二维数据</h2><p><strong>原理：</strong><br>小波包分解是常用的信号分析方法，能够将原始信号分解至多个频段，获得多组小波系数。<br>而这些小波系数可以堆叠在一起，构成二维的小波系数矩阵，如下图所示[1]。<br>值得注意的是，这个矩阵有着明确的意义：<strong>每一行表示了某个具体频段的信息。</strong></p>
<div align=center>
<img src= >
</div>

<p>更进一步地，在小波包分解时，可以采用多个小波基函数，从而获得多个二维矩阵，堆叠成三维矩阵[2]。</p>
<div align=center>
<img src= >
</div>

<h2 id="VAE-变分自动编码器"><a href="#VAE-变分自动编码器" class="headerlink" title="VAE(变分自动编码器)"></a>VAE(变分自动编码器)</h2><h3 id="VAE的基本架构"><a href="#VAE的基本架构" class="headerlink" title="VAE的基本架构"></a>VAE的基本架构</h3><p>由AE发展而来，却不同于AE，属于生成模型的一种。<br>与普通自动编码器一样，变分自动编码器有编码器Encoder与解码器Decoder两大部分组成，原始图像从编码器输入，经编码器后形成隐式表示（Latent Representation），之后隐式表示被输入到解码器、再复原回原始输入的结构。然而，与普通Autoencoders不同的是，<strong>变分自用编码器的Encoder与Decoder在数据流上并不是相连的，我们不会直接将Encoder编码后的结果传递给Decoder，而是要使得隐式表示满足既定分布。</strong></p>
<p>具体流程如下：</p>
<blockquote>
<p>①首先，变分自动编码器中的编码器会尽量将样本$X$所携带的所有特征信息的分布转码成类高斯分布。<br>②编码器需要输出该类高斯分布的均值$\mu$与标准差$\sigma$作为编码器的输出。<br>③以编码器生成的均值$\mu$与标准差$\sigma$为基础构建高斯分布。<br>④从构建的高斯分布中随机采样出一个数值$Z$，将该数值输入解码器。<br>⑤解码器基于$Z$进行解码，并最终输出与样本的原始特征结构一致的数据，作为VAE的输出$X$′。</p>
</blockquote>
<div align=center>
<img src= >
</div>


<p>根据以上流程，变分自动编码器的Encoder在输出时，并不会直接输出原始数据的隐式表示，而是会输出从原始数据提炼出的均值和标准差。之后，我们需要建立均值为、标准差为的正态分布，并从该正态分布中抽样出隐式表示z，再将隐式表示z输入到Decoder中进行解码。对隐式表示z而言，它传递给Decoder的就不是原始数据的信息，而只是与原始数据同均值、同标准差的分布中的信息了。这样做就正好印证了3.1节提出背景中的两点：</p>
<ol>
<li>斩断了神经网络中惯例的“从输入到输出”的数据流，以此杜绝了信息直接被复制到输出层的可能性。</li>
<li>使得编码器具有以满足特定分布数据的输入到输出的能力，也就是解码器的生成能力。<br>由于第④步采样的存在，使得VAE的整体过程较为复杂，下面将一步步拆解。</li>
</ol>
<h3 id="VAE的正向传播"><a href="#VAE的正向传播" class="headerlink" title="VAE的正向传播"></a>VAE的正向传播</h3><p>m个样本每个样本生成一组标准差和均值</p>
<div align=center>
<img src= >
</div>

<p>在变分自动编码器的流程当中，<strong>均值和标准差都不是通过他们的数学定义计算出来的，而是通过Encoder提炼出来的</strong>。这就是说<strong>当前的均值和标准差不是真实数据的统计量，而是通过Encoder推断出的、当前样本数据可能服从的任意分布中的属性</strong>。我们不可能知道当前样本服从的真实分布的状态，因此这一推断过程自然可以根据不同的规则（Encoder中不同的权重）得出不同的结果。</p>
<p>例如，我们可以令Encoder的输出层存在3个神经元，这样Encoder就会对每一个样本推断出三对不同的均值和标准差。这个行为相当于对样本数据所属的原始分布进行估计，但给出了三个可能的答案。因此现在，在每个样本下，我们就可以基于三个均值和标准差的组合生成三个不同的正态分布了。</p>
<div align=center>
<img src= >
</div>

<p>对任意的自动编码器而言，隐式空间越大，隐式表示z所携带的信息自然也会越多，自动编码器的表现就可能变得更好，因此在实际使用变分自动编码器的过程中，一个样本上至少都会生成10~100组均值和标准差，隐式表示z的结构一般也是较高维的矩阵。</p>
<p><strong>变分自动编码器有如下的三个特点：</strong></p>
<ul>
<li>斩断了神经网络中惯例的“从输入到输出”的数据流，以此<strong>杜绝了信息直接被复制到输出层的可能性。</strong></li>
<li><strong>无论在训练还是预测过程中，模型都存在随机性</strong>，相比之下，大部分带有随机性的算法只会在训练的过程中有随机性，而在测试过程中对模型进行固定。但由于变分自动编码器的“随机性”是与网络架构及输入数据结构都高度相关的随机性，因此当训练数据变化的时候，随机抽样的情况也会跟着变化。</li>
<li><strong>可以作为生成模型使用</strong>。其他的自动编码器都是在原始图像上进行修改，而变分自动编码器可以从无到有生成与训练集高度相似的数据。由于输入Decoder的信息只是从正态分布中抽选的随机样本，因此其本质与随机数差异不大，当我们训练完变分自动编码器之后，就可以只使用解码器部分，只要对解码器输入结构正确随机数/随机矩阵，就可以生成与训练时所用的真实数据高度类似的数据。</li>
</ul>
<h3 id="VAE的损失函数"><a href="#VAE的损失函数" class="headerlink" title="VAE的损失函数"></a>VAE的损失函数</h3><ul>
<li>第一部分：<strong>让编码器输出的概率分布和我们的先验的概率分布一样，这样就能够完成我们的生成任务。这一项是为了将数据的分布训练成为指定概率分布，方便生成任务的。</strong></li>
<li>第二部分：<strong>为了保证输出的精度，还需要让模型的输出与输入满足一定的关系。这又叫重构损失，例如可以看作回归任务的MSE，亦可能是交叉熵等其他损失函数。这一项是衡量模型输入与输出的关系的。</strong></li>
</ul>
<div align=center>
<img src= >
</div>

<h3 id="VAE的重参数化"><a href="#VAE的重参数化" class="headerlink" title="VAE的重参数化"></a>VAE的重参数化</h3><p>现在我们已经了解了变分自动编码器的基本流程和训练目标，那我们是否可以尝试着手动去实现变分自动编码器了呢？有一个隐藏的陷阱还没有被我们注意到，那就是前面讲过的VAE为了<strong>杜绝了信息直接被复制到输出层的可能性</strong>，斩断了神经网络中惯例的“从输入到输出”的数据流，用采样来连接网络的编码器与解码器。由<strong>于这个抽样流程的存在，架构中的数据流是断裂的，因此反向传播无法进行。反向传播要求每一层数据之间必有函数关系，而抽样流程不是一个函数关系，因此无法被反向传播</strong>。为了解决这一问题，变分自动编码器的原始论文提出了<strong>重参数化技巧，这一技巧可以帮助我们在抽样的同时建立$\mu$与$z$和$\sigma$之间的函数关系，这样就可以令反向传播顺利进行了。</strong></p>
<div align=center>
<img src= >
</div>

<p>VAE:<a href="https://zhuanlan.zhihu.com/p/628604566">https://zhuanlan.zhihu.com/p/628604566</a></p>
<p>todo</p>
]]></content>
      <categories>
        <category>study notes</category>
      </categories>
  </entry>
  <entry>
    <title>study-note1</title>
    <url>/2022/05/10/study-note/study-note1/</url>
    <content><![CDATA[<h2 id="1-blog中图片的正确使用方式"><a href="#1-blog中图片的正确使用方式" class="headerlink" title="1.blog中图片的正确使用方式"></a>1.blog中图片的正确使用方式</h2><div align=center>
<img src="https://raw.githubusercontent.com/datawhalechina/dive-into-cv-pytorch/master/markdown_imgs/whale_pytorch.jpg" width="250">
</div>

<h2 id="2-transfer-learning"><a href="#2-transfer-learning" class="headerlink" title="2.transfer learning"></a>2.transfer learning</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">resnet18 model:</span><br><span class="line"><span class="keyword">import</span> torch, torchvision</span><br><span class="line">model = torchvision.models.resnet18(pretrained=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h2 id="3-torch-rand用于生成服从区间-0-1-均匀分布的随机张量"><a href="#3-torch-rand用于生成服从区间-0-1-均匀分布的随机张量" class="headerlink" title="3.torch.rand用于生成服从区间[0,1)均匀分布的随机张量"></a>3.torch.rand用于生成服从区间[0,1)均匀分布的随机张量</h2><p>torch.randn用于生成服从均值为0、方差为1正太分布的随机张量</p>
<h2 id="4-tensor-detach"><a href="#4-tensor-detach" class="headerlink" title="4.tensor.detach()"></a>4.tensor.detach()</h2><p>如果某个张量不想要被继续追踪，可以调用.detach()将其从追踪记录中分离出来，可以防止将来的计算被追踪，这样梯度就传不过去了。</p>
<h2 id="5-pytorch一些包的总结"><a href="#5-pytorch一些包的总结" class="headerlink" title="5.pytorch一些包的总结"></a>5.pytorch一些包的总结</h2><p><strong>torch.nn：包含一些网络和module(构造网络）,functional（激活函数，需要在开始导入包)<br>import torch.nn.functional as F<br>torch.optim：包含优化器<br>torch.utils.data: 包含dataset和dataloadaer（构造数据集和loader）<br>torchvision: 包含transforms，datasets（数据集），models（模型用于transfer learning)</strong></p>
<ul>
<li>models：提供深度学习中各种经典网络的网络结构以及预训练好的模型，包括<code>AlexNet</code>、VGG系列、ResNet系列、Inception系列等。</li>
<li>datasets： 提供常用的数据集加载，设计上都是继承<code>torhc.utils.data.Dataset</code>，主要包括<code>MNIST</code>、<code>CIFAR10/100</code>、<code>ImageNet</code>、<code>COCO</code>等。</li>
<li>transforms：提供常用的数据预处理操作，主要包括对Tensor以及PIL Image对象的操作。</li>
</ul>
<span id="more"></span>
<h2 id="6-dataloader-本质是一个list迭代器？"><a href="#6-dataloader-本质是一个list迭代器？" class="headerlink" title="6.dataloader 本质是一个list迭代器？"></a>6.dataloader 本质是一个list迭代器？</h2><p>可以用len（某个loader）知道样本数量</p>
<h2 id="7-torchvision-数据集的下载方法"><a href="#7-torchvision-数据集的下载方法" class="headerlink" title="7.torchvision 数据集的下载方法"></a>7.torchvision 数据集的下载方法</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision </span><br><span class="line"><span class="keyword">from</span> torch.utils.data.dataset <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms        </span><br></pre></td></tr></table></figure>
<p>读取训练集</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">custom_transform=transforms.transforms.Compose([</span><br><span class="line">              transforms.Resize((<span class="number">64</span>, <span class="number">64</span>)),    <span class="comment"># 缩放到指定大小 64*64</span></span><br><span class="line">              transforms.ColorJitter(<span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>),    <span class="comment"># 随机颜色变换</span></span><br><span class="line">              transforms.RandomRotation(<span class="number">5</span>),    <span class="comment"># 随机旋转</span></span><br><span class="line">              transforms.Normalize([<span class="number">0.485</span>,<span class="number">0.456</span>,<span class="number">0.406</span>],    <span class="comment"># 对图像像素进行归一化</span></span><br><span class="line">                                   [<span class="number">0.229</span>,<span class="number">0.224</span>,<span class="number">0.225</span>])])</span><br><span class="line">train_data=torchvision.datasets.CIFAR10(<span class="string">&#x27;../../../dataset&#x27;</span>, </span><br><span class="line">                                        train=<span class="literal">True</span>,                                       </span><br><span class="line">                                        transform=custom_transforms,</span><br><span class="line">                                        target_transform=<span class="literal">None</span>, </span><br><span class="line">                                        download=<span class="literal">False</span>)   </span><br></pre></td></tr></table></figure>
<p>参数：</p>
<blockquote>
<p>dataset_dir：存放数据集的路径。<br>train（bool，可选）–如果为True，则构建训练集，否则构建测试集。<br>transform：定义数据预处理，数据增强方案都是在这里指定。<br>target_transform：标注的预处理，分类任务不常用。<br>download：是否下载，若为True则从互联网下载，如果已经在dataset_dir下存在，就不会再次下载</p>
</blockquote>
<h2 id="8-构建数据集"><a href="#8-构建数据集" class="headerlink" title="8.构建数据集"></a>8.构建数据集</h2><p>图像数据 ➡ 图像索引文件 ➡ 使用Dataset构建数据集 ➡ 使用DataLoader读取数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data.dataset <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyDataset</span>(<span class="title class_ inherited__">Dataset</span>):  <span class="comment"># 继承Dataset类</span></span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">       <span class="comment"># 初始化图像文件路径或图像文件名列表等</span></span><br><span class="line">       <span class="keyword">pass</span></span><br><span class="line">   </span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, index</span>):</span><br><span class="line">        <span class="comment"># 1.根据索引index从文件中读取一个数据（例如，使用numpy.fromfile，PIL.Image.open，cv2.imread）</span></span><br><span class="line">        <span class="comment"># 2.预处理数据（例如torchvision.Transform）</span></span><br><span class="line">        <span class="comment"># 3.返回数据对（例如图像和标签）</span></span><br><span class="line">       <span class="keyword">pass</span></span><br><span class="line">   </span><br><span class="line">   <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">       <span class="keyword">return</span> count  <span class="comment"># 返回数据量</span></span><br></pre></td></tr></table></figure>
<h2 id="9-imagefolder"><a href="#9-imagefolder" class="headerlink" title="9.imagefolder"></a>9.imagefolder</h2><p>特殊的构造数据集方式imagefolder（针对以文件目录分类好的图像）</p>
<blockquote>
<p>ImageFolder(root, transform=None, target_transform=None, loader=default_loader)</p>
</blockquote>
<p>它主要有四个参数：</p>
<ul>
<li><code>root</code>：在root指定的路径下寻找图片</li>
<li><code>transform</code>：对PIL Image进行的转换操作，transform的输入是使用loader读取图片的返回对象</li>
<li><code>target_transform</code>：对label的转换</li>
<li><code>loader</code>：给定路径后如何读取图片，默认读取为RGB格式的PIL Image对象</li>
</ul>
<div align=center>
<img src="https://pic.imgdb.cn/item/6294e0bf09475431295c6e4d.png" width ="90%">
</div>

<h2 id="10-数据增强简介"><a href="#10-数据增强简介" class="headerlink" title="10.数据增强简介"></a>10.数据增强简介</h2><p>图像的增广是通过对训练图像进行一系列变换，产生相似但不同于主体图像的训练样本，来扩大数据集的规模的一种常用技巧。另一方面，随机改变训练样本降低了模型对特定数据进行记忆的可能，有利于增强模型的泛化能⼒，提高模型的预测效果，因此可以说数据增强已经不算是一种优化技巧，而是CNN训练中默认要使用的标准操作。在常见的数据增广方法中，一般会从图像颜色、尺寸、形态、亮度/对比度、噪声和像素等角度进行变换。当然不同的数据增广方法可以自由进行组合，得到更加丰富的数据增广方法。<br><strong><a href="https://datawhalechina.github.io/dive-into-cv-pytorch/#/chapter02_image_classification_introduction/appendix/appendixA_data_augment">torchvision.transforms速查</a></strong></p>
]]></content>
      <categories>
        <category>study notes</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title>study-note2</title>
    <url>/2022/05/11/study-note/study-note2/</url>
    <content><![CDATA[<h2 id="街景识别思路"><a href="#街景识别思路" class="headerlink" title="街景识别思路"></a>街景识别思路</h2><h3 id="1-model-dataset：建立两个数据集，两个模型"><a href="#1-model-dataset：建立两个数据集，两个模型" class="headerlink" title="1.model-dataset：建立两个数据集，两个模型"></a>1.model-dataset：建立两个数据集，两个模型</h3><p>标签数据集img_path,img_label,transforms<br>长度数据集接收img_path,img_length,transforms<br>transforms:进行数据增强还改变了原像素</p>
<p>模型为transfer learning中的resnet18<br>标签模型经过四个全连接层返回list<br>长度模型经过一个全连接层返回【x，4】</p>
<p>注：传入crossentropyloss的length和label要是能转变成one-hot编码的longtensor，length传入前要减1<br>label训练时每次传入补齐的四个标签，使模型学习到的预测总有四个</p>
<h3 id="2-length-recognition："><a href="#2-length-recognition：" class="headerlink" title="2.length_recognition："></a>2.length_recognition：</h3><p>先要得到去掉长度大于四的path和length<br>构建trainloader<br>…<br>构建validoader<br>定义训练函数（trainloader，model，criterion，optimizer，epoch）<br>定义验证函数（validloader，model，criterion）<br><strong>参考网站进行可视化</strong>:在train和valid中加入loss，acc，每次epoch结束添加losses，acces</p>
<h3 id="3-label-recognition"><a href="#3-label-recognition" class="headerlink" title="3.label_recognition:"></a>3.label_recognition:</h3><p>与模块2最大的不同的是check__acc模块</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#1.check_acc中改变pred的维度tensor.transpose</span></span><br><span class="line"><span class="comment">#2.将装有tensor的list转为tensor——即将多个一维tensor合并成1个多维tensor——torch.stack()</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tensor1=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">tensor2=torch.tensor([<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>])</span><br><span class="line">tensor_list=[]</span><br><span class="line"></span><br><span class="line">tensor_list.append(tensor1)</span><br><span class="line">tensor_list.append(tensor2)</span><br><span class="line"></span><br><span class="line">final_tensor=torch.stack(tensor_list,<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(tensor_list)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(tensor_list))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(final_tensor)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(final_tensor))</span><br></pre></td></tr></table></figure>
<h2 id="torch-from-numpy-VS-torch-Tensor"><a href="#torch-from-numpy-VS-torch-Tensor" class="headerlink" title="torch.from_numpy VS torch.Tensor"></a>torch.from_numpy VS torch.Tensor</h2><p>当转换的源是float类型，torch.Tensor与torch.from_numpy会共享一块内存！且转换后的结果的类型是torch.float32<br>当转换的源不是float类型，torch.Tensor得到的是torch.float32，而torch.from_numpy则是与源类型一致！<br><strong>尽量使用torch.from_numpy()</strong></p>
<h2 id="数据类型转换"><a href="#数据类型转换" class="headerlink" title="数据类型转换"></a>数据类型转换</h2><p>CPU或GPU张量之间的转换<br><strong>一般只要在Tensor后加long(), int(), double(),float(),byte()等函数就能将Tensor进行类型转换；</strong> 例如：Torch.LongTensor—-&gt;Torch.FloatTensor, 直接使用data.float()即可<br>还可以使用type()函数，data为Tensor数据类型，data.type()为给出data的类型，如果使用data.type(torch.FloatTensor)则强制转换为torch.FloatTensor类型张量。<br><strong>一般先换成tensor再转换数据类型(普通数据还要先变成ndarry)</strong></p>
<h2 id="tensor-to-gpu"><a href="#tensor-to-gpu" class="headerlink" title="tensor to gpu"></a>tensor to gpu</h2><h3 id="方法一"><a href="#方法一" class="headerlink" title="方法一"></a>方法一</h3><p>如果GPU可用，将模型和张量加载到GPU上</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    model = model.cuda()</span><br><span class="line">    x = x.cuda()</span><br><span class="line">    y = y.cuda()</span><br></pre></td></tr></table></figure>
<h3 id="方法二"><a href="#方法二" class="headerlink" title="方法二"></a>方法二</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#分配到的GPU或CPU</span></span><br><span class="line">device=torch.device(<span class="string">&quot;cuda:0&quot;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&quot;cpu&quot;</span>)</span><br><span class="line"><span class="comment">#将模型加到GPU</span></span><br><span class="line">model=model.to(device)</span><br><span class="line"><span class="comment">#将张量加到GPU</span></span><br><span class="line">x=x.to(device)</span><br><span class="line">y=y.to(device)</span><br></pre></td></tr></table></figure>
<h2 id="使每次训练结果一致"><a href="#使每次训练结果一致" class="headerlink" title="使每次训练结果一致"></a>使每次训练结果一致</h2><p>固定随机种子</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">init_seeds</span>(<span class="params">seed=<span class="number">0</span></span>):</span><br><span class="line">    torch.manual_seed(seed) <span class="comment"># sets the seed for generating random numbers.</span></span><br><span class="line">    torch.cuda.manual_seed(seed) <span class="comment"># Sets the seed for generating random numbers for the current GPU. It’s safe to call this function if CUDA is not available; in that case, it is silently ignored.</span></span><br><span class="line">    torch.cuda.manual_seed_all(seed) <span class="comment"># Sets the seed for generating random numbers on all GPUs. It’s safe to call this function if CUDA is not available; in that case, it is silently ignored.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> seed == <span class="number">0</span>:</span><br><span class="line">        torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line">        torch.backends.cudnn.benchmark = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">init_seeds(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>study notes</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title>study-note4</title>
    <url>/2022/05/18/study-note/study-note4/</url>
    <content><![CDATA[<h2 id="bounding-box-目标的外接矩形框"><a href="#bounding-box-目标的外接矩形框" class="headerlink" title="bounding box(目标的外接矩形框)"></a>bounding box(目标的外接矩形框)</h2><p>用来表达bbox的格式通常有两种，(x1, y1, x2, y2) 和 (c_x, c_y, w, h)如图 </p>
<div align=center>
<img src='https://pic.imgdb.cn/item/62850fea0947543129ce5992.png'width='70%'higth='70%'>
</div>

<h2 id="python中assert的用法"><a href="#python中assert的用法" class="headerlink" title="python中assert的用法"></a>python中assert的用法</h2><p>使用assert是学习python的一个非常好的习惯，在没完善一个程序之前，我们不知道程序在哪里会出错，与其让它在运行时崩溃，不如在出现错误条件时就崩溃。用一个程序说明：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span>  <span class="title function_">zero</span>(<span class="params">s</span>):</span><br><span class="line">    a = <span class="built_in">int</span>(s)</span><br><span class="line">    <span class="keyword">assert</span> a &gt; <span class="number">0</span>,<span class="string">&quot;a超出范围&quot;</span>   <span class="comment">#这句的意思：如果a确实大于0，程序正常往下运行</span></span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line">zero(<span class="string">&quot;-2&quot;</span>)  <span class="comment">#但是如果a是小于0的，程序会抛出AssertionError错误，报错为参数内容“a超出范围”</span></span><br></pre></td></tr></table></figure>
<p>运行结果如下：</p>
<blockquote>
<p>Traceback (most recent call last):<br>  File “e:\Python_list\class_student\temp.py”, line 6, in <module><br>    zero(“-2”)<br>  File “e:\Python_list\class_student\temp.py”, line 3, in zero<br>    assert a &gt; 0,”a超出范围”<br>AssertionError: a超出范围</p>
</blockquote>
<h2 id="iou-交并比"><a href="#iou-交并比" class="headerlink" title="iou(交并比)"></a>iou(交并比)</h2><p>在目标检测任务中，关于IOU的计算贯穿整个模型的训练测试和评价过程，是非常非常重要的一个概念，其目的是用来衡量两个目标框的重叠程度。<br>IoU的全称是交并比（Intersection over Union），表示两个目标框的交集占其并集的比例</p>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><h3 id="matching-strategy"><a href="#matching-strategy" class="headerlink" title="matching strategy"></a>matching strategy</h3><p>第一个原则：从ground truth box出发，寻找与每一个ground truth box有最大的jaccard overlap的prior bbox，这样就能保证每一个groundtruth box一定与一个prior bbox对应起来(jaccard overlap就是IOU)。 反之，若一个prior bbox没有与任何ground truth进行匹配，那么该prior bbox只能与背景匹配，就是负样本。<br>第二个原则：从prior bbox出发，对剩余的还没有配对的prior bbox与任意一个ground truth box尝试配对，只要两者之间的jaccard overlap大于阈值（一般是0.5），那么该prior bbox也与这个ground truth进行匹配。这意味着某个ground truth可能与多个Prior box匹配，这是可以的。但是反过来却不可以，因为一个prior bbox只能匹配一个ground truth，如果多个ground truth与某个prior bbox的 IOU 大于阈值，那么prior bbox只与IOU最大的那个ground truth进行匹配。</p>
<h3 id="目标检测常用损失函数-类别损失-位置损失"><a href="#目标检测常用损失函数-类别损失-位置损失" class="headerlink" title="目标检测常用损失函数-类别损失+位置损失"></a>目标检测常用损失函数-类别损失+位置损失</h3><div align=center>
<img src='https://pic.imgdb.cn/item/62850fea0947543129ce5989.jpg'width=100%>
</div>

<p><strong>类别损失</strong>为交叉熵损失cross entropy loss 如图4 因为背景的类别为0所以添加一项</p>
<div align=center>
<img src='https://pic.imgdb.cn/item/62850fea0947543129ce5989.jpg'width=100%>
</div>

<p><strong>位置损失</strong>为 smooth l1 loss </p>
<p><img src="https://pic.imgdb.cn/item/62850fea0947543129ce599d.png" alt=""></p>
<p>参考网站<a href="https://blog.csdn.net/xjp_xujiping/article/details/107589950">目标检测常用损失函数-类别损失+位置损失</a></p>
<h3 id="Hard-negative-minin"><a href="#Hard-negative-minin" class="headerlink" title="Hard negative minin"></a>Hard negative minin</h3><p>值得注意的是，一般情况下negative prior bboxes数量 &gt;&gt; positive prior bboxes数量，直接训练会导致网络过于重视负样本，预测效果很差。为了保证正负样本尽量平衡，我们这里使用SSD使用的在线难例挖掘策略(hard negative mining)，即依据confidience loss对属于负样本的prior bbox进行排序，只挑选其中confidience loss高的bbox进行训练，将正负样本的比例控制在positive：negative=1:3。其核心作用就是只选择负样本中容易被分错类的困难负样本来进行网络训练，来保证正负样本的平衡和训练的有效性。</p>
<p>举个例子：假设在这 441 个 prior bbox 里，经过匹配后得到正样本先验框P个，负样本先验框 441−P 个。将负样本prior bbox按照prediction loss从大到小顺序排列后选择最高的M个prior bbox。这个M需要根据我们设定的正负样本的比例确定，比如我们约定正负样本比例为1:3时。我们就取M=3P，这M个loss最大的负样本难例将会被作为真正参与计算loss的prior bboxes，其余的负样本将不会参与分类损失的loss计算。</p>
]]></content>
      <categories>
        <category>study notes</category>
      </categories>
  </entry>
  <entry>
    <title>study-note3</title>
    <url>/2022/05/17/study-note/study-note3/</url>
    <content><![CDATA[<h2 id="获得最大值索引"><a href="#获得最大值索引" class="headerlink" title="获得最大值索引"></a>获得最大值索引</h2><p>ndarry返回数组最大值索引<code>idx=np.argmax(ndarry)</code><br>tensor返回数组最大值索引<code>_,idx=torch.max(tensor)</code></p>
<h2 id="分类网络发展"><a href="#分类网络发展" class="headerlink" title="分类网络发展"></a>分类网络发展</h2><h3 id="VGG案例"><a href="#VGG案例" class="headerlink" title="VGG案例"></a>VGG案例</h3><p>使用了<strong>3个3x3卷积核</strong>来代替<strong>7x7卷积核</strong>，使用了<strong>2个3x3卷积核</strong>来代替<strong>5*5卷积核</strong>。这样做的主要目的是在保证具有相同感知野的条件下，<strong>提升了网络的深度</strong>（因为多层非线性层可以增加网络深度来保证学习更复杂的模式），在一定程度上提升了神经网络的效果。<br>设输入通道数和输出通道数都为C， 3个步长为1的3x3卷积核的一层层叠加作用可看成一个大小为7的感受野（其实就表示3个3x3连续卷积相当于一个7x7卷积），其参数总量为<code>3\times (9\times C^2)3×(9×C 2)</code> ,如果直接使用7x7卷积核，其参数总量为 <code>49\times C^249×C</code> ,即减少了参数；而且3x3卷积核有利于更好地保持图像性质。<br>VGGNet的结构非常简洁，整个网络都使用了同样大小的卷积核尺寸（3x3）和最大池化尺寸（2x2）。几个小滤波器（3x3）卷积层的组合比一个大滤波器（5x5或7x7）卷积层好：验证了通过不断加深网络结构可以提升性能。缺点是VGG耗费更多计算资源，并且使用了更多的参数，这里不是3x3卷积的原因，其中绝大多数的参数都是来自于第一个全连接层。</p>
<h3 id="nin案例"><a href="#nin案例" class="headerlink" title="nin案例"></a>nin案例</h3><p>1x1卷积等效于该像素点在所有特征上进行一次全连接的计算，起到了压缩通道，即降维的作用，减少了通道的数量。</p>
<h3 id="残差网络-residual-net"><a href="#残差网络-residual-net" class="headerlink" title="残差网络 residual net"></a>残差网络 residual net</h3><p>ResNets要解决的是深度神经网络的“退化”问题。我们知道，对浅层网络逐渐叠加layers，模型在训练集和测试集上的性能会变好，因为模型复杂度更高了，表达能力更强了，可以对潜在的映射关系拟合得更好。而“退化”指的是，给网络叠加更多的层后，性能却快速下降的情况，如图：</p>
<h2 id="batch-normalization"><a href="#batch-normalization" class="headerlink" title="batch normalization"></a>batch normalization</h2><p>对卷积层做批量归一化<br>对卷积层来说，批量归一化发生在卷积计算之后、应用激活函数之前。如果卷积计算输出多个通道，我们需要对这些通道的输出分别做批量归一化，且每个通道都拥有独立的拉伸和偏移参数，并均为标量。设小批量中有m个样本。在单个通道上，假设卷积计算输出的高和宽分别为p和q。我们需要对该通道中m×p×q个元素同时做批量归一化。对这些元素做标准化计算时，我们使用相同的均值和方差，即该通道中m×p×q个元素的均值和方差。</p>
<h2 id="查看dataloader中的内容"><a href="#查看dataloader中的内容" class="headerlink" title="查看dataloader中的内容"></a>查看dataloader中的内容</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dataiter = <span class="built_in">iter</span>(trainloader) <span class="comment">#使dataloader变成一个迭代器</span></span><br><span class="line">images, labels = dataiter.<span class="built_in">next</span>() </span><br><span class="line"><span class="comment"># or images,labels=next(dataiter)</span></span><br></pre></td></tr></table></figure>
<h2 id="神经网络中一些常用的层及参数"><a href="#神经网络中一些常用的层及参数" class="headerlink" title="神经网络中一些常用的层及参数"></a>神经网络中一些常用的层及参数</h2><h3 id="bn"><a href="#bn" class="headerlink" title="bn"></a>bn</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line">Copy to clipboardErrorCopied</span><br></pre></td></tr></table></figure>
<blockquote>
<p>函数介绍：<br><strong>torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)</strong><br><strong>num_features</strong>： 来自期望输入的特征数，该期望输入的大小为batch_size x num_features [x width]<br><strong>eps</strong>： 为保证数值稳定性（分母不能趋近或取0）,给分母加上的值。默认为1e-5。<br><strong>momentum</strong>： 动态均值和动态方差所使用的动量。默认为0.1。<br><strong>affine</strong>： 布尔值，当设为true，给该层添加可学习的仿射变换参数。<br><strong>track_running_stats</strong>：布尔值，当设为true，记录训练过程中的均值和方差；</p>
</blockquote>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self.drop1 = nn.Dropout2d()</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>study notes</category>
      </categories>
      <tags>
        <tag>study notes</tag>
      </tags>
  </entry>
  <entry>
    <title>study-note5</title>
    <url>/2022/05/20/study-note/study-note5/</url>
    <content><![CDATA[<h2 id="colab的使用"><a href="#colab的使用" class="headerlink" title="colab的使用"></a>colab的使用</h2><p>将文件（代码和dataset）上传到Googledrive中在云盘中进入需要创建ipynb的地方打开colab（此时会在改目录下创建ipynb），在ipynb中运行代码</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 加载盘</span></span><br><span class="line"><span class="keyword">from</span> google.colab <span class="keyword">import</span> drive</span><br><span class="line">drive.mount(<span class="string">&#x27;/content/drive/&#x27;</span>) <span class="comment">#目前还无法挂载云盘指定的文件，只能将云盘整个挂载上来</span></span><br><span class="line"><span class="comment"># 指定当前的工作文件夹</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 此处为google drive中的文件路径,drive为之前指定的工作根目录，要加上</span></span><br><span class="line">os.chdir(<span class="string">&quot;/content/drive/My Drive/Colab/Notebooks/17.yoloV3&quot;</span>) </span><br><span class="line"><span class="comment">###之后就可以在ipynb中import</span></span><br></pre></td></tr></table></figure>
<h3 id="在colab中的一些常用操作"><a href="#在colab中的一些常用操作" class="headerlink" title="在colab中的一些常用操作"></a>在colab中的一些常用操作</h3><p><code>!pip install</code><br><code>!ls #查看当前路径的文件</code><br><code>!pwd #查看当前路径</code><br><code>!python xxx.py #运行py文件</code></p>
<h3 id="下载数据集："><a href="#下载数据集：" class="headerlink" title="下载数据集："></a>下载数据集：</h3><ol>
<li>先定位到要存放数据集的文件地址</li>
<li><p>对于某些大型数据集可直接在官网找到下载url</p>
<ul>
<li>!wget url ##下载数据集</li>
</ul>
</li>
<li><p>对于kaggle中数据集 <a href="https://zhuanlan.zhihu.com/p/433334194">ref</a></p>
<ul>
<li>!pip install kaggle</li>
<li>上传kaggle.json文件到colab中</li>
<li>!mkdir -p ~/.kaggle<br>!cp kaggle.json ~/.kaggle/<br>!chmod 600 ~/.kaggle/kaggle.json </li>
<li>在kaggle数据集找到下载api，在前面加上!既可下载</li>
</ul>
</li>
<li><p>解压数据集</p>
<ul>
<li>!unzip 解压压缩包（解压时先在根目录解压较快）</li>
<li><blockquote>
<p>import zipfile<br>filename=’xxx.zip’<br>outpath=’xxx/xxx’<br>with zipfile.ZipFile(filename) as zfile:<br>  zfile.extractall(outpath)</p>
</blockquote>
</li>
</ul>
</li>
</ol>
<h3 id="防止colab断开"><a href="#防止colab断开" class="headerlink" title="防止colab断开"></a>防止colab断开</h3><p><a href="https://blog.csdn.net/Thebest_jack/article/details/124565741">https://blog.csdn.net/Thebest_jack/article/details/124565741</a></p>
<h3 id="colab保存选了模型"><a href="#colab保存选了模型" class="headerlink" title="colab保存选了模型"></a>colab保存选了模型</h3><p>在drive文件夹内的内容会保存到Googledrive上，所以训练后的模型要存到drive下<br>!cp -r checkpoints ‘/content/drive/MyDrive/checkpoints’</p>
<h2 id="目标检测mAP计算（mean-average-precision）"><a href="#目标检测mAP计算（mean-average-precision）" class="headerlink" title="目标检测mAP计算（mean average precision）"></a>目标检测mAP计算（mean average precision）</h2><p><strong>TP、FP、FN、TN</strong><br>TP（True Positive）：IoU &gt; IoU~threshold~（threshold 一般取 0.5）的检测框数量（同一Ground Truth只计算一次）<br>FP（False Positive）：IoU &lt;= IoU~threshold~的检测框，或者是检测到同一个GT的多余检测框的数量<br>FN（False Negative）：没有检测到的GT的数量<br>TN（True Negative）：在mAP评价指标中不会使用到</p>
<p><strong>precision和recall</strong><br><img src="https://pic.imgdb.cn/item/6287274909475431292bd9b8.png" alt=""></p>
<p><strong>ap就是p-r曲线下的面积</strong><br><strong>ap计算过程</strong><br>经过RPN等网络得到proposal -&gt; 对proposal用非极大值抑制筛选部分proposal -&gt;对proposal使用两个准则和ground truth配对并按confidence从大到小排列 -&gt;根据某个confidence计算TP FP FN</p>
<p><strong>MAP计算和loss计算不同<br>map只是计算proposal和gt的precision<br>loss是计算结果的loss，考虑confidence</strong></p>
]]></content>
      <categories>
        <category>study notes</category>
      </categories>
      <tags>
        <tag>colab</tag>
      </tags>
  </entry>
  <entry>
    <title>study-note6</title>
    <url>/2022/05/30/study-note/study-note6/</url>
    <content><![CDATA[<h2 id="zip函数的用法"><a href="#zip函数的用法" class="headerlink" title="zip函数的用法"></a>zip函数的用法</h2><p><a href="https://blog.csdn.net/csdn15698845876/article/details/73411541?spm=1001.2101.3001.6650.2&amp;utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-73411541-blog-79242387.pc_relevant_antiscanv3&amp;depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7ERate-2-73411541-blog-79242387.pc_relevant_antiscanv3&amp;utm_relevant_index=5">参考</a></p>
<h2 id="pytorch常用工具模块"><a href="#pytorch常用工具模块" class="headerlink" title="pytorch常用工具模块"></a>pytorch常用工具模块</h2><p>针对上述问题，PyTorch提供了torchvision。它是一个视觉工具包，提供了很多视觉图像处理的工具，其中<code>transforms</code>模块提供了对PIL <code>Image</code>对象和<code>Tensor</code>对象的常用操作。</p>
<p>对PIL Image的操作包括：</p>
<ul>
<li><code>Scale</code>：调整图片尺寸，长宽比保持不变</li>
<li><code>CenterCrop</code>、<code>RandomCrop</code>、<code>RandomResizedCrop</code>： 裁剪图片</li>
<li><code>Pad</code>：填充</li>
<li><code>ToTensor</code>：将PIL Image对象转成Tensor，会自动将[0, 255]归一化至[0, 1]</li>
</ul>
<p>对Tensor的操作包括：</p>
<ul>
<li>Normalize：标准化，即减均值，除以标准差</li>
<li>ToPILImage：将Tensor转为PIL Image对象</li>
</ul>
<p>如果要对图片进行多个操作，可通过<code>Compose</code>函数将这些操作拼接起来，类似于<code>nn.Sequential</code>。注意，这些操作定义后是以函数的形式存在，真正使用时需调用它的<code>__call__</code>方法，这点类似于<code>nn.Module</code>。例如要将图片调整为$224\times 224$，首先应构建这个操作<code>trans = Resize((224, 224))</code>，然后调用<code>trans(img)</code>。下面我们就用transforms的这些操作来优化上面实现的dataset。<br>代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transform = T.Compose([</span><br><span class="line">    T.Resize(<span class="number">224</span>), <span class="comment"># 缩放图片(Image)，保持长宽比不变，最短边为224像素</span></span><br><span class="line">    T.CenterCrop(<span class="number">224</span>), <span class="comment"># 从图片中间切出224*224的图片</span></span><br><span class="line">    T.ToTensor(), <span class="comment"># 将图片(Image)转成Tensor，归一化至[0, 1]</span></span><br><span class="line">    T.Normalize(mean=[<span class="number">.5</span>, <span class="number">.5</span>, <span class="number">.5</span>], std=[<span class="number">.5</span>, <span class="number">.5</span>, <span class="number">.5</span>]) <span class="comment"># 标准化至[-1, 1]，规定均值和标准差</span></span><br><span class="line">])</span><br><span class="line">data=transform（data）</span><br></pre></td></tr></table></figure>
<h2 id="fliter函数的用法"><a href="#fliter函数的用法" class="headerlink" title="fliter函数的用法"></a>fliter函数的用法</h2><p>filter() 函数用于过滤列表形式的序列，过滤掉不符合条件的元素，返回由符合条件元素组成的新列表，并以迭代器对象的形式返回。<br><code>filter(function, iterable)</code></p>
<ul>
<li>function – 判断函数。</li>
<li>iterable – 可迭代对象。</li>
</ul>
<p>示例</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">过滤出列表中的所有奇数</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_odd</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">return</span> n % <span class="number">2</span> == <span class="number">1</span></span><br><span class="line">newlist = <span class="built_in">filter</span>(is_odd, [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>])</span><br><span class="line"><span class="comment"># 因为返回的是迭代器，所以要打印出迭代器中每个元素</span></span><br><span class="line"><span class="built_in">print</span>(newlist.__next__())</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>study notes</category>
      </categories>
  </entry>
  <entry>
    <title>study-note7</title>
    <url>/2022/06/07/study-note/study-note7/</url>
    <content><![CDATA[<h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><p>在PyTorch中，以下对象可以持久化到硬盘，并能通过相应的方法加载到内存中：</p>
<ul>
<li>Tensor</li>
<li>Variable</li>
<li>nn.Module</li>
<li>Optimizer</li>
</ul>
<p>本质上上述这些信息最终都是保存成Tensor。Tensor的保存和加载十分的简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用的pickle模块，在load时还可将GPU tensor映射到CPU或其它GPU上。</p>
<p>我们可以通过<code>t.save(obj, file_name)</code>等方法保存任意可序列化的对象，然后通过<code>obj = t.load(file_name)</code>方法加载保存的数据。对于Module和Optimizer对象，这里建议保存对应的<code>state_dict</code>，而不是直接保存整个Module/Optimizer对象。Optimizer对象保存的主要是参数，以及动量信息，通过加载之前的动量信息，能够有效地减少模型震荡，下面举例说明。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Module对象的保存与加载</span></span><br><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line">t.save(model.state_dict(), <span class="string">&#x27;squeezenet.pth&#x27;</span>)</span><br><span class="line">model.load_state_dict(t.load(<span class="string">&#x27;squeezenet.pth&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h2 id="文件组织架构"><a href="#文件组织架构" class="headerlink" title="文件组织架构"></a>文件组织架构</h2><p>checkpoints/： 用于保存训练好的模型，可使程序在异常退出后仍能重新载入模型，恢复训练<br>data/：数据相关操作，包括数据预处理、dataset实现等<br>models/：模型定义，可以有多个模型，例如上面的AlexNet和ResNet34，一个模型对应一个文件<br>utils/：可能用到的工具函数，在本次实验中主要是封装了可视化工具<br>config.py：配置文件，所有可配置的变量都集中在此，并提供默认值<br>main.py：主文件，训练和测试程序的入口，可通过不同的命令来指定不同的操作和参数<br>requirements.txt：程序依赖的第三方库<br>README.md：提供程序的必要说明</p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">├── checkpoints/</span><br><span class="line">├── data/</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── dataset.py</span><br><span class="line">│   └── get_data.sh</span><br><span class="line">├── models/</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   ├── AlexNet.py</span><br><span class="line">│   ├── BasicModule.py</span><br><span class="line">│   └── ResNet34.py</span><br><span class="line">└── utils/</span><br><span class="line">│   ├── __init__.py</span><br><span class="line">│   └── visualize.py</span><br><span class="line">├── config.py</span><br><span class="line">├── main.py</span><br><span class="line">├── requirements.txt</span><br><span class="line">├── README.md</span><br></pre></td></tr></table></figure>
<h2 id="关于init-py"><a href="#关于init-py" class="headerlink" title="关于init.py"></a>关于<strong>init</strong>.py</h2><p>可以看到，几乎每个文件夹下都有<strong>init</strong>.py，一个目录如果包含了<strong>init</strong>.py 文件，那么它就变成了一个包（package）。<strong>init</strong>.py可以为空，也可以定义包的属性和方法，但其必须存在，其它程序才能从这个目录中导入相应的模块或函数。例如在data/文件夹下有<strong>init</strong>.py，则在main.py 中就可以<br><code>from data.dataset import DogCat</code></p>
<p>而如果在data/<strong>init</strong>.py中写入<br><code>from .dataset import DogCat</code><br>则在main.py中就可以直接写为：<br><code>from data import DogCat</code></p>
<h2 id="hasattr-getattr-setattr"><a href="#hasattr-getattr-setattr" class="headerlink" title="hasattr,getattr,setattr"></a>hasattr,getattr,setattr</h2><h3 id="hasattr-object-name-函数"><a href="#hasattr-object-name-函数" class="headerlink" title="hasattr(object,name) 函数"></a>hasattr(object,name) 函数</h3><p>判断一个对象里面是否有 name 属性或者 name 方法，返回 bool 值，如果有 name 属性（方法）则返回 True ，否则返回 False 。注意： name 需要使用引号括起来。</p>
<h3 id="getattr-object-name-default-函数"><a href="#getattr-object-name-default-函数" class="headerlink" title="getattr(object,name[,default]) 函数"></a>getattr(object,name[,default]) 函数</h3><p>获取对象 object 的属性或者方法，若存在则打印出来；若不存在，则打印默认值，默认值可选。注意：如果返回的是对象的方法，那么打印的结果是方法的内存地址。如果需要运行这个方法，那么可以在后面添加括号 () 。</p>
<h3 id="setattr-object-name-values-函数"><a href="#setattr-object-name-values-函数" class="headerlink" title="setattr(object,name,values) 函数"></a>setattr(object,name,values) 函数</h3><p>给对象的属性赋值，若属性不存在，则先创建再赋值。</p>
<h2 id="torch进入no-grad-的两种方式"><a href="#torch进入no-grad-的两种方式" class="headerlink" title="torch进入no_grad()的两种方式"></a>torch进入no_grad()的两种方式</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model.<span class="built_in">eval</span>()                                </span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">   <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">eval</span>():</span><br><span class="line">	...</span><br></pre></td></tr></table></figure>
<h2 id="与-区别"><a href="#与-区别" class="headerlink" title="/与../区别"></a>/与../区别</h2><p><a href="https://blog.csdn.net/qq_40967319/article/details/107671625">https://blog.csdn.net/qq_40967319/article/details/107671625</a></p>
<h2 id="nn-Module和nn-functinal中模型的区别"><a href="#nn-Module和nn-functinal中模型的区别" class="headerlink" title="nn.Module和nn.functinal中模型的区别"></a>nn.Module和nn.functinal中模型的区别</h2><p>如果模型有可学习的参数，最好用nn.Module，否则既可以使用nn.functional也可以使用nn.Module，二者在性能上没有太大差异，具体的使用取决于个人的喜好。<strong>如激活函数（ReLU、sigmoid、tanh），池化（MaxPool）等层由于没有可学习参数，则可以使用对应的functional函数代替，而对于卷积、全连接等具有可学习参数的网络建议使用nn.Module</strong>。下面举例说明，如何在模型中搭配使用nn.Module和nn.functional。另外虽然dropout操作也没有可学习操作，但建议还是使用<code>nn.Dropout</code>而不是<code>nn.functional.dropout</code>，因为dropout在训练和测试两个阶段的行为有所差别，使用<code>nn.Module</code>对象能够通过<code>model.eval</code>操作加以区分。</p>
]]></content>
      <categories>
        <category>study notes</category>
      </categories>
  </entry>
  <entry>
    <title></title>
    <url>/2024/06/06/paper-note/RUL/An%20enhanced%20encoder%E2%80%93decoder%20framework%20for%20bearing%20remaining%20useful%20life%20prediction/</url>
    <content><![CDATA[<h2 id="An-enhanced-encoder–decoder-framework-for-bearing-remaining-useful-life-prediction"><a href="#An-enhanced-encoder–decoder-framework-for-bearing-remaining-useful-life-prediction" class="headerlink" title="An enhanced encoder–decoder framework for bearing remaining useful life prediction"></a>An enhanced encoder–decoder framework for bearing remaining useful life prediction</h2><p><strong>2020.8 北航 5.6 2区 Measurement</strong></p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>This paper explores the degradation process of bearings and proposes an enhanced encoder–decoder framework. The framework attempts to construct a decoder with the ability to look back and selectively mine underlying information in the encoder.<br><strong>trigonometric functions and cumulative operation</strong> are employed to enhance the quality of <strong>health indicators.</strong></p>
<h3 id="1introduction"><a href="#1introduction" class="headerlink" title="1introduction"></a>1introduction</h3><p><strong>two types of bearing maintenance strategies</strong> :post-failure maintenance and scheduled maintenance<br> -&gt; <strong>three aproch</strong>:physical model-based methods [2,3], statistical model-based methods, and data-driven methods.<br>-&gt;<strong>two major challenges</strong>:health indicator (HI) construction, and prognostics model selection.</p>
<blockquote>
<p>hi构造的重要性：features with monotonicity tend to bring about better RUL prognostics results. Besides, some original features may fail to reflect the degradation tendency.<br>RNN缺陷：Additionally, the accuracy of existing RNN-based prognostics models decreases when time-series data covers the whole life of the machine, as too long sequence will result in vanishing or exploding gradient problems. On the other hand, short time-series cannot reflect the internal relationship and evolution trend.</p>
</blockquote>
<h3 id="2-Theoretical-framework"><a href="#2-Theoretical-framework" class="headerlink" title="2. Theoretical framework"></a>2. Theoretical framework</h3><div align=center>
<img src="" >
</div>







]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>Encoder–decoder</tag>
        <tag>Trigonometric functions</tag>
        <tag>Cumulative operation</tag>
        <tag>Fitness analysis</tag>
      </tags>
  </entry>
  <entry>
    <title>A deep attention residual neural network-based remaining useful life prediction of machinery</title>
    <url>/2024/06/06/paper-note/RUL/A%20deep%20attention%20residual%20neural%20network-based%20remaining%20useful%20life%20prediction%20of%20machinery/</url>
    <content><![CDATA[<h2 id="A-deep-attention-residual-neural-network-based-remaining-useful-life-prediction-of-machinery"><a href="#A-deep-attention-residual-neural-network-based-remaining-useful-life-prediction-of-machinery" class="headerlink" title="A deep attention residual neural network-based remaining useful life prediction of machinery"></a>A deep attention residual neural network-based remaining useful life prediction of machinery</h2><p><strong>2021 东北大学 5.6 2区 measurement</strong></p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>这篇论文提出了一种新颖的深度注意力残差神经网络（DARNN）用于机械剩余寿命（RUL）的预测。DARNN可以有效提取信号中的降解特征，并且在预测性能和自稳定性上显著优于现有方法。</p>
<h3 id="1introduction"><a href="#1introduction" class="headerlink" title="1introduction"></a>1introduction</h3><p>PHM -&gt; Model-based approaches -&gt; The AI-based approach -&gt; deep learning (DL) -&gt; RNN models for RUL estimation -&gt; Although the above research has achieved great success, the following <strong>limitations</strong> still exist. (1) The performance of the data-driven model depends on a large amount of training data, and the prediction accuracy is still limited by the lack of training samples. (2) In practical application, the established model may be affected by noise, resulting in a serious decline in prediction accuracy.<br>-&gt; <strong>DARNN</strong> is proposed in this paper. The proposed method has the following advantages. (1) The proposed deep attention residual (DAR) module can effectively improve the feature extraction capability of the model and improve the prediction performance of the model. (2) The Bayesian approximation technique is applied to the established model to turn point forecasting into interval forecasting and improve the self-stability of the proposed model.</p>
<h3 id="2-Proposed-DARNN-model"><a href="#2-Proposed-DARNN-model" class="headerlink" title="2. Proposed DARNN model"></a>2. Proposed DARNN model</h3><div align=center>
<img src="https://pic.imgdb.cn/item/657f00b2c458853aef1653a0.png" >
</div>

<h4 id="2-1-DAR-module"><a href="#2-1-DAR-module" class="headerlink" title="2.1. DAR module"></a>2.1. DAR module</h4><div align=center>
<img src="https://pic.imgdb.cn/item/657f00b3c458853aef1654de.png" >
</div>

<p>2.1.1. Temporal convolution<br>Both causal convolution operation as well as dilation rate strategy are used in the temporal convolution to improve the representative extraction capability of the established DL model.<br>(1) Causal convolution:<br>(2) Dilation rate strategy:</p>
<p>2.1.2. PReLU activation function<br>‘‘Dead ReLU Problem’’ for using the ReLU activation function, i.e., during the process of training, when the weighted inputs of neurons are negative, outputs of neurons become 0 consistently</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/657f00b3c458853aef165522.png" >
</div>

<p>2.1.3. Dual attention mechanism</p>
<p>(1) Channel attention mechanism<br>The channel attention mechanism [28,29] is utilized to judge which channels are more informative, and an attention weights are given to each channel according to their importance.<br>(2) Temporal attention mechanism<br>Similar to the principle of channel attention mechanism, temporal attention mechanism is capable of aggregating some crucial information in the time dimension of feature maps.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/657f00b3c458853aef165637.png" >
</div>

<p>2.1.4. BayesIan approximation technique (Dropout)</p>
<p>To improve the self-stability of model, the Bayesian approximation technique is adopted in our DARNN to reduce the interference from environmental noise and turn point estimation into interval estimation</p>
<h4 id="2-2-Deep-domain-adaption"><a href="#2-2-Deep-domain-adaption" class="headerlink" title="2.2. Deep domain adaption"></a>2.2. Deep domain adaption</h4><p>In order to solve these two problems and improve the generalization ability of the established model, maximum mean discrepancy is applied to realize deep domain adaption.</p>
<h3 id="3-Case-study-1-FEMTO-dataset"><a href="#3-Case-study-1-FEMTO-dataset" class="headerlink" title="3. Case study 1: FEMTO dataset"></a>3. Case study 1: FEMTO dataset</h3><h4 id="3-1-Data-description"><a href="#3-1-Data-description" class="headerlink" title="3.1. Data description"></a>3.1. Data description</h4><p>z-score</p>
<h4 id="3-2-Prognostic-metrics"><a href="#3-2-Prognostic-metrics" class="headerlink" title="3.2. Prognostic metrics"></a>3.2. Prognostic metrics</h4><ol>
<li>RMSE</li>
<li>score</li>
<li>CRA is also used to illustrate performance of models. The closer the calculated CRA value is to 1, the smaller the relative error between the real values and the predicted values is. CRA is defined as:</li>
</ol>
<div align=center>
<img src="https://pic.imgdb.cn/item/657f00b2c458853aef165213.png" >
</div>

<h4 id="3-3-Hyperparameters-selection-of-DARNN"><a href="#3-3-Hyperparameters-selection-of-DARNN" class="headerlink" title="3.3. Hyperparameters selection of DARNN"></a>3.3. Hyperparameters selection of DARNN</h4><p>Rectified Adam (RAdam) Optimizer ,early stopping and gradient cropping<br>to promote the convergence of the model, the RUL label corresponding to the data is converted into RUL percentage for training.</p>
<h4 id="3-4-Ablation-studies"><a href="#3-4-Ablation-studies" class="headerlink" title="3.4. Ablation studies"></a>3.4. Ablation studies</h4><p>(1) Effectiveness of dual attention block:<br>a model without dual attention mechanism,</p>
<p>(2) Effectiveness of temporal convolution: Meanwhile, a model with traditional convolution is also established,</p>
<h4 id="3-5-Experiment-results-and-discussion"><a href="#3-5-Experiment-results-and-discussion" class="headerlink" title="3.5. Experiment results and discussion"></a>3.5. Experiment results and discussion</h4><p>3.5.1. Effectiveness of the proposed DARNN<br>the absolute value of Pearson correlation coefficient is adopted to evaluate the correlation between features and predicted values. The Pearson correlation coefficient is defined as</p>
<p>3.5.2. Comparison and discussion</p>
<p>3.5.3. Self-stability of the proposed DARNN<br>Signal–Noise ratio (SNR) is usually used to describe the level of noise in the signal, which is defined as</p>
<p><div align=center>
<img src="https://pic.imgdb.cn/item/657f00e0c458853aef1719f3.png" >
</div><br>where 𝑃𝑆 and 𝑃𝑁 denote the power of signal and noise respectively. The lower the SNR, the larger the random noise contained in the data, e.g., when the SNR equals 0 dB, the power of noise is equivalent to that of the original signal. In the experiment, signals with different levels of additive white Gaussian noise from −2 dB to 8 dB were used as noisy data.</p>
<h3 id="4-Case-study-2-CAMPSS-dataset"><a href="#4-Case-study-2-CAMPSS-dataset" class="headerlink" title="4. Case study 2: CAMPSS dataset"></a>4. Case study 2: CAMPSS dataset</h3><p>4.1. Dataset description and preprocessing<br>preprocess the training data set and the test data set by using min–max normalization and time window strategy, and the size of time window is 30.</p>
<p>4.2. Experiment results and discussion</p>
<h3 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5. Conclusion"></a>5. Conclusion</h3><p>(1) In this paper, a DAR module is proposed to improve the representations extraction capability and prediction performance. Also, the Bayesian approximation technique is also applied to the proposed DARNN model to improve the self-stability of the DARNN model. (2) To demonstrate the effectiveness and superiority of the proposed DARNN, we organized two case study experiments. In the first case study, rolling element bearing dataset is adopted to illustrate the effectiveness and self-stability of the proposed DARNN model. In the second experiment, the famous CMAPSS dataset is utilized to further validate the effectiveness of the proposed model by making a comparison with the four state-of-art methods. The experimental results demonstrate the proposed model actually has better performance compared with other four data driven methods.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>Dual attention mechanism，Resnet</tag>
      </tags>
  </entry>
  <entry>
    <title>Deep separable convolutional network for remaining useful life prediction of machinery</title>
    <url>/2024/06/06/paper-note/RUL/Deep%20separable%20convolutional%20network%20for%20remaining%20useful%20life%20prediction%20of%20machinery/</url>
    <content><![CDATA[<h2 id="Deep-separable-convolutional-network-for-remaining-useful-life-prediction-of-machinery"><a href="#Deep-separable-convolutional-network-for-remaining-useful-life-prediction-of-machinery" class="headerlink" title="Deep separable convolutional network for remaining useful life prediction of machinery"></a>Deep separable convolutional network for remaining useful life prediction of machinery</h2><p><strong>2019 西安交通大学 8.4 1区 Mechanical Systems and Signal Processing</strong></p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>These deep learning-based prognostics approaches, however, have the following weaknesses: 1) Their prediction performance largely depends on the hand-crafted feature design. 2) The correlations of different sensor data are not explicitly considered in representation learning.</p>
<h3 id="1-introduction"><a href="#1-introduction" class="headerlink" title="1 introduction"></a>1 introduction</h3><ul>
<li>With the advancement of sensor technologies, telecommunications and computer science, industrial internet of things has been widely applied into health monitoring of machinery in recent years, and a considerable number of monitoring data are acquired by multiple sensors during machine operation.</li>
<li>In general,<strong>data-driven RUL prediction</strong> consists of four major processes: data acquisition, feature extraction and selection, degradation behavior learning and RUL estimation<br>Recently, <strong>deep learning</strong> [13] is gaining more and more attention in data-driven RUL prediction. </li>
<li>Although deep learning has achieved promising results in RUL prediction of machinery, existing prognostics approaches suffer from the following limitations. 1) Their prediction performance largely depends on the hand-crafted feature design in RUL prediction of many machines 2) Current deep prognostics models do not explicitly consider the correlations of different sensor data in representation learning.</li>
<li>To deal with the above-mentioned limitations, a new deep prognostics network, named deep separable convolutional network (DSCN), is proposed for RUL prediction of machinery in this paper.</li>
</ul>
<h3 id="2-preliminary"><a href="#2-preliminary" class="headerlink" title="2 preliminary"></a>2 preliminary</h3><h4 id="2-1Convolutional-networks"><a href="#2-1Convolutional-networks" class="headerlink" title="2.1Convolutional networks"></a>2.1Convolutional networks</h4><p>1) Convolutional layer.<br>2) Pooling layer.<br>3) Fully-connected layer.</p>
<h4 id="2-2-Residual-connections-for-convolutional-networks"><a href="#2-2-Residual-connections-for-convolutional-networks" class="headerlink" title="2.2. Residual connections for convolutional networks"></a>2.2. Residual connections for convolutional networks</h4><p>Residual connections are first proposed by He et al. in [27], aiming at reducing the training complexity of convolutional networks and enabling them to be substantially deeper.</p>
<h3 id="3-Proposed-DSCN-for-RUL-prediction-of-machinery"><a href="#3-Proposed-DSCN-for-RUL-prediction-of-machinery" class="headerlink" title="3 Proposed DSCN for RUL prediction of machinery"></a>3 Proposed DSCN for RUL prediction of machinery</h3><h4 id="3-1-Separable-convolutions"><a href="#3-1-Separable-convolutions" class="headerlink" title="3.1. Separable convolutions"></a>3.1. Separable convolutions</h4><p>multi-channel time-series data are usually used as the inputs of deep prognostics models, where each channel represents one sensor sequence,aiming to effectively model interrelationships of different sensor data by decoupling temporal correlations and cross-channel correlations.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/657eb7f1c458853aefe27d21.png" >
</div>

<p>代码实现</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Separable_conv</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_ch, out_ch</span>):</span><br><span class="line">        <span class="built_in">super</span>(Separable_conv, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.channelwise_conv = nn.Conv1d(in_channels=in_ch,</span><br><span class="line">                                          out_channels=in_ch,</span><br><span class="line">                                          kernel_size=<span class="number">8</span>,</span><br><span class="line">                                          stride=<span class="number">1</span>,</span><br><span class="line">                                          groups=in_ch)</span><br><span class="line">        <span class="comment">#在创建conv1d时指定groups数量可以设定卷积核数量实现separable_conv</span></span><br><span class="line">        self.point_conv = nn.Conv1d(in_channels=in_ch,</span><br><span class="line">                                    out_channels=out_ch,</span><br><span class="line">                                    kernel_size=<span class="number">1</span>,</span><br><span class="line">                                    stride=<span class="number">1</span>,</span><br><span class="line">                                    padding=<span class="number">0</span>,</span><br><span class="line">                                    groups=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,<span class="built_in">input</span></span>):</span><br><span class="line">        <span class="built_in">input</span> = F.pad(<span class="built_in">input</span>=<span class="built_in">input</span>, pad=(<span class="number">4</span>,<span class="number">3</span>), mode=<span class="string">&#x27;constant&#x27;</span>, value=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># print(input.shape)</span></span><br><span class="line">        out = self.channelwise_conv(<span class="built_in">input</span>)</span><br><span class="line">        out = self.point_conv(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></table></figure>
<p><strong>conv1d中groups作用</strong></p>
<ul>
<li>groups: 控制输入和输出之间的连接， group=1，输出是所有的输入的卷积；group=2，此时相当于有并排的两个卷积层，每个卷积层计算输入通道的一半，并且产生的输出是输出通道的一半，随后将这两个输出连接起来。</li>
</ul>
<p>groups=1时与标准卷积一致，具体作用参考<a href="https://zhuanlan.zhihu.com/p/616003771">1</a></p>
<h4 id="3-2-Feature-response-recalibrations"><a href="#3-2-Feature-response-recalibrations" class="headerlink" title="3.2. Feature response recalibrations"></a>3.2. Feature response recalibrations</h4><p>the process of feature response recalibrations is comprised of two steps, including squeeze operation and excitation operation.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/657eb7f1c458853aefe27e00.png" >
</div>

<h4 id="3-3-Separable-convolutional-building-blocks"><a href="#3-3-Separable-convolutional-building-blocks" class="headerlink" title="3.3. Separable convolutional building blocks"></a>3.3. Separable convolutional building blocks</h4><div align=center>
<img src="https://pic.imgdb.cn/item/657eb7f2c458853aefe27e92.png" >
</div>

<p><strong>pre-activate</strong><br>BN and ReLU are added after convolution operations in the standard convolutional networks, but this postactivation strategy may not take full advantage of the benefits of BN in the convolutional networks with residual connections [33].the pre-activation strategy in [33] is also used to improve the regularization of DSCN so as to relieve the problem of overfitting.<br><strong>bn</strong><br>the use of BN is to make the input distribution of each separable convolution more stable and reduce the overfitting [34]. Further, BN is able to allow DSCN to use much higher learning rates and to be insensitive to the parameter initialization.<br><strong>identity skip connection</strong><br>kernel size为1<br>stride为4，为了与另一条conv输出特征一样的conv1d</p>
<h4 id="3-4-Architecture-of-the-proposed-DSCN"><a href="#3-4-Architecture-of-the-proposed-DSCN" class="headerlink" title="3.4. Architecture of the proposed DSCN"></a>3.4. Architecture of the proposed DSCN</h4><div align=center>
<img src="https://pic.imgdb.cn/item/657eb7f1c458853aefe27b76.png" >
</div>

<p>1) The representation learning sub-network firstly uses a separable convolutional layer to convolute the input sensor data，$\rightarrow$ an average pooling layer is employed to conduct down-sampling $\rightarrow$  forwarded to succeeding separable convolutional building blocks to obtain higher-level representations<br>2) The RUL estimation sub-network utilizes a global average pooling layer to receive the high-level representations from the representation learning sub-network,</p>
<h3 id="4-Experimental-verification"><a href="#4-Experimental-verification" class="headerlink" title="4. Experimental verification"></a>4. Experimental verification</h3><h4 id="4-1-Case-study-1-XJTU-SU-dataset"><a href="#4-1-Case-study-1-XJTU-SU-dataset" class="headerlink" title="4.1. Case study 1: XJTU-SU dataset"></a>4.1. Case study 1: XJTU-SU dataset</h4><p>4.1.1. Data description</p>
<p>4.1.2. Evaluation metrics</p>
<p>1) Scoring function<br>2) RMSE</p>
<p>4.1.3. Data preprocessing</p>
<p>1) Normalization<br>2) Time window embedding</p>
<p><strong>4.1.4. Configuration of DSCN</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/657eb7f1c458853aefe27bf7.png" >
</div>

<p>hyperparameters are determined by performing 4-fold cross-validation on the training datasets</p>
<p>the input to net is (batch_size,chanel,time_windows_lengh*time_sequence_length)<br>(128,2,5*2560)<br>the out put of net is (128)</p>
<p><strong>4.1.5. Experimental results</strong></p>
<p>1) Analysis of time window embedding<br>the time window embedding is able to gather more useful degradation information<br>2) Effect of dimensionality reduction ratio and network depth<br>The dimensionality reduction ratio r is a critical hyperparameter in the SE units, whose size may affect the capacity and computational cost of SE units.<br>3) Benefits of separable convolutions and feature response recalibrations<br>the network with standard convolutions (denoted as standard ConvNet) and the network with separable convolutions (denoted as separable ConvNet). Except for not containing the SE units, these two prognostics networks have the same architecture and hyperparameter settings as the proposed DSCN.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/657eb835c458853aefe37206.png" >
</div>

<p>4) Comparison with state-of-the-art prognostics approaches<br>SVM [9], DBN [24], multi-scale CNN (MCNN) [25] and convolutional LSTM (CLSTM) [26].</p>
<h4 id="4-2-Case-study-2-Public-C-MAPSS-datasets"><a href="#4-2-Case-study-2-Public-C-MAPSS-datasets" class="headerlink" title="4.2. Case study 2: Public C-MAPSS datasets"></a>4.2. Case study 2: Public C-MAPSS datasets</h4><p>4.2.1. C-MAPSS datasets</p>
<p><strong>4.2.2. Prognostics results, comparison and discussion</strong></p>
<p>using min-max scaling [28]. In addition, a time window of size 30.piece-wise linear degradation assumption,the same configuration tabulated in Table 2 except for the time window size<br>the input of net is (batch_size,num_sensor,time_windows_lengh*time_sequence_length)<br>(128,14,30*1)</p>
<h3 id="5-Conclusions"><a href="#5-Conclusions" class="headerlink" title="5. Conclusions"></a>5. Conclusions</h3><p>引入可分离卷积操作来代替标准卷积操作，以有效地建模不同传感器数据之间的相互关系。同时，为了提高对信息特征图的敏感性，构建了一个SE单元来执行自适应特征响应重新校准<br>1）DSCN直接使用原始的多传感器数据作为输入，摆脱了手动特征提取和选择的复杂过程。2）DSCN明确考虑了不同传感器数据之间的相关性。</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>Separable convolutions</tag>
        <tag>Resnet</tag>
        <tag>Squeeze and excitation operations</tag>
        <tag>RUL</tag>
      </tags>
  </entry>
  <entry>
    <title>Hierarchical attention graph convolutional network to fuse multi-sensor signals for remaining useful life prediction</title>
    <url>/2024/06/06/paper-note/RUL/Hierarchical%20attention%20graph%20convolutional%20network%20to%20fuse%20multi-sensor%20signals%20for%20remaining%20useful%20life%20prediction/</url>
    <content><![CDATA[<h2 id="Hierarchical-attention-graph-convolutional-network-to-fuse-multi-sensor-signals-for-remaining-useful-life-prediction"><a href="#Hierarchical-attention-graph-convolutional-network-to-fuse-multi-sensor-signals-for-remaining-useful-life-prediction" class="headerlink" title="Hierarchical attention graph convolutional network to fuse multi-sensor signals for remaining useful life prediction"></a>Hierarchical attention graph convolutional network to fuse multi-sensor signals for remaining useful life prediction</h2><p><strong>2021 西安交通大学 8.1 1区 Reliability Engineering &amp; System Safety</strong></p>
<h3 id="1abstract"><a href="#1abstract" class="headerlink" title="1abstract"></a>1abstract</h3><p>1.过去方法的缺陷：1）传感器间的联系为被考虑。2）更注重对传感器时间依赖性的建模，而忽略空间依赖性。<br>2.文章提出的Hierarchical attention graph convolutional network(HAGCN),包含对spatial dependencies建模的<em>the hierarchical graph representation layer</em>和对tenporal dependencies建模的 <em>bi-directional long short-term memory network</em><br>此外还设计了 <em>regularized self-attention graph pooling</em>对传感器信息的融合</p>
<h3 id="2introduction"><a href="#2introduction" class="headerlink" title="2introduction"></a>2introduction</h3><p>PHM<br>model-driven and data-driven<br>ML and DL in data-driven<br>DL focus more temporal dependencies than spatial and geographic information<br>GCN<br>HAGCN</p>
<h3 id="3preliminary"><a href="#3preliminary" class="headerlink" title="3preliminary"></a>3preliminary</h3><h4 id="problem-definition-of-RUL-prediction-with-GCN"><a href="#problem-definition-of-RUL-prediction-with-GCN" class="headerlink" title="problem definition of RUL prediction with GCN"></a>problem definition of RUL prediction with GCN</h4><p>for a graph G=(V,E,X) and label y<br>X change with time ,with constructd datasetD={(G1,y1),(G2,y2),……},the task of rul can mapping f:G-&gt;y</p>
<h4 id="temporal-dependency-modeling"><a href="#temporal-dependency-modeling" class="headerlink" title="temporal dependency modeling"></a>temporal dependency modeling</h4><p>BiLSTM</p>
<h4 id="spatial-dependency-modeling"><a href="#spatial-dependency-modeling" class="headerlink" title="spatial dependency modeling"></a>spatial dependency modeling</h4><p>by stacking graph convolution layers and graph pooling layers</p>
<p><strong>1.graph isomorphism convolution layer</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/656805ffc458853aef01eea2.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/656805ffc458853aef01ef50.png" >
</div>
GNN中aggregate和combine结合在一起

对于GIN
<div align=center>
<img src="https://pic.imgdb.cn/item/656805ffc458853aef01ef92.png" >
</div>

<p><strong>2.self-attention graph pooling</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/656805f8c458853aef01ddc1.png" >
</div>

<h3 id="4HAGCN"><a href="#4HAGCN" class="headerlink" title="4HAGCN"></a>4HAGCN</h3><h4 id="Regularized-self-attention-graph-pooling-layer"><a href="#Regularized-self-attention-graph-pooling-layer" class="headerlink" title="Regularized self-attention graph pooling layer"></a>Regularized self-attention graph pooling layer</h4><div align=center>
<img src="https://pic.imgdb.cn/item/656805f8c458853aef01dde8.png" >
</div>

<h4 id="Framework-of-HAGCN-for-RUL-prediction"><a href="#Framework-of-HAGCN-for-RUL-prediction" class="headerlink" title="Framework of HAGCN for RUL prediction"></a>Framework of HAGCN for RUL prediction</h4><div align=center>
<img src="https://pic.imgdb.cn/item/656805f4c458853aef01d40d.png" >
</div>

<h3 id="5case-study"><a href="#5case-study" class="headerlink" title="5case study"></a>5case study</h3><h4 id="CMAPSS"><a href="#CMAPSS" class="headerlink" title="CMAPSS"></a>CMAPSS</h4><p><strong>1.overview</strong></p>
<p><strong>2.data preprocessing</strong></p>
<p>按工况进行了数据标准化</p>
<p><strong>3.Constructing spatial-temporal graph datasets</strong></p>
<blockquote>
<p>for every time stamps constructing the graph G=(V,E,X)<br>V vetex set<br>E edge set<br>X 特征矩阵 (M,WL) WL:length of sliding window<br>A 连接矩阵 (2,|E|) |E|:number of edge  [[[[0,1,2],[1,2,3]]]] 代表0与1 ，1与2，,2与3相连 </p>
</blockquote>
<div align=center>
<img src="https://pic.imgdb.cn/item/656805f4c458853aef01d43c.png" >
</div>

<p><strong>input:</strong></p>
<p>X for M sensor use sliding window of length 30<br>A<br>把M个传感器当成M个点，用两个点的Euclidean distance判断两个node是否相连</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/656805f4c458853aef01d37e.png" >
</div>

<p>因为前面做过标准化</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/656805f4c458853aef01d3a3.png" >
</div>

<p>cos(Θ)即代表两个点的余弦相似度</p>
<p><strong>label</strong><br>piece-wise linear degradation</p>
<p><strong>4评价标准</strong><br>RMSE and SF</p>
<p><strong>5模型设置</strong></p>
<p>optimzer:</p>
<center>adam with lr decay</center>

<p>loss：</p>
<script type="math/tex; mode=display">
L{total} = L_{MSE}(ω)+ αL_{att}(Θ)</script><h3 id="6-model-analysis"><a href="#6-model-analysis" class="headerlink" title="6 model analysis"></a>6 model analysis</h3><p><strong>1The influence of prediction interval</strong></p>
<p><strong>2Ablation study and the importance analysis of RSAGPool</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/656805f0c458853aef01ca65.png" >
</div>

<p>Model I without the ability to model the temporal dependencies of the sensor measurements achieves the worst performance in these four models.Model II without the ability to model the spatial dependencies of the multiple sensors performs worse than Model III without ability to achieve hierarchical learning.</p>
<p><strong>3The influence of the attention loss of RSAGPool</strong><br>with the increase of alpha, especially when the training graphs come from a difficult dataset. This may be because the more complex the graph, the more difficult it is for the model to simultaneously focus on the importance of each node and learn the graph representation to make predictions. However, we can still observe that when the value of alpha is 100, HAGCN can achieve the best results on all datasets.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/656805f0c458853aef01cadd.png" >
</div>

<p><strong>4The influence of number of HGRLs</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/656805f0c458853aef01ca03.png" >
</div>


<p><strong>5The graph topology learned by HAGCN</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/656805efc458853aef01c9a7.png" >
</div>


<p>GIN <a href="https://www.cnblogs.com/akaman98/p/17355702.html(详">https://www.cnblogs.com/akaman98/p/17355702.html(详</a>)<br><a href="https://www.jianshu.com/p/3b072a2813cf(略">https://www.jianshu.com/p/3b072a2813cf(略</a>)</p>
<p>SAGpooling <a href="https://blog.csdn.net/yyl424525/article/details/103327551">https://blog.csdn.net/yyl424525/article/details/103327551</a></p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>RUL</tag>
        <tag>GCU</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2024/06/06/paper-note/RUL/Remaining%20Useful%20Life%20(RUL)%20Prediction%20of%20Mechanical%20Bearings%20Using%20Convolution%20Neural%20Network%20(CNN)%20and%20Long%20Short%20Term%20Memory%20(LSTM)/</url>
    <content><![CDATA[<h2 id="Remaining-Useful-Life-RUL-Prediction-of-Mechanical-Bearings-Using-Convolution-Neural-Network-CNN-and-Long-Short-Term-Memory-LSTM"><a href="#Remaining-Useful-Life-RUL-Prediction-of-Mechanical-Bearings-Using-Convolution-Neural-Network-CNN-and-Long-Short-Term-Memory-LSTM" class="headerlink" title="Remaining Useful Life (RUL) Prediction of Mechanical Bearings Using Convolution Neural Network (CNN) and Long Short Term Memory (LSTM)"></a>Remaining Useful Life (RUL) Prediction of Mechanical Bearings Using Convolution Neural Network (CNN) and Long Short Term Memory (LSTM)</h2><p><strong>high technology letters</strong></p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>In this article, PRONOSTIA Dataset for Bearings(PHM IEEE 2012 Data Challenge Dataset) is used. For every 10 seconds, vibration signals are recorded for 0.1 seconds in the dataset. The recorded signals are 1D raw vibration signals(Time-Domain). The 1D signals are difﬁcult to analyze, so 1D vibration signals are transformed into the 2D image-like features(Time and Frequency-Domain) using CWT(Continuous Wavelet Transform). And CNN(Convolution Neural Network) + LSTM(Long Short-Term Memory) is applied to the acquired features from CWT to calculate the HI. The calculated HI is then used for RUL Prediction.</p>
<h3 id="keys"><a href="#keys" class="headerlink" title="keys"></a>keys</h3><p>We employed Data Normalization technique after applying CWT to the 1D collected data to normalize the coefﬁcients of 2D CWT data. To achieve Data Normalization we performed Data Re-scaling method A.K.A min max normalization to re-scale or adjust all the 2D data</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">WIN_SIZE=<span class="number">20</span></span><br><span class="line">DATA_POINTS_PRE_FILE=<span class="number">2560</span></span><br><span class="line">WAVELET_TYPE=<span class="string">&#x27;morl&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_feature_image</span>(<span class="params">ind, feature_name=<span class="string">&#x27;horiz accel&#x27;</span></span>):</span><br><span class="line">    data_range = df_row_ind_to_data_range(ind)</span><br><span class="line">    data = df[feature_name].values[data_range[<span class="number">0</span>]:data_range[<span class="number">1</span>]]</span><br><span class="line">    <span class="comment"># use window to process(= prepare, develop) 1D signal</span></span><br><span class="line">    data = np.array([np.mean(data[i:i+WIN_SIZE]) <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, DATA_POINTS_PER_FILE, WIN_SIZE)])</span><br><span class="line">    <span class="comment">#data (128,1)</span></span><br><span class="line">    <span class="comment"># perform CWT on 1D data(= 1D array)</span></span><br><span class="line">    coef, _ = pywt.cwt(data, np.linspace(<span class="number">1</span>,<span class="number">128</span>,<span class="number">128</span>), WAVELET_TYPE)</span><br><span class="line">    <span class="comment">#coef (128,128)</span></span><br><span class="line">    <span class="comment"># transform to power and apply logarithm?!</span></span><br><span class="line">    coef = np.log2(coef**<span class="number">2</span>+<span class="number">0.001</span>)</span><br><span class="line">    <span class="comment"># normalize coef</span></span><br><span class="line">    coef = (coef - coef.<span class="built_in">min</span>())/(coef.<span class="built_in">max</span>() - coef.<span class="built_in">min</span>()) </span><br><span class="line">    <span class="keyword">return</span> coef</span><br></pre></td></tr></table></figure>
<ul>
<li>for bearing1_1 the data finally be (2803, 2, 128, 128)</li>
<li>the scale of cwt decide the size of coef which also means the scale of the wavelet</li>
</ul>
<p>todo</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>2DCNN，LSTM，CWT，RUL</tag>
      </tags>
  </entry>
  <entry>
    <title>Remaining useful life estimation via transformer encoder enhanced by a gated convolutional unit</title>
    <url>/2024/06/06/paper-note/RUL/Remaining%20useful%20life%20estimation%20via%20transformer%20encoder%20enhanced%20by%20a%20gated%20convolutional%20unit/</url>
    <content><![CDATA[<h2 id="Remaining-useful-life-estimation-via-transformer-encoder-enhanced-by-a-gated-convolutional-unit"><a href="#Remaining-useful-life-estimation-via-transformer-encoder-enhanced-by-a-gated-convolutional-unit" class="headerlink" title="Remaining useful life estimation via transformer encoder enhanced by a gated convolutional unit"></a>Remaining useful life estimation via transformer encoder enhanced by a gated convolutional unit</h2><h3 id="Journal-of-Intelligent-Manufacturing-8-3-1区"><a href="#Journal-of-Intelligent-Manufacturing-8-3-1区" class="headerlink" title="Journal of Intelligent Manufacturing 8.3 1区"></a>Journal of Intelligent Manufacturing 8.3 1区</h3><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>使用的数据：C-MAPSS数据集。<br>创新点：论文使用了基于深度神经网络的新方法，采用了Transformer编码器作为模型主体，并结合点积自注意力机制和门控卷积单元，以捕捉时间序列中的短期和长期依赖关系。与传统的卷积神经网络和循环神经网络方法相比，该模型克服了核大小和顺序依赖性的限制，同时充分利用了并行计算的优势。在实验中，该模型在C-MAPSS数据集上展现出优于或与其他现有方法相当的性能，同时通过消融研究证明了所使用组件的必要性和有效性。</p>
<h2 id="论文结构"><a href="#论文结构" class="headerlink" title="论文结构"></a>论文结构</h2><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>该部分介绍了剩余寿命（RUL）估计的研究背景和传统基于物理模型和数据驱动模型的方法。</p>
<h3 id="related-work"><a href="#related-work" class="headerlink" title="related work"></a>related work</h3><p>introduces the related work on RUL estimation and backbone of the proposed method </p>
<p>Deep learning based RUL estimation</p>
<p>Transformer architecture</p>
<h3 id="Methodology"><a href="#Methodology" class="headerlink" title="Methodology"></a>Methodology</h3><p>describe the proposed model structure</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/655caee7c458853aef2fc757.png" >
</div>

<h4 id="1-Encoder-Layer"><a href="#1-Encoder-Layer" class="headerlink" title="1.Encoder Layer"></a>1.Encoder Layer</h4><h5 id="Multi-head-attention"><a href="#Multi-head-attention" class="headerlink" title="Multi-head attention"></a>Multi-head attention</h5><p>重点是把q，k，v分成H份，移到前面的维度。使计算权重系数a时为每个头分别计算。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, heads, d_model, dropout=<span class="number">0.5</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line"></span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.d_k = d_model // heads</span><br><span class="line">        self.h = heads</span><br><span class="line"></span><br><span class="line">        self.q_linear = nn.Linear(d_model, d_model)</span><br><span class="line">        self.v_linear = nn.Linear(d_model, d_model)</span><br><span class="line">        self.k_linear = nn.Linear(d_model, d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.out = nn.Linear(d_model, d_model)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, q, k, v, mask=<span class="literal">None</span></span>):</span><br><span class="line">        bs = q.size(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># perform linear operation and split into h heads</span></span><br><span class="line"></span><br><span class="line">        k = self.k_linear(k).view(bs, -<span class="number">1</span>, self.h, self.d_k)</span><br><span class="line">        q = self.q_linear(q).view(bs, -<span class="number">1</span>, self.h, self.d_k)</span><br><span class="line">        v = self.v_linear(v).view(bs, -<span class="number">1</span>, self.h, self.d_k)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># transpose to get dimensions bs * h * sl * d_model</span></span><br><span class="line"></span><br><span class="line">        k = k.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        q = q.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        v = v.transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># calculate attention using function we will define next</span></span><br><span class="line">        scores = attention(q, k, v, self.d_k, mask, self.dropout)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># concatenate heads and put through final linear layer</span></span><br><span class="line">        concat = scores.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous() \</span><br><span class="line">            .view(bs, -<span class="number">1</span>, self.d_model)</span><br><span class="line">        <span class="comment"># scores.trabspose().contiguous() 使scores不会被改变</span></span><br><span class="line">        output = self.out(concat)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">attention</span>(<span class="params">q, k, v, d_k, mask=<span class="literal">None</span>, dropout=<span class="literal">None</span></span>):</span><br><span class="line">    scores = torch.matmul(q, k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(d_k)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        mask = mask.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    <span class="comment"># scores = scores.masked_fill(mask == 0, -1e9)</span></span><br><span class="line">    scores = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> dropout <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        scores = dropout(scores)</span><br><span class="line"></span><br><span class="line">    output = torch.matmul(scores, v)</span><br><span class="line">    <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<h5 id="Feed-forward-network"><a href="#Feed-forward-network" class="headerlink" title="Feed forward network"></a>Feed forward network</h5><p>FC</p>
<h4 id="2-Local-featrue-extraction-layer"><a href="#2-Local-featrue-extraction-layer" class="headerlink" title="2.Local featrue extraction layer"></a>2.Local featrue extraction layer</h4><h5 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h5><p>Conv2d</p>
<h5 id="Gating-mechanism"><a href="#Gating-mechanism" class="headerlink" title="Gating mechanism"></a>Gating mechanism</h5><h5 id="Liner-mapping"><a href="#Liner-mapping" class="headerlink" title="Liner mapping"></a>Liner mapping</h5><h5 id="Position-Encoding"><a href="#Position-Encoding" class="headerlink" title="Position Encoding"></a>Position Encoding</h5><div align=center>
<img src="https://pic.imgdb.cn/item/655caee7c458853aef2fc660.png" >
</div>


<h4 id="3-Regression-layer"><a href="#3-Regression-layer" class="headerlink" title="3.Regression layer"></a>3.Regression layer</h4><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><h4 id="settings"><a href="#settings" class="headerlink" title="settings"></a>settings</h4><h5 id="Benchmark-Datasets："><a href="#Benchmark-Datasets：" class="headerlink" title="Benchmark Datasets："></a>Benchmark Datasets：</h5><p>C-MAPSS</p>
<h5 id="Data-pre-processing："><a href="#Data-pre-processing：" class="headerlink" title="Data pre-processing："></a>Data pre-processing：</h5><p><strong><em>sensor selection</em></strong><br>More specifically, the monotonicity matrix and correlation matrix are used for sensor selection. The monotonicity matrix represents the tendency of features along the time dimension, the feature dimensions with monotonicity contain more abundant degeneration information. The correlation matrix reflects the relationship between feature dimension and operation time.<br>reason:数据集共有26个传感器，但不是所以传感器都有与RUL相关的信息，因此根据论文[1]进行传感器选择。主要计算相关性矩阵和单调矩阵（？）</p>
<p><strong><em>Normalization</em></strong><br>min-max normalization</p>
<p><strong><em>Piece-wise RUL</em></strong><br>we first label RULs at the beginning with a constant RU Lmax, i.e., the horizon dot line. After running for a period of time, the system begins to enter a phase of linear degradation until it fails.<br>reason:the system degradation is negligible at the early stage of its life cycle</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/655caee7c458853aef2fc7f1.png" >
</div>

<h5 id="Implementation-details"><a href="#Implementation-details" class="headerlink" title="Implementation details"></a>Implementation details</h5><p>参数设置</p>
<h5 id="Evaluation-metric"><a href="#Evaluation-metric" class="headerlink" title="Evaluation metric"></a>Evaluation metric</h5><p>RMSE</p>
<h4 id="Performers-comparison"><a href="#Performers-comparison" class="headerlink" title="Performers comparison"></a>Performers comparison</h4><h4 id="Ablation-study"><a href="#Ablation-study" class="headerlink" title="Ablation study"></a>Ablation study</h4><p>1.w/o GCU<br>2.w/o GCU w/o sigmoid</p>
<h3 id="conclusion"><a href="#conclusion" class="headerlink" title="conclusion"></a>conclusion</h3><h2 id="code"><a href="#code" class="headerlink" title="code"></a>code</h2><blockquote>
<p>baseline only for FD001<br>( <a href="https://github.com/jiaxiang-cheng/PyTorch-Transformer-for-RUL-Prediction">https://github.com/jiaxiang-cheng/PyTorch-Transformer-for-RUL-Prediction</a>)</p>
<p>more complicate realize<br>(<a href="https://github.com/GuoHaoren/Implementation-of-GCU-Transformer-for-RUL-Prediction-on-CMAPSS">https://github.com/GuoHaoren/Implementation-of-GCU-Transformer-for-RUL-Prediction-on-CMAPSS</a>)</p>
</blockquote>
<p>[1]:Wu, Q., Ding, K., &amp; Huang, B. (2018). Approach for fault prognosis using recurrent neural network. Journal of Intelligent Manufacturing, 1–13.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>RUL</tag>
        <tag>Transformer Encoder</tag>
        <tag>GRU</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>/2024/06/06/paper-note/RUL/layout/</url>
    <content><![CDATA[<h2 id=""><a href="#" class="headerlink" title=" "></a> </h2><p><strong>2021 西安交通大学 8.1 1区 Reliability Engineering &amp; System Safety</strong></p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><h3 id="1introduction"><a href="#1introduction" class="headerlink" title="1introduction"></a>1introduction</h3><h3 id="2preliminary"><a href="#2preliminary" class="headerlink" title="2preliminary"></a>2preliminary</h3><div align=center>
<img src="" >
</div>







]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Variational encoding approach for interpretable assessment of remaining useful life estimation</title>
    <url>/2024/06/06/paper-note/RUL/transformer/Variational%20encoding%20approach%20for%20interpretable%20assessment%20of%20remaining%20useful%20life%20estimation/</url>
    <content><![CDATA[<h2 id="Variational-encoding-approach-for-interpretable-assessment-of-remaining-useful-life-estimation"><a href="#Variational-encoding-approach-for-interpretable-assessment-of-remaining-useful-life-estimation" class="headerlink" title="Variational encoding approach for interpretable assessment of remaining useful life estimation"></a>Variational encoding approach for interpretable assessment of remaining useful life estimation</h2><p><strong>2022.1 西班牙 8.1 1区 Reliability Engineering &amp; System Safety</strong></p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p><strong>most of them lack an explanatory component to understand model learning and/or the nature of the data. To overcome this gap we propose a novel approach based on variational encoding.</strong>The model consists of a recurrent encoder and a regression model: the encoder learns to compress the input data to a latent space that serves as a basis to build a self-explanatory map that can visually evaluate the rate of deterioration of aircraft engines. <strong>Obtaining such a latent space is regularized by a new cost function guided by variational inference and a term that penalizes prediction errors.</strong></p>
<h3 id="1introduction"><a href="#1introduction" class="headerlink" title="1introduction"></a>1introduction</h3><ul>
<li>model-based and data-driven approaches </li>
<li>the use of machine learing models</li>
<li>the use of cnn and rnn(there is a clear gap between all Deep Learning oriented approaches: although they do achieve remarkable results, models are treated as black boxes where inputs are used to obtain some output) </li>
<li>To meet the goals stated one can think of unsupervised learning techniques as a possible way to approach this. Especially, when it comes to reveal insights about the nature of the data, Representation Learning approaches such as autoencoders come in handy</li>
<li>Variational Autoencoders (VAEs).(In VAEs, variational inference is added to the error function through the Kullback–Leibler term, which guarantees that data with similar patterns will also be encoded nearby in the latent space. VAEs are a recent but well-known alternative with numerous applications in anomaly analysis)</li>
<li>This problem has many points in common with the diagnosis of anomalies and has also been solved by using autoencoders [36,37]. Despite these similarities, both problems have a fundamental difference: in anomaly diagnosis, the aim is to look for individuals in unlikely areas of the latent space. In RUL prediction, on the contrary, the objective for a complete and interpretable diagnosis should be to project the evolution of the system in the latent space</li>
</ul>
<h3 id="2preliminary"><a href="#2preliminary" class="headerlink" title="2preliminary"></a>2preliminary</h3><p>2.1 piece-wise RUL</p>
<p>2.2 metric<br>RMSE and Score</p>
<h3 id="3model"><a href="#3model" class="headerlink" title="3model"></a>3model</h3><h4 id="3-1model1"><a href="#3-1model1" class="headerlink" title="3.1model1"></a>3.1model1</h4><p>The proposed model consists of three components: an encoder network, a regression model and a latent space.<br><strong>encode:</strong><br> The encoder learns to compress the data into the latent space so that it is described by the parameters that initialize the probability distribution to which the data belongs. Variational inference is added to the loss function through the <strong>Kullback–Leibler divergence, which measures how much one probability distribution diverges from another, to learn the abovementioned parameters. A second term is also added to penalize wrong estimations of the regression model.</strong><br>Thereby, the main difference with respect to a VAE is that we replace the decoder with a regression model, as shown in Fig. 3, and the training is done differently.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65a5de07871b83018a2c1688.png" >
</div>

<p><strong>regression</strong><br>Among the different types of RNNs that can be found, LSTM networks are the most popular. These networks process data from backward to forward conserving information from the past through hidden states. However,<br>Bidirectional LSTM networks are in high demand because they provide not only information about the past but also about the future: data is first processed from past to future and then from future to past, thus preserving the information from both periods.</p>
<h4 id="3-2-Interpretable-diagnosis"><a href="#3-2-Interpretable-diagnosis" class="headerlink" title="3.2. Interpretable diagnosis"></a>3.2. Interpretable diagnosis</h4><p>The diagnostic tool introduced in this work is a map that shows the actual state of the engine and also the rate of change from healthy to deteriorated</p>
<h3 id="4-Experimental-design"><a href="#4-Experimental-design" class="headerlink" title="4. Experimental design"></a>4. Experimental design</h3><p>preprocess:<br>With this approach, all records of the same operating condition are grouped together and scaled using a standard scaler.(工况标准化)</p>
<p>On the other hand, although sensor data have a general trend, it is known that they are subject to local oscillations, mainly caused by highfrequency sensors, which lead to noise [23,43]. To ease the processing of the series, an exponential weighted moving average is carried out.（输入指数平滑）</p>
<p>In time series problems it is quite recurrent to split the data into sequences for better prediction performance.In those cases a masked value is used and will be treated in the first layer of the model by simply ignoring those values. In this way, as much information as possible is used.（time window）</p>
<p><strong>along with the use of the Hyperopt Bayesian optimization library（hyperparameter 确定）</strong></p>
<p><strong>The choice of the sensors is not arbitrary, we only use the following six: T30, T50, P30, EPRA, PS30 and phi。Moreover, for datasets FD001 and FD003 the EPRA sensor is not necessary since it measures the engine thrust under different operating conditions while FD001 and FD003 operate under the same condition</strong></p>
<h3 id="5-Experimental-results"><a href="#5-Experimental-results" class="headerlink" title="5. Experimental results"></a>5. Experimental results</h3><p>bold. It can be quickly noted that with datasets FD001 and FD003, although the metrics are considered good, they are not the best. However, the interest lies mostly in FD002 and FD004 as the increasing number of operating conditions and failure modes make these two datasets contain more complicated multiscale degradation features.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Dual-Aspect Self-Attention Based on Transformer for Remaining Useful Life Prediction</title>
    <url>/2024/06/06/paper-note/RUL/transformer/Dual-Aspect%20Self-Attention%20Based%20on%20Transformer%20for%20Remaining%20Useful%20Life%20Prediction/</url>
    <content><![CDATA[<h2 id="Dual-Aspect-Self-Attention-Based-on-Transformer-for-Remaining-Useful-Life-Prediction"><a href="#Dual-Aspect-Self-Attention-Based-on-Transformer-for-Remaining-Useful-Life-Prediction" class="headerlink" title="Dual-Aspect Self-Attention Based on Transformer for Remaining Useful Life Prediction"></a>Dual-Aspect Self-Attention Based on Transformer for Remaining Useful Life Prediction</h2><p><strong>2022.12 电子科技大学  arxiv</strong></p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>With the widespread deployment of sensors on industrial equipment, building the Industrial Internet of Things (IIoT) to interconnect these devices has become an inexorable trend in the development of the digital factory.<br>介绍了IIOT</p>
<p>We investigated the mainstream RUL prediction models and summarized the basic steps of RUL prediction modeling in this scenario. On this basis, a data-driven approach for RUL estimation is proposed in this paper.<br>介绍方法</p>
<h3 id="1introduction"><a href="#1introduction" class="headerlink" title="1introduction"></a>1introduction</h3><p>The emergence of low-cost micro-sensors and high-bandwidth wireless networks enables real-time access to machine data. Big data and AI extract valuable insights, enhancing productivity and reducing failure risks. This has led to the rise of Industrial Internet of Things (IIoT), transforming data into a critical resource. A key IIoT application involves using big data for predictive maintenance of complex machinery.</p>
<p>RUL estimation algorithm is the core algorithms for predictive maintenance and PHM systems.</p>
<p>In recent years, data-driven RUL prediction algorithms have received increasing attention.</p>
<p>Traditional machine learning algorithms (such as SVM[11], ANN[12], and DBN[13]) have achieved good results in RUL prediction.</p>
<p><strong>However, these networks all treat the time series of multiple sensors equally, but in fact, different sensors have different contributions to theRUL values.According to where the attention mechanisms are used, they are divided into three types: time weighting, feature (sensor) weighting, and two dimensions weighting together.Furthermore, the basic attention mechanism cannot model the intrinsic connection of features.</strong><br>上述方法中没有考虑不同传感器的重要程度，而对于注意力机制有time weighting和feature weighting and two dimensions weighting together，而传统的注意力机制没有考虑特征的内在联系。</p>
<p>the self-attention mechanism does not require external information and is better at capturing the internal correlations of features. Additionally, the multi-head attention mechanism uses the attention weights of multiple dimensions to aggregate different contributions of features from multiple perspectives.</p>
<h3 id="2preliminary"><a href="#2preliminary" class="headerlink" title="2preliminary"></a>2preliminary</h3><h3 id="2-1-Multi-head-Attention"><a href="#2-1-Multi-head-Attention" class="headerlink" title="2.1 Multi-head Attention"></a>2.1 Multi-head Attention</h3><p>Polosukhin, Attention is all you need (2017）</p>
<h3 id="2-2-proposed-methond"><a href="#2-2-proposed-methond" class="headerlink" title="2.2 proposed methond"></a>2.2 proposed methond</h3><div align=center>
<img src="https://pic.imgdb.cn/item/65a5dd6d871b83018a2add19.png" >
</div>

<h3 id="3-Experiment-Setting"><a href="#3-Experiment-Setting" class="headerlink" title="3. Experiment Setting"></a>3. Experiment Setting</h3><h4 id="3-1DATAset"><a href="#3-1DATAset" class="headerlink" title="3.1DATAset"></a>3.1DATAset</h4><p>CMAPSS dataset</p>
<h4 id="3-2-Data-Pre——Processing"><a href="#3-2-Data-Pre——Processing" class="headerlink" title="3.2 Data Pre——Processing"></a>3.2 Data Pre——Processing</h4><p><strong>piece-wise RUL</strong></p>
<p>refer：<br>Sun, Remaining useful life estimation in prognostics using deep convolution neural networks, Reliability Engineering &amp; System Safety 172 (2018) 1–11</p>
<p>Li, Machine remaining useful life prediction via an attention-based deep learning approach, IEEE Transactions on Industrial Electronics 68 (3) (2021) 2521–2531</p>
<p><strong>cluster and normalization</strong></p>
<p>In the absence of prior expertise, reasonable max-min values for sensors are difficult to determine, so we use z-score normalization,</p>
<p>why norm by cluster：<br>Figure 7.c) illustrates the data colored by working conditions, and 7.d) is the data normalized separately by working conditions, which shows that the data trend with the time step is distinct from the unnormalized data.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65a5dd6e871b83018a2adda1.png" >
</div>

<p><strong>time window</strong></p>
<p>sliding_size=T step=1</p>
<p>对于序列长度小于time window的用第一个数据值对数据pad</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># row number &lt; sequence length, only one sequence</span></span><br><span class="line"><span class="comment"># pad width first time-cycle value</span></span><br><span class="line">all_array.append(np.pad(id_array, ((seq_len - id_array.shape[<span class="number">0</span>], <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>)), <span class="string">&#x27;edge&#x27;</span>))</span><br></pre></td></tr></table></figure>
<h4 id="3-3-setup"><a href="#3-3-setup" class="headerlink" title="3.3 setup"></a>3.3 setup</h4><p><strong>metric</strong></p>
<p>RMSE and score</p>
<p><strong>hyper parameters</strong></p>
<p>T=30<br>use all sensor feature</p>
<p>The Adam[44] algorithm was applied to optimize the proposed model. The learning rate of Adam was set to 0.0002. The number of training batches was set to 128, and the early stop mechanism was employed to stop training after 50 rounds without better results. For the proposed model, the LSTM uses three hidden layers with 100 nodes per layer. The MLP layer has a hidden layer with 100 nodes, the activation function is ReLU[45], and the dropout is set to 0.5. The final output layer uses a single node to predict the RUL value.</p>
<h3 id="4-result-analysis"><a href="#4-result-analysis" class="headerlink" title="4 result analysis"></a>4 result analysis</h3><h4 id="4-1-parameter-study"><a href="#4-1-parameter-study" class="headerlink" title="4.1 parameter study"></a>4.1 parameter study</h4><p><strong>feature head</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65a5dd6e871b83018a2addea.png" >
</div>

<p><strong>time head</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65a5dd6e871b83018a2ade24.png" >
</div>

<p><strong>Impact of RNN Cell</strong></p>
<p>RNN and GRU：Among them, RNN has the worst performance, followed by GRU. Meanwhile, the repeated trials of these two models are not as stable as others.</p>
<p>Bi-LSTM performs better than GRU but worse than LSTM because the bidirectional LSTM increases the model complexity. Further, the Bi-LSTM is intended to infer the following content through the bidirectional relationship of the sequences. In contrast, the signal sequence of the sensor in the RUL estimation scenario is meaningless when reversed. Therefore BiLSTM did not achieve the desired results.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65a5dd6e871b83018a2ade73.png" >
</div>

<p><strong>Impact of Window Size</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65a5ddb6871b83018a2b7810.png" >
</div>

<p><strong>Impact of Piece-Wise RUL</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65a5ddb6871b83018a2b787a.png" >
</div>

<h4 id="4-2-ablation-study"><a href="#4-2-ablation-study" class="headerlink" title="4.2 ablation study"></a>4.2 ablation study</h4><p>Where L denotes the single LSTM network, A denotes the single head attention mechanism on features, F denotes using only multi-head attention on features, and F + T denotes using multi-head attention both on features and sequences.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65a5ddb6871b83018a2b78db.png" >
</div>


<h4 id="4-3-Attention-Scheme-Study"><a href="#4-3-Attention-Scheme-Study" class="headerlink" title="4.3 Attention Scheme Study"></a>4.3 Attention Scheme Study</h4><p>Where P is the proposed model; F and S are basic attention weighted in feature and time dimension, respectively; F+S represents first weighted by features and then weighted by time.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65a5ddb7871b83018a2b7939.png" >
</div>

<h4 id="4-4Case-Study"><a href="#4-4Case-Study" class="headerlink" title="4.4Case Study"></a>4.4Case Study</h4><h4 id="4-5-Interpretability-of-Attention"><a href="#4-5-Interpretability-of-Attention" class="headerlink" title="4.5. Interpretability of Attention"></a>4.5. Interpretability of Attention</h4><h4 id="4-6-Comparison-with-other-work"><a href="#4-6-Comparison-with-other-work" class="headerlink" title="4.6. Comparison with other work"></a>4.6. Comparison with other work</h4><p>Results on C-MAPSS</p>
<p>Results on PHM08</p>
<h3 id="5-Conclution"><a href="#5-Conclution" class="headerlink" title="5 Conclution"></a>5 Conclution</h3>]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Contrastive BiLSTM-enabled Health Representation Learning for Remaining Useful Life Prediction</title>
    <url>/2024/06/06/paper-note/RUL/%E5%AF%B9%E6%AF%94%E5%AD%A6%E4%B9%A0/Contrastive%20BiLSTM-enabled%20Health%20Representation%20Learning%20for%20Remaining%20Useful%20Life%20Prediction/</url>
    <content><![CDATA[<p><strong>2023.12 西安交通大学 8.1 1区 Reliability Engineering &amp; System Safety</strong></p>
<h2 id="1-abstract"><a href="#1-abstract" class="headerlink" title="1.abstract"></a>1.abstract</h2><p>many existing deep learning approaches overlook the inherent ordered relationship between samples in the direct mapping from sliced data to RUL pattern. To capture the faithful and ordered health representation of a given system, a Contrastive Bidirectional LSTM-enabled Health Representation Learning (CBHRL) framework is proposed.</p>
<h2 id="2-contributions"><a href="#2-contributions" class="headerlink" title="2.contributions"></a>2.contributions</h2><ol>
<li>A contrastive BiLSTM-enabled health representation learning framework is proposed. CBHRL ranks the similarity between different samples through supervised contrastive regression loss to construct a more monotonic, smooth and trended health representation, which is beneficial to improve the prediction performance.</li>
<li>To construct the multi-view of degradation data, the series odd-even decomposition (SOED) method is proposed. The proposed data augmentation method can construct two degradation series which share mutual information but distinguish with each other. Additionally, this approach proves beneficial in enhancing the model’s generalization ability and prediction accuracy.</li>
<li>The regression prediction method exhibits more accurately during the majority of lifetime in degradation, while the similarity prediction method shows superior performance in serval cycles before failure and better interpretability.</li>
</ol>
<h2 id="3-proposed-method"><a href="#3-proposed-method" class="headerlink" title="3.proposed method"></a>3.proposed method</h2><p>The SOED is designed to construct multi-view degradation series, enhancing the contrastive learning ability. In health representation learning, the supervised contrastive regression loss is used to optimize the encoder. The supervised contrastive regression loss forces the encoder to extract the continuous health representation.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/66571badd9c307b7e9475cad.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/66571badd9c307b7e9475ce6.png" >
</div>

<h3 id="Supervised-Contrastive-Regression-Loss"><a href="#Supervised-Contrastive-Regression-Loss" class="headerlink" title="Supervised Contrastive Regression Loss"></a>Supervised Contrastive Regression Loss</h3><div align=center>
<img src="https://pic.imgdb.cn/item/66571badd9c307b7e9475d40.png" >
</div>

<h3 id="Series-Odd-Even-Decomposition"><a href="#Series-Odd-Even-Decomposition" class="headerlink" title="Series Odd-Even Decomposition"></a>Series Odd-Even Decomposition</h3><div align=center>
<img src="https://pic.imgdb.cn/item/66571baed9c307b7e9475da5.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/66571be5d9c307b7e94795f1.png" >
</div>

<p>Good multi-view data are expected to have certain mutual information and distinguish with each other. The most common method to build multi-view data is data augmentation. However, the common series data augmentation method is not suitable for degradation series. The common series augmentations method such as warping, flipping will not consider how to keep the tendency of degradation, while other methods such as noise injection is unable to provide adequate differences.</p>
<p><strong>The two sub-series keep part of the detail information and the tendency of the degradation. To remain the sequence length, the linear interpolation is used to fill two sub-series to return the original sequence length.</strong></p>
<h3 id="RUL-Prediction-Method"><a href="#RUL-Prediction-Method" class="headerlink" title="RUL Prediction Method"></a>RUL Prediction Method</h3><div align=center>
<img src="https://pic.imgdb.cn/item/66571be5d9c307b7e94795a8.png" >
</div>

<h2 id="4-Experimental-Study"><a href="#4-Experimental-Study" class="headerlink" title="4. Experimental Study"></a>4. Experimental Study</h2><h3 id="4-1-Case-1-C-MAPSS-dataset"><a href="#4-1-Case-1-C-MAPSS-dataset" class="headerlink" title="4.1. Case 1: C-MAPSS dataset"></a>4.1. Case 1: C-MAPSS dataset</h3><h4 id="Experiment-setup"><a href="#Experiment-setup" class="headerlink" title="Experiment setup"></a>Experiment setup</h4><p>For FD002 and FD004, the SOED is not applied since the degradation tendency is disordered by multi-operating conditions. To generate the sequence samples, the time window sliding method is employed. At each time step, all historical sensor data within time window are collected to create a high-dimensional feature vector, and used as the input for the encoder. In this study, the time window size keeps constant 30 for all sub-datasets.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/66571df1d9c307b7e949a4b2.png" >
</div>

<h4 id="Health-representation-analysis"><a href="#Health-representation-analysis" class="headerlink" title="Health representation analysis"></a>Health representation analysis</h4><div align=center>
<img src="https://pic.imgdb.cn/item/66571df1d9c307b7e949a449.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/66571df1d9c307b7e949a3ff.png" >
</div>

<h4 id="Comparative-study-of-SOED"><a href="#Comparative-study-of-SOED" class="headerlink" title="Comparative study of SOED"></a>Comparative study of SOED</h4><div align=center>
<img src="https://pic.imgdb.cn/item/66571df1d9c307b7e949a3c2.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/66571df1d9c307b7e949a395.png" >
</div>

<h4 id="Effectiveness-of-CBHRL"><a href="#Effectiveness-of-CBHRL" class="headerlink" title="Effectiveness of CBHRL"></a>Effectiveness of CBHRL</h4><div align=center>
<img src="https://pic.imgdb.cn/item/66571d9bd9c307b7e9494fef.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/66571d9bd9c307b7e9494fd1.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/66571d9ad9c307b7e9494f9c.png" >
</div>

<h3 id="4-2-Case-2-PRONOSTIA-dataset"><a href="#4-2-Case-2-PRONOSTIA-dataset" class="headerlink" title="4.2. Case 2: PRONOSTIA dataset"></a>4.2. Case 2: PRONOSTIA dataset</h3><h4 id="Implement-details"><a href="#Implement-details" class="headerlink" title="Implement details"></a>Implement details</h4><p>Therefore, twelve features are extracted from horizontal vibration signals as suggested in [33]. Specifically, serval statistical indicators are extracted from both time and frequency domain. Regarding time-frequency domain, energy ratios are calculated through wavelet package decomposition with three-level Daubechies 4 and 4 sub-bands’ energy ratios are selected as features.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/66571d9ad9c307b7e9494f82.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/66571d9ad9c307b7e9494f32.png" >
</div>]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Enhanced residual convolutional domain adaptation network with CBAM for RUL prediction of cross-machine rolling bearing</title>
    <url>/2024/06/06/paper-note/RUL/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/Enhanced%20residual%20convolutional%20domain%20adaptation%20network%20with%20CBAM%20for%20RUL%20prediction%20of%20cross-machine%20rolling%20bearing/</url>
    <content><![CDATA[<h2 id="Enhanced-residual-convolutional-domain-adaptation-network-with-CBAM-for-RUL-prediction-of-cross-machine-rolling-bearing"><a href="#Enhanced-residual-convolutional-domain-adaptation-network-with-CBAM-for-RUL-prediction-of-cross-machine-rolling-bearing" class="headerlink" title="Enhanced residual convolutional domain adaptation network with CBAM for RUL prediction of cross-machine rolling bearing"></a>Enhanced residual convolutional domain adaptation network with CBAM for RUL prediction of cross-machine rolling bearing</h2><p><strong>2024 苏科大 8.1 1区 Reliability Engineering &amp; System Safety</strong></p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>Most of the existing methods are domain adaptation (DA) based RUL prediction on the same machine with different conditions, but <strong>few on cross-machine</strong>.</p>
<p>DA can cope with the data distribution discrepancy (domain shift) under different machines or other conditions, but <strong>the potential negative transfer will affect the effect of DA and prediction performance.</strong></p>
<p><strong>an enhanced residual convolutional domain adaptation network (ERCDAN)</strong></p>
<ol>
<li>Firstly, the enhanced residual convolutional module (ERCM) is designed for degradation feature extraction from limited data, and with the convolutional block attention module (CBAM) to enhance the extracted features. （特征提取：残差加卷积注意力）</li>
<li>Secondly, the DA module with a collaborative full connection structure and attenuation multi-kernel maximum mean discrepancy is designed <strong>for mitigating negative transfer to effective domain-invariant feature extraction.</strong>（域适应：协同全连接层设计加衰减多核MMD）</li>
<li>Finally, the experimental analysis of cross-machine rolling bearing RUL prediction is conducted on the PHM2012, XJTUSY, and EBFL datasets.</li>
</ol>
<h3 id="1introduction"><a href="#1introduction" class="headerlink" title="1introduction"></a>1introduction</h3><p>相对其他域适应模型的改进</p>
<ol>
<li><p>The above cross-machine RUL prediction methods all use a training set composed of data from multiple bearings to train the prediction model. However, under the practical engineering with the lack of prior operating data, there may be the situation where trainable resources (trainable bearing data) are scarce, which makes it difficult to ensure that sufficient data is available to train the prediction model.(模型的特征提取能力更强，需要的训练样本更少)</p>
</li>
<li><p>by reducing the data distribution discrepancy, its potential negative transfer will affect the effect of domain-invariant feature extraction and prediction performance, so a negative transfer mitigation scheme is designed in ERCDAN.（模型能有效解决负迁移问题）</p>
</li>
</ol>
<h3 id="2-Proposed-RUL-prediction-method"><a href="#2-Proposed-RUL-prediction-method" class="headerlink" title="2. Proposed RUL prediction method"></a>2. Proposed RUL prediction method</h3><h4 id="2-1-Problem-description"><a href="#2-1-Problem-description" class="headerlink" title="2.1. Problem description"></a>2.1. Problem description</h4><h4 id="2-2-Overview"><a href="#2-2-Overview" class="headerlink" title="2.2. Overview"></a>2.2. Overview</h4><div align=center>
<img src="https://pic.imgdb.cn/item/6629aa4f0ea9cb14035d251d.png" >
</div>

<h4 id="2-3-Enhanced-residual-convolutional-domain-adaptation-network"><a href="#2-3-Enhanced-residual-convolutional-domain-adaptation-network" class="headerlink" title="2.3. Enhanced residual convolutional domain adaptation network"></a>2.3. Enhanced residual convolutional domain adaptation network</h4><p>2.3.1. Enhanced residual convolutional module</p>
<p>(1) To use two convolutional layers of Conv-BN to simulate a large channel convolutional layer to improve the network’s feature representation ability.<br>(2) To address the degradation of network performance caused by the deepening of network layers, a residual structure is used </p>
<div align=center>
<img src="https://pic.imgdb.cn/item/6629aa4f0ea9cb14035d2533.png" >
</div>

<p>2.3.2. CBAM attention mechanism</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/6629aa4f0ea9cb14035d2578.png" >
</div>

<p>2.3.3. Unsupervised domain adaptation and negative transfer mitigation</p>
<ol>
<li><p>Unsupervised domain adaptation:<br>多核最大均值差异Multiple Kernel Maximum Mean Discrepancy（MK-MMD）<br><a href="https://blog.csdn.net/qq_42983182/article/details/127430650">https://blog.csdn.net/qq_42983182/article/details/127430650</a></p>
</li>
<li><p>negative transfer mitigation</p>
<ol>
<li>A collaborative full connection structure is designed between the feature extractor, RUL predictor, and DA module (as shown in FC1 and FC2_1 in Fig. 1), in a way that <strong>the parameters are not shared to avoid the impact of MK-MMD on the RUL predictor during loss backpropagation.</strong></li>
<li>An exponential type attenuation penalty coefficient is designed for the MK-MMD loss, as shown in Fig. 1, to obtain a larger DA gain generated by the larger proportion of the MK-MMD loss in the total loss during the early stages of network training for helping learn domain-invariant features and achieving domain adaptation. <strong>As the number of training iterations increases, the proportion of MK-MMD in total loss gradually decreases, weakening the impact of DA gain on the network</strong>,<strong>(系数取值为1-0.5)</strong> enhancing the network’s stability, and relying more on the feature extractor composed of ERCM with CBAM and RUL predictor for feature learning and RUL prediction, thereby achieving negative transfer mitigation and enhancing feature learning. Domain adaptation loss calculation of the constructed network and the exponential type atten- uation penalty coefficient λattenuation are as follows:</li>
</ol>
</li>
</ol>
<div align=center>
<img src="https://pic.imgdb.cn/item/6629aa4f0ea9cb14035d258f.png" >
</div>

<h4 id="2-4-Training-optimization-objective"><a href="#2-4-Training-optimization-objective" class="headerlink" title="2.4. Training optimization objective"></a>2.4. Training optimization objective</h4><h4 id="2-5-ERCDAN-for-cross-machine-rolling-bearing-RUL-prediction"><a href="#2-5-ERCDAN-for-cross-machine-rolling-bearing-RUL-prediction" class="headerlink" title="2.5. ERCDAN for cross-machine rolling bearing RUL prediction"></a>2.5. ERCDAN for cross-machine rolling bearing RUL prediction</h4><ol>
<li>数据预处理(z-score,寿命标签尺度缩放)，划分数据集</li>
<li>模型训练</li>
<li>寿命预测</li>
</ol>
<h3 id="3-Experimental-analysis"><a href="#3-Experimental-analysis" class="headerlink" title="3. Experimental analysis"></a>3. Experimental analysis</h3><h4 id="3-1-Experimental-setup"><a href="#3-1-Experimental-setup" class="headerlink" title="3.1. Experimental setup"></a>3.1. Experimental setup</h4><p>数据集：PHM 2012 bearing dataset [40], XJTU-SY bearing dataset [41], and EBFL dataset</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/6629aa4f0ea9cb14035d24ba.png" >
</div>

<p>3.1.2. Performance evaluation metrics<br>RMSE，MAE，R2（The closer the value of R2 is to 1, the better the regression fitting effect）</p>
<h4 id="3-2-Ablation-experiment-analysis"><a href="#3-2-Ablation-experiment-analysis" class="headerlink" title="3.2. Ablation experiment analysis"></a>3.2. Ablation experiment analysis</h4><p>3.2.1. Comparison of backbone networks<br><strong>对比不同特征提取网络</strong> </p>
<div align=center>
<img src="https://pic.imgdb.cn/item/6629aa8d0ea9cb14035d71bc.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/6629aa8d0ea9cb14035d7209.png" >
</div>

<p>3.2.2. Comparison of penalty coefficients<br><strong>对比不同损失系数</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/6629aa8d0ea9cb14035d7241.png" >
</div>

<p>3.2.3. Comparison of different modules<br><strong>对比不同模型结构</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/6629aab30ea9cb14035d9fa7.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/6629aab40ea9cb14035da01b.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/6629aab40ea9cb14035d9fc3.png" >
</div>

<blockquote>
<p>Fig. 8 shows the visualization effect of RUL prediction for various methods in structural ablation experiments. It can be seen from Fig. 8 that the RUL curve predicted by the proposed method can better fit the real RUL line. From Fig. 8(a), (h), and (k), it can be seen that Method 4, due to the attenuation MK-MMD and the RUL predictor sharing the fully connected layer, is affected by the MK-MMD during backpropagation, resulting in the final output of the RUL prediction value being more easily misled by negative transfer. The prediction trend of the local RUL curve is opposite to the real RUL, and even significant prediction errors occur, as shown in Fig. 8(b) and (e). From Fig. 8(a) and (e), it can be seen that both Method 4 and Method 5 exhibit a predicted trend of RUL curves that is opposite to the real RUL, while the proposed method does not exhibit a predicted trend of RUL curves that is opposite to the real RUL, indicating the effectiveness of the collaborative full connection structure designed in the proposed method in alleviating negative transfer.</p>
</blockquote>
<h4 id="3-3-Comparative-experiment-analysis"><a href="#3-3-Comparative-experiment-analysis" class="headerlink" title="3.3. Comparative experiment analysis"></a>3.3. Comparative experiment analysis</h4><p>Method 1: CNN-based method; Method 2: CNN-Attention-based method; Method 3: CNN-LSTM-based method. Domain-adaptation methods, Method 4: CORAL-based method; Method 5: DAN-based method; Method 6: DANNbased method.</p>
<p>3.3.1. Cross-machine experiment</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/6629aab30ea9cb14035d9f77.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/6629aab40ea9cb14035da05e.png" >
</div>

<p>负迁移产生的原因</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/6629ac9d0ea9cb14036197c6.png" >
</div>

<p>3.3.2. Cross-bearing experiment on the same machine</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Mixup domain adaptations for dynamic remaining useful life predictions</title>
    <url>/2024/06/06/paper-note/RUL/%E8%BF%81%E7%A7%BB%E5%AD%A6%E4%B9%A0/Mixup%20domain%20adaptations%20for%20dynamic%20remaining%20useful%20life%20predictions/</url>
    <content><![CDATA[<h2 id="Mixup-domain-adaptations-for-dynamic-remaining-useful-life-predictions"><a href="#Mixup-domain-adaptations-for-dynamic-remaining-useful-life-predictions" class="headerlink" title="Mixup domain adaptations for dynamic remaining useful life predictions"></a>Mixup domain adaptations for dynamic remaining useful life predictions</h2><h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>MDAN encompasses a three-staged mechanism where the <strong>mix-up strategy is not only performed to regularize the source and target domains but also applied to establish an intermediate mix-up domain where the source and target domains are aligned</strong>.The self-supervised learning strategy is implemented to prevent the supervision collapse problem.</p>
<h3 id="1-introduction"><a href="#1-introduction" class="headerlink" title="1.introduction"></a>1.introduction</h3><div align=center>
<img src="https://pic.imgdb.cn/item/664e95c6d9c307b7e9ad47a9.png" >
</div>

<ol>
<li>first trained to master the source domain under the mix-up regularization strategy [14] to enhance predictive consistency and to enrich intrinsic structures of the latent space.</li>
<li>The second stage is designed to create an intermediate mix-up domain where the source and target domains are aligned onto it. This is made possible by applying the progressive mix-up approach [15] which creates convex combinations between the source-domain samples and the target-domain samples using pseudo-labels of the target domain.</li>
<li>The last step is to apply the mix-up regularization strategy in the target domain using pseudo-label information.The self-supervised learning strategy via the controlled reconstruction learning step [16] is integrated to prevent the supervision collapse problems and to create transferable representations.</li>
</ol>
<p><strong>主要贡献：</strong></p>
<ol>
<li>To the best of our knowledge, we are the first to put forward the mixup domain adaptation strategy for time-series problems;</li>
<li>对14，15的改进</li>
<li>对16的改进</li>
</ol>
<h3 id="2-Related-works"><a href="#2-Related-works" class="headerlink" title="2.Related works"></a>2.Related works</h3><h4 id="2-1-Mixup"><a href="#2-1-Mixup" class="headerlink" title="2.1. Mixup"></a>2.1. Mixup</h4><div align=center>
<img src="https://pic.imgdb.cn/item/664e95c6d9c307b7e9ad47b8.png" >
</div>

<h3 id="3-Preliminaries"><a href="#3-Preliminaries" class="headerlink" title="3. Preliminaries"></a>3. Preliminaries</h3><h4 id="3-1-Problem-formulation"><a href="#3-1-Problem-formulation" class="headerlink" title="3.1. Problem formulation"></a>3.1. Problem formulation</h4><p>Unsupervised Domain Adaptation</p>
<h3 id="4-Learning-policy-of-MDAN"><a href="#4-Learning-policy-of-MDAN" class="headerlink" title="4. Learning policy of MDAN"></a>4. Learning policy of MDAN</h3><p>MDAN learning policy comprises three steps: source domain training, intermediate domain training and target domain training.<br><strong>1. The source domain training process learns original labelled samples and mixup samples in the supervised learning manner while also performing the self-supervised training process with the absence of any labels via the controlled reconstruction process</strong> </p>
<p><strong>2.The intermediate domain training process is executed by creating mixup samples between the source domain samples and the target domain samples aided by the pseudo-labelling steps in both input level and feature level. The mixup samples are learned in the supervised learning manner.</strong> </p>
<p><strong>3. Last but not least, the target domain training process is driven by the self-learning mechanism of unlabelled target domain samples and the mixup regularization strategy to combat the issue of noisy pseudo labels.</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/664e95c7d9c307b7e9ad482c.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/664e96b3d9c307b7e9ae09cd.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/664e95c7d9c307b7e9ad48b6.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/664e95c6d9c307b7e9ad4742.png" >
</div>

<h3 id="5-Experiments"><a href="#5-Experiments" class="headerlink" title="5. Experiments"></a>5. Experiments</h3><h4 id="5-1-result"><a href="#5-1-result" class="headerlink" title="5.1 result"></a>5.1 result</h4><div align=center>
<img src="https://pic.imgdb.cn/item/664e96b3d9c307b7e9ae0a67.png" >
</div>

<h4 id="5-6-Analysis-of-domain-discrepancy"><a href="#5-6-Analysis-of-domain-discrepancy" class="headerlink" title="5.6. Analysis of domain discrepancy"></a>5.6. Analysis of domain discrepancy</h4><div align=center>
<img src="https://pic.imgdb.cn/item/664e96b3d9c307b7e9ae09ea.png" >
</div>]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Towards trustworthy remaining useful life prediction through multi-source information fusion and a novel LSTM-DAU model</title>
    <url>/2024/06/06/paper-note/RUL/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96+%E5%88%86%E6%AE%B5%E9%A2%84%E6%B5%8B/7.Towards%20trustworthy%20remaining%20useful%20life%20prediction%20through%20multi-source%20information%20fusion%20and%20a%20novel%20LSTM-DAU%20model/</url>
    <content><![CDATA[<p><strong>2024.02.08 西北工业大学 8.1 1区 Reliability Engineering &amp; System Safety</strong></p>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>To solve the above problems, a novel adaptive multi-source fusion method based on genetic programming is proposed for building a HI that can effectively reflect the health state of machines. Subsequently, a new LSTM model with a dual-attention mechanism</p>
<h2 id="1introduction"><a href="#1introduction" class="headerlink" title="1introduction"></a>1introduction</h2><p>Condition-based maintenance (CBM) exploits condition monitoring (CM) information to identify or predict potential problems</p>
<h3 id="1-1-Review-of-different-signals-for-machinery-CM-and-RUL-prediction"><a href="#1-1-Review-of-different-signals-for-machinery-CM-and-RUL-prediction" class="headerlink" title="1.1. Review of different signals for machinery CM and RUL prediction"></a>1.1. Review of different signals for machinery CM and RUL prediction</h3><div align=center>
<img src="https://pic.imgdb.cn/item/660403cb9f345e8d035eb868.png" >
</div>

<p>Considering the advantages of high utility and low cost of vibration and temperature signals, this paper proposes a novel method for machine degradation monitoring and RUL prediction based on the fusion of vibration and temperature signals.</p>
<h3 id="1-2-Review-of-LSTM-based-RUL-prediction-methods"><a href="#1-2-Review-of-LSTM-based-RUL-prediction-methods" class="headerlink" title="1.2. Review of LSTM-based RUL prediction methods"></a>1.2. Review of LSTM-based RUL prediction methods</h3><blockquote>
<p><strong>lstm不足:</strong><br> uses the same updating method for all input and recurrent information which fails to process the data in accordance with the importance of the information<br><strong>Attention不足：</strong><br> the attentional mechanisms on the input side to weigh different input features. These methods ignore the backward and forward dependencies of the sequences and seldom investigate the attentional mechanism of the recurrent information in the sequences.</p>
</blockquote>
<h3 id="1-3-Contributions"><a href="#1-3-Contributions" class="headerlink" title="1.3. Contributions"></a>1.3. Contributions</h3><ul>
<li>First, feature extraction is performed based on vibration and temperature signals, and two optimal features are selected respectively.提取振动和温度信号的特征。 </li>
<li>Second, considering that vibration and temperature features may contribute differently to the RUL prediction, genetic programming (GP) is adopted to adaptively fuse the above two features, which in turn yields the HI characterizing the equipment’s health state. 用GP算法进行特征融合，构造HI</li>
<li>Finally, based on the proposed LSTMDAU, an unsupervised learning framework is developed to predict the health state and RUL of machines.用LSTM结合注意力的网络进行预测</li>
</ul>
<h2 id="2-GP-based-feature-fusion"><a href="#2-GP-based-feature-fusion" class="headerlink" title="2. GP-based feature fusion"></a>2. GP-based feature fusion</h2><p><strong>why GP?</strong></p>
<blockquote>
<p>GP is a popular heuristic <strong>nonlinear algorithm</strong>. It not only preserves the original features as much as possible but also finds the optimal HI in a shorter time. Most importantly, the HI generated adaptively by GP can be expressed in an easy-to-read and well-interpretable form, which <strong>solves the black-box modeling problem</strong> of some approaches, such as NN.</p>
</blockquote>
<h2 id="3-Developed-LSTM-DAU-method"><a href="#3-Developed-LSTM-DAU-method" class="headerlink" title="3. Developed LSTM-DAU method"></a>3. Developed LSTM-DAU method</h2><p>3.1. Input attention unit<br>3.2. Recurrent attention unit<br>3.3. Dual attention unit</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/660403cb9f345e8d035eb9a2.png" >
</div>

<h2 id="4-Proposed-methodology"><a href="#4-Proposed-methodology" class="headerlink" title="4. Proposed methodology"></a>4. Proposed methodology</h2><div align=center>
<img src="https://pic.imgdb.cn/item/660403cb9f345e8d035ebbeb.png" >
</div>

<h3 id="4-1-HI-construction"><a href="#4-1-HI-construction" class="headerlink" title="4.1. HI construction"></a>4.1. HI construction</h3><h4 id="4-1-1-Feature-extraction"><a href="#4-1-1-Feature-extraction" class="headerlink" title="4.1.1. Feature extraction"></a>4.1.1. Feature extraction</h4><p>振动信号特征<br>AWSPT-Kurtosis, AWSPT-SI, AWSPT-NE, AWSPT-SI, AWSPT-GI [41]<br>温度信号特征<br>mean, root mean square (RMS), maximum (Max), and minimum (Min) of temperature signals</p>
<h4 id="4-1-2-Feature-selection"><a href="#4-1-2-Feature-selection" class="headerlink" title="4.1.2. Feature selection"></a>4.1.2. Feature selection</h4><p><strong>why</strong>?</p>
<blockquote>
<p>The purpose of feature selection is to obtain preferred features from the original feature set according to some rules that minimize the redundancy between features while containing the most valuable degradation features [42]. This section describes the detailed steps of feature selection</p>
</blockquote>
<p>筛选指标<br>composite evaluation index (CEI)</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/660403cc9f345e8d035ebc92.png" >
</div>

<h4 id="4-1-3-Feature-fusion"><a href="#4-1-3-Feature-fusion" class="headerlink" title="4.1.3. Feature fusion"></a>4.1.3. Feature fusion</h4><p><strong>why？</strong></p>
<blockquote>
<p>the vibration signal is more sensitive to structural damage and the temperature signal is more sensitive to non-structural damage</p>
</blockquote>
<p>Employ the GP algorithm to adaptively fuse the preferred features by the fitness function</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/660403cc9f345e8d035ebd30.png" >
</div>

<h3 id="4-2-The-specific-procedure-of-RUL-prediction-based-on-LSTM-DAU"><a href="#4-2-The-specific-procedure-of-RUL-prediction-based-on-LSTM-DAU" class="headerlink" title="4.2. The specific procedure of RUL prediction based on LSTM-DAU"></a>4.2. The specific procedure of RUL prediction based on LSTM-DAU</h3><p>pass</p>
<h2 id="5-Case-study"><a href="#5-Case-study" class="headerlink" title="5. Case study"></a>5. Case study</h2><h3 id="5-1-Data-description-and-preprocessing"><a href="#5-1-Data-description-and-preprocessing" class="headerlink" title="5.1. Data description and preprocessing"></a>5.1. Data description and preprocessing</h3><div align=center>
<img src="https://pic.imgdb.cn/item/660404099f345e8d0360abfd.png" >
</div>

<h3 id="5-2-HI-construction"><a href="#5-2-HI-construction" class="headerlink" title="5.2. HI construction"></a>5.2. HI construction</h3><div align=center>
<img src="https://pic.imgdb.cn/item/660404099f345e8d0360ad5f.png" >
</div>

<p>对比不同的降维方法在bearing1_1</p>
<blockquote>
<p>Moreover, GP is nonlinear learning compared to PCA. To better characterize the nonlinear nature of the degradation process, GP is adopted to generate HI.</p>
</blockquote>
<div align=center>
<img src="https://pic.imgdb.cn/item/660404099f345e8d0360afd2.png" >
</div>

<h3 id="5-3-LSTM-DAU-network-setting-and-evaluation-metric"><a href="#5-3-LSTM-DAU-network-setting-and-evaluation-metric" class="headerlink" title="5.3. LSTM-DAU network setting and evaluation metric"></a>5.3. LSTM-DAU network setting and evaluation metric</h3><h3 id="5-4-RUL-prognosis-and-comparison-analysis"><a href="#5-4-RUL-prognosis-and-comparison-analysis" class="headerlink" title="5.4. RUL prognosis and comparison analysis"></a>5.4. RUL prognosis and comparison analysis</h3><p>only the data in the degradation phase of a bearing are adopted to train the LSTM-DAU. The determination of the degradation phase can be found in Ref. [42].</p>
<p>实验部分：<br>对比不同窗口大小，对比不同训练数据集大小，消融实验（两个单特征，两个单注意力LSTM，和LSTM），对比实验（对比SOTA）</p>
<p>[41]:Hou B, Wang D, Wang Y, Yan T, Peng Z, Tsui K-L. Adaptive weighted signal preprocessing technique for machine health monitoring. IEEE Trans Instrum Meas 2020;70:1–11.<br>[42]:Mi J, Liu L, Zhuang Y, Bai L, Li Y-F. A synthetic feature processing method for remaining useful life prediction of rolling bearings. IEEE Trans Reliab 2022;72(1):125–36.</p>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>A piecewise method for bearing remaining useful life estimation using temporal convolutional networks</title>
    <url>/2024/06/06/paper-note/RUL/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96+%E5%88%86%E6%AE%B5%E9%A2%84%E6%B5%8B/A%20piecewise%20method%20for%20bearing%20remaining%20useful%20life%20estimation%20using%20temporal%20convolutional%20networks/</url>
    <content><![CDATA[<h2 id="A-piecewise-method-for-bearing-remaining-useful-life-estimation-using-temporal-convolutional-networks"><a href="#A-piecewise-method-for-bearing-remaining-useful-life-estimation-using-temporal-convolutional-networks" class="headerlink" title="A piecewise method for bearing remaining useful life estimation using temporal convolutional networks"></a>A piecewise method for bearing remaining useful life estimation using temporal convolutional networks</h2><p><strong>2023.04 华科 12.1 1区 Journal of Manufacturing Systems</strong></p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>traditional network models such as <strong>CNN and RNN have limitations in directly dealing with time series problems</strong>. This paper proposes an adaptive degradation stage division strategy and a temporal convolutional network (TCN)-based RUL piecewise estimation method, called ADSD-TCNPE. This method mainly includes three steps.</p>
<ul>
<li>(1) <strong>Extract features</strong> from different domains and select the features that are highly correlated with bearing degradation. </li>
<li>(2) <strong>Adaptively divide the whole lifecycle</strong> of bearing into different degradation stages.</li>
<li>(3) Establish a <strong>TCN-based piecewise degradation model</strong> for <strong>different degradation stages</strong> to accurately predict bearing RUL.</li>
</ul>
<h3 id="1introduction"><a href="#1introduction" class="headerlink" title="1introduction"></a>1introduction</h3><p>PHM -&gt;model-based methods [2,3] and data-driven methods -&gt;<br>Although the above deep learning-based methods have achieved excellent results, <strong>there are still some challenges</strong>:</p>
<ul>
<li>Firstly, most existing methods focus on building a <strong>whole-lifecycle degradation model for bearings</strong>, which is <strong>different from the actual bearing degradation process</strong>.</li>
<li>Secondly, the <strong>corresponding mapping relationships</strong> between extracted features and operating states have not been fully learned for these different degradation states.</li>
<li>Thirdly, some commonly used deep learning <strong>models have limitations</strong> for the RUL prediction problem.like CNN,RNN</li>
</ul>
<hr>
<p>The main contributions of this paper can be summarized as follows.</p>
<ul>
<li>First, <strong>multiple degradation features</strong>, including time-domain features, frequency domain features, and time-frequency domain features, are extracted from the vibration signal. <strong>Then the effective features are selected according to the established comprehensive evaluation metric.</strong> <strong>多尺度退化特征的筛选及评级指标</strong></li>
<li>the density-based clustering algorithm is used to adaptively divide the bearing degradation state. <strong>退化过程的自适应划分</strong></li>
<li>A piecewise estimation model is established for the RUL prediction of bearings. <strong>The corresponding estimation model is built for each health state to predict RUL.</strong> Our <strong>TCN-based RUL estimation method has superior</strong> feature extraction ability and long-term sequence processing capability, which is more suitable for the bearing RUL prediction task.</li>
</ul>
<h3 id="2Temporal-convolutional-network"><a href="#2Temporal-convolutional-network" class="headerlink" title="2Temporal convolutional network"></a>2Temporal convolutional network</h3><div align=center>
<img src="https://pic.imgdb.cn/item/65f115ad9f345e8d039be4c5.png">
</div>

<h3 id="3-Proposed-ADSD-TCNPE-approach"><a href="#3-Proposed-ADSD-TCNPE-approach" class="headerlink" title="3. Proposed ADSD-TCNPE approach"></a>3. Proposed ADSD-TCNPE approach</h3><div align=center>
<img src=https://pic.imgdb.cn/item/65f115ad9f345e8d039be584.png >
</div>

<ul>
<li><p>Firstly, <strong>the time, frequency, and time-frequency domain features are extracted</strong> from bearing vibration signals to form the original feature space. Meanwhile, <strong>a comprehensive evaluation metric is constructed to select features with high degradation correlation from the original feature space.</strong><br>时域，频域，时频域特征的提取及用综合评价指标进行特征筛选</p>
</li>
<li><p>Secondly, <strong>a nonlinear compression method is used to make the boundaries between different degradation stages more exact, and density-based clustering is employed to adaptively divide different degradation stages</strong>.<br>用非线性压缩方法(T-SNE)使不同退化阶段的特征边界更明显，再使用密度聚类的方法区分不同退化阶段</p>
</li>
<li><p>Finally, the TCN-based degradation models of different degradation stages are established to realize bearing RUL prediction. The detailed description of each step is as follows.<br>最后在不同退化阶段建立不同的TCN模型进行预测</p>
</li>
</ul>
<h4 id="3-1-Feature-extraction"><a href="#3-1-Feature-extraction" class="headerlink" title="3.1. Feature extraction"></a>3.1. Feature extraction</h4><p>why？</p>
<blockquote>
<p>the original signal data is often high-dimensional and has redundant information, making it difficult to establish an accurate degradation model.</p>
</blockquote>
<p>3.1.1. Time-domain features<br>Time-domain features that give a general profile of bearing degradation trends have been verified to be effective in monitoring machinery health states.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65f115ad9f345e8d039be5f9.png" >
</div>

<hr>
<p>3.1.2. Frequency-domain features</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65f115ad9f345e8d039be64a.png" >
</div>

<hr>
<p>3.1.3. Time-frequency features</p>
<p>The time and frequency domain features give a general profile of bearing degradation trends, while the time-frequency domain features can represent the local characteristics of the vibration signal.</p>
<p><strong>empirical mode decomposition (EMD) and Hilbert spectrum analysis</strong></p>
<p>todo</p>
<h4 id="3-2-Feature-selection-using-comprehensive-evaluation-metric"><a href="#3-2-Feature-selection-using-comprehensive-evaluation-metric" class="headerlink" title="3.2. Feature selection using comprehensive evaluation metric"></a>3.2. Feature selection using comprehensive evaluation metric</h4><p>why?</p>
<blockquote>
<p>It can be found that <strong>good prediction features have similar degradation properties</strong>, so the distance between feature sequences should be as close as possible, and they should also <strong>be monotonically correlated with the degradation process</strong>.</p>
</blockquote>
<p><strong>The dynamic time warping distance</strong> is first used to measure the <strong>similarity between feature sequences</strong>. Moreover, <strong>monotonicity and correlation metrics are employed to measure the monotonic correlation of features</strong></p>
<hr>
<p>3.2.1. Dynamic time warping distance metric<br>Compared with traditional distance measurement methods such as Euclidean distance, <strong>dynamic time warping (DTW) shows superior performance in measuring the similarity of time series.</strong></p>
<p><strong>补充：</strong><br><strong>Dynamic Time Warping（动态时间调），用来进行时间序列相似性度量</strong></p>
<p>在传统算法中，可以用余弦相似度和pearson相关系数来描述两个序列的相似度。但是时间序列比较特殊，可能存在两个问题：</p>
<ul>
<li>两段时间序列长度不同。如何求相似度？</li>
<li>一个序列是另一个序列平移之后得到的。如何求相似距离？</li>
</ul>
<p>第一个问题，导致了根本不能用余弦相似度和pearson相关系数来求解相似。第二个问题，导致了也不能基于欧式距离这样的算法，来求解相似距离。<br>Dynamic Time Warping (DTW) 本质上和通过动态规划来计算这个序列的相似距离。</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65f118d39f345e8d03a74722.png" >
</div>

<p>在文中用DWT获得了不同特征序列与特征序列中心的相似度，原因是Features with smaller similarity distances are more appropriate for RUL prediction.</p>
<p>参考:<a href="https://blog.csdn.net/weixin_39910711/article/details/108178110">https://blog.csdn.net/weixin_39910711/article/details/108178110</a><br><a href="https://blog.csdn.net/xsdxs/article/details/86648605">https://blog.csdn.net/xsdxs/article/details/86648605</a></p>
<hr>
<p>3.2.2. Correlation metric</p>
<p>The time correlation metric measures the degree of linear correlation between feature sequences and running time, denoted as Corr and the calculation equation is as follows:</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65f115ad9f345e8d039be6b7.png" >
</div>

<p>3.2.3. Monotonicity metric</p>
<p>The monotonicity metric (denoted as Mon) assesses the feature increasing or decreasing trend as follows</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65f116bb9f345e8d03a042e6.png" >
</div>

<p>the  comprehensive evaluation metric</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65f1173b9f345e8d03a1e980.png" >
</div>

<h4 id="3-3-Degradation-state-division"><a href="#3-3-Degradation-state-division" class="headerlink" title="3.3. Degradation state division"></a>3.3. Degradation state division</h4><p>why？</p>
<blockquote>
<p>The degraded expression after feature selection may still have some redundant information.which may lead to a long distance between degradation state points in the same degradation stage and a short distance between state points in different degradation stages, further resulting in <strong>the boundaries between different degradation stages are not distinguishable enough</strong>.<br>经过特征提取后的特征可能有冗余的信息，会导致在相同退化阶段的退化特征点之间距离较长，不同退化阶段的点之间的距离较短，从而使不同退化阶段的边界不够明显。</p>
</blockquote>
<hr>
<p>3.3.1. Feature compression based on t-SNE<br>This algorithm first transforms the Euclidean distance into a joint probability distribution and then compresses the data from high-dimension to low-dimension to express the similarity between state points.<br>经过t-SNE的降维后，不同阶段的退化状态点距离拉长，相同点距离缩短。</p>
<p><strong>补充两种常见的降维方法</strong>：<br>Principal Component Analysis(PCA)，t-Distributed Stochastic Neighbour Embedding(t-SNE，t分布随机邻居嵌入)<br>减少数据维数的目标 (Goals for reducing the dimensionality of the data)</p>
<ol>
<li><p>Preserve as much significant structure or information of the data present in the high-dimensional data as possible in the low-dimensional representation.<br><strong>在低维表示中，尽可能保留高维数据中存在的数据的重要结构或信息。</strong></p>
</li>
<li><p>Increase the interpretability of the data in the lower dimension<strong>在较低维度上增加数据的可解释性</strong></p>
</li>
<li><p>Minimizing information loss of data due to dimensionality reduction<strong>最小化因降维而导致的数据信息丢失</strong></p>
</li>
</ol>
<p><strong>主成分分析(PCA)</strong>:</p>
<blockquote>
<p>一种无监督的确定性算法，用于特征提取和可视化<br>应用线性降维技术 ，其重点是在低维空间中将相异的点保持较远的距离 。<br>通过使用特征值保留数据中的方差，将原始数据转换为新数据 。<br>离群值影响PCA。</p>
</blockquote>
<p><strong>t-分布随机邻居嵌入(t-SNE) (t-Distributed Stochastic Neighbourh Embedding(t-SNE))</strong></p>
<blockquote>
<p>一种无监督的随机算法，仅用于可视化<br>应用非线性 降 维技术 ，其中重点是在低维空间中将非常相似的数据点保持在一起 。<br>使用学生t分布保留数据的局部结构，以计算低维空间中两点之间的相似度。<br>t-SNE使用重尾学生t分布来计算低维空间中两点之间的相似度，而不是高斯分布，这有助于解决拥挤和优化问题 。<br>t-SNE使用重尾学生t分布来计算低维空间中两点之间的相似度，而不是高斯分布，这有助于解决拥挤和优化问题<br>离群值不影响t-SNE</p>
</blockquote>
<p>参考：<a href="https://blog.csdn.net/weixin_26752765/article/details/108132823">https://blog.csdn.net/weixin_26752765/article/details/108132823</a></p>
<p>3.3.2. Degradation state division based on DBSCAN</p>
<blockquote>
<p>why?<br>Since the spatial distribution shape of <strong>feature state points after feature compression may be irregular</strong>, this paper adopts the density-based spatial clustering of applications with noise (DBSCAN) algorithm to deal with the state points after feature compression.</p>
</blockquote>
<h4 id="3-4RUL-piecewise-estimation-based-on-TCN"><a href="#3-4RUL-piecewise-estimation-based-on-TCN" class="headerlink" title="3.4RUL piecewise estimation based on TCN"></a>3.4RUL piecewise estimation based on TCN</h4><p>why piecewise estimation？<br>Mixing data from the slow and rapid degradation stages to train the RUL estimation model together will <strong>generate an unbalanced data problem and result in an inaccurate prediction model</strong> for the rapid degradation stage.</p>
<p>Here the input data for both SVM and TCN models are the feature points vector f after feature selection, and the output data are the degradation stage labels and RUL labels, respectively.</p>
<p>normalization：minmax<br>metric：RMSE，MAPE，Er(percent error)</p>
<h3 id="4-Experimental-verification"><a href="#4-Experimental-verification" class="headerlink" title="4. Experimental verification"></a>4. Experimental verification</h3><h4 id="4-1-Data-description"><a href="#4-1-Data-description" class="headerlink" title="4.1. Data description"></a>4.1. Data description</h4><p>FEMTO</p>
<h4 id="4-2-Feature-extraction-and-selection"><a href="#4-2-Feature-extraction-and-selection" class="headerlink" title="4.2. Feature extraction and selection"></a>4.2. Feature extraction and selection</h4><div align=center>
<img src="https://pic.imgdb.cn/item/65f1173c9f345e8d03a1eb4c.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/65f1173c9f345e8d03a1ec58.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/65f1173c9f345e8d03a1ed40.png" >
</div>

<p>In this study, 14 features least correlated to bearing degradation are eliminated to keep the number of selected features the same.</p>
<h4 id="4-3-Division-of-degradation-stages"><a href="#4-3-Division-of-degradation-stages" class="headerlink" title="4.3. Division of degradation stages"></a>4.3. Division of degradation stages</h4><p>In this section, the <strong>t-SNE algorithm</strong> is adopted to compress the selected effective features into two dimensions. <strong>The low-dimensional spatial distribution of compressed features can distinguish different degradation states.</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65f117859f345e8d03a2e695.png" >
</div>

<h4 id="4-4-RUL-estimation-results-and-analysis"><a href="#4-4-RUL-estimation-results-and-analysis" class="headerlink" title="4.4. RUL estimation results and analysis"></a>4.4. RUL estimation results and analysis</h4><p>4.4.1. Estimation results and analysis</p>
<blockquote>
<p>为什么不用T—sne降维后的特征作为输入？<br>To avoid the loss of degradation information, this paper uses the 30-dimensional features after feature selection as network inputs instead of the 2-dimensional features after t-SNE dimensionality reduction.</p>
</blockquote>
<div align=center>
<img src="https://pic.imgdb.cn/item/65f117859f345e8d03a2e70b.png" >
</div>

<hr>
<p>The comparison of <strong>piecewise estimation</strong> using the ADSD-TCNPE model proposed in this paper, and Who represents the result of the unified degradation model for <strong>the whole lifecycle</strong>.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65f117859f345e8d03a2e96f.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/65f117859f345e8d03a2e8ca.png" >
</div>


<p>prediction model can achieve <strong>better prediction</strong> results, especially <strong>in the rapid degradation stage</strong> when the bearing is about to fail. <strong>This is because the prediction model in the rapid degradation stage focuses on a small number of training samples in that stage and is not affected by the noise of other samples in the slow degradation stage.</strong></p>
<p>4.4.2. Method validation</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65f118aa9f345e8d03a6bede.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/65f118aa9f345e8d03a6bf4a.png" >
</div>]]></content>
      <categories>
        <category>paper note</category>
      </categories>
      <tags>
        <tag>特征提取+分段预测</tag>
      </tags>
  </entry>
  <entry>
    <title>AE构建HI</title>
    <url>/2024/06/06/paper-note/RUL/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96+%E5%88%86%E6%AE%B5%E9%A2%84%E6%B5%8B/8.Remaining%20useful%20life%20prediction%20of%20bearing%20based%20on%20stacked%20autoencoder%20and%20recurrent%20neural%20network/</url>
    <content><![CDATA[<h1 id="Remaining-useful-life-prediction-of-bearing-based-on-stacked-autoencoder-and-recurrent-neural-network"><a href="#Remaining-useful-life-prediction-of-bearing-based-on-stacked-autoencoder-and-recurrent-neural-network" class="headerlink" title="Remaining useful life prediction of bearing based on stacked autoencoder and recurrent neural network"></a>Remaining useful life prediction of bearing based on stacked autoencoder and recurrent neural network</h1><p><strong>2021.04 北京科技大学 12.1 1区 Journal of Manufacturing Systems</strong></p>
<h2 id="1introduction"><a href="#1introduction" class="headerlink" title="1introduction"></a>1introduction</h2><p>非线性HI的优势</p>
<blockquote>
<p>nonlinear degradation processes are common in nature, and the nonlinear shape of degradation proves their advantage in tracking the dynamics of many actual degradation processes used for prediction [17,18]</p>
</blockquote>
<div align=center>
<img src="https://pic.imgdb.cn/item/660404549f345e8d036311d8.png" >
</div>

<p>flow chat</p>
<ol>
<li>训练部分(ims)<ol>
<li>提取十种时域特征，筛选出四种特征，因为只采用退化阶段数据进行训练，数据量小所以对数据进行了三次指数线性插值，将特征输入带注意力的aotu encoder构建HI指标</li>
<li>将数据的STD特征作为输入，HI作为标签对LSTM模型进行训练</li>
</ol>
</li>
</ol>
<div align=center>
<img src="https://pic.imgdb.cn/item/660404559f345e8d0363132a.png" >
</div>

<ol>
<li>测试部分(ims)<ol>
<li>将测试数据的std输入网络，将输出HI与原HI对比</li>
</ol>
</li>
<li>验证部分(cwru)<ol>
<li>用CWRU数据集中不同载荷下的数据提取出的STD输入网络，看输出的HI是否合理</li>
</ol>
</li>
</ol>
<h1 id="A-new-supervised-multi-head-self-attention-autoencoder-for-health-indicator-construction-and-similarity-based-machinery-RUL-prediction"><a href="#A-new-supervised-multi-head-self-attention-autoencoder-for-health-indicator-construction-and-similarity-based-machinery-RUL-prediction" class="headerlink" title="A new supervised multi-head self-attention autoencoder for health indicator construction and similarity-based machinery RUL prediction"></a>A new supervised multi-head self-attention autoencoder for health indicator construction and similarity-based machinery RUL prediction</h1><p>重庆大学 2023.01. 8.8 1区</p>
<h2 id="知识点"><a href="#知识点" class="headerlink" title="知识点"></a>知识点</h2><blockquote>
<p>why hi?<br>However, it is unreasonable to treat the degradation rate of a machine as a constant value in practice, since the accumulation of damage would lead to the accelerated degradation at the end of its life. In addition, the detection of the initial degradation point is an extremely difficult task, and it is also related to the experience of engineer [35]. Therefore, it is necessary to design an appropriate HI label to reflect the machine degradation trend.</p>
</blockquote>
<h2 id="简记"><a href="#简记" class="headerlink" title="简记"></a>简记</h2><div align=center>
<img src="https://pic.imgdb.cn/item/660404559f345e8d03631548.png" >
</div>

<p>flowchart</p>
<ol>
<li>振动信号先经过低通滤波，消除异常值等处理。之后按照公式5建立HI曲线作为标签，（对不同的数据组，进行HI构造实验，通过综合指标判断构建的HI的有效性），其与普通的线性HI区别如图fig4</li>
</ol>
<div align=center>
<img src="https://pic.imgdb.cn/item/660404559f345e8d036315d7.png" >
</div>
<div align=center>
<img src="https://pic.imgdb.cn/item/660404559f345e8d03631749.png" >
</div>

<ol>
<li>用HI曲线作为标签，用振动信号作为输入，训练带注意力的自编码器，其中带有两项损失，分别是encoder的输入和decoder输出的差别的损失，还有中间变量与公式构建的HI差别的损失。</li>
</ol>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
  <entry>
    <title>Remaining Useful Life Prediction of Rolling Bearings Based on ECA-CAE and Autoformer</title>
    <url>/2024/06/06/paper-note/RUL/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96+%E5%88%86%E6%AE%B5%E9%A2%84%E6%B5%8B/Remaining%20Useful%20Life%20Prediction%20of%20Rolling%20Bearings%20Based%20on%20ECA-CAE%20and%20Autoformer/</url>
    <content><![CDATA[<h2 id="Remaining-Useful-Life-Prediction-of-Rolling-Bearings-Based-on-ECA-CAE-and-Autoformer"><a href="#Remaining-Useful-Life-Prediction-of-Rolling-Bearings-Based-on-ECA-CAE-and-Autoformer" class="headerlink" title="Remaining Useful Life Prediction of Rolling Bearings Based on ECA-CAE and Autoformer"></a>Remaining Useful Life Prediction of Rolling Bearings Based on ECA-CAE and Autoformer</h2><p><strong>2024 福州大学 4.5 1区 Biomimetics</strong></p>
<h3 id="abstract"><a href="#abstract" class="headerlink" title="abstract"></a>abstract</h3><p>In response to the need for multiple complete bearing degradation datasets in traditional deep learning networks to predict the impact on individual bearings, a novel deep learning-based rolling bearing remaining life prediction method is proposed <strong>in the absence of fully degraded bearng data</strong><br>This method involves processing the raw vibration data through <strong>Channel-wise Attention Encoder (CAE) from the Encoder-Channel Attention (ECA)</strong>, extracting features related to mutual correlation and relevance, selecting the desired characteristics, and incorporating the selected features into the constructed <strong>Autoformer-based time prediction model</strong> to forecast the degradation trend of bearings’ remaining time.</p>
<h3 id="1introduction"><a href="#1introduction" class="headerlink" title="1introduction"></a>1introduction</h3><p>In summary, in order to minimize the impact of limited degraded bearing data on the accuracy of life prediction, this paper proposes a new deep learning network model framework.</p>
<ul>
<li>Compared with traditional deep learning methods that require a large amount of complete degraded bearing data for model training, this method only uses the first half of the current bearing degradation features to predict future degradation trends. <strong>This solves the problem of non-optimal model parameters due to insufficient data volume</strong></li>
<li>By comparing with the latest Informer and Transformer models [34,35], the advantages of the proposed method are validated.</li>
<li>Additionally, this model is suitable for predicting the life of bearings under the same operating conditions.</li>
</ul>
<h3 id="2preliminary"><a href="#2preliminary" class="headerlink" title="2preliminary"></a>2preliminary</h3><h4 id="2-1-CAE-Model-convolutional-Auto-Encode"><a href="#2-1-CAE-Model-convolutional-Auto-Encode" class="headerlink" title="2.1. CAE Model(convolutional Auto-Encode)"></a>2.1. CAE Model(convolutional Auto-Encode)</h4><p>卷积自编码器的编码器部分由卷积层（Convolution2D）和MaxPooling层（MaxPooling2D）构成，MaxPooling负责空域下采样。而解码器由卷积层（Convolution2D）和上采样层（UpSampling2D）构成。</p>
<hr>
<p>最为简单的自动编码器是由线性层构成的，它看起来就像是一个普通的深度神经网络DNN，只不过包含两大其他架构不具备的特征：</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e876a19f345e8d037e5c9e.png" >
</div>

<p>①输出层的神经元数量往往与输入层的神经元数量一致<br>②网络架构往往呈对称性，且中间结构简单、两边结构复杂<br>在上面的架构图中，从输入层开始压缩数据、直至架构中心的部分被称为编码器Encoder，编码器的职责是从原始数据中提取必要的信息，从原始数据中提纯出的信息被称之为编码Code或隐式表示；<br>从编码开始拓展数据、直至输出层的部分被称为解码器Decoder，解码器的输出一般被称为重构数据，解码器的职责是将提取出的信息还原为原来的结构。</p>
<hr>
<h4 id="2-2-ECANet-Efficient-Channel-Attention"><a href="#2-2-ECANet-Efficient-Channel-Attention" class="headerlink" title="2.2. ECANet(Efficient Channel Attention)"></a>2.2. ECANet(Efficient Channel Attention)</h4><p>To cope with the problem of weight assignment among different channels, scholars have proposed ECANet</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e70e759f345e8d038fa844.png" >
</div>


<p>dynamic convolution kernels are used to effectively extract features under different sensory fields and learn the weights between different channels.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e70e759f345e8d038fa8ee.png" >
</div>

<h4 id="2-3-Autoformer-Model"><a href="#2-3-Autoformer-Model" class="headerlink" title="2.3. Autoformer Model"></a>2.3. Autoformer Model</h4><ul>
<li>as the prediction time lengthens,<strong>it is difficult to find reliable temporal dependencies from complex temporal patterns by directly using the self-attention mechanism</strong></li>
<li>secondly, due to the <strong>self-attention secondary complexity problem</strong>, the model has to use its <strong>sparse version</strong>, but it will <strong>limit the information utilization efficiency and affect the prediction effect</strong></li>
</ul>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e70e759f345e8d038fa969.png" >
</div>

<p>Based on the above progressive decomposition architecture, the model can gradually decompose the hidden variables in the forecasting process and obtain the forecasting results of cycle and trend components respectively through the autocorrelation mechanism and accumulation, so as to realize the alternate and mutual promotion of decomposition and optimization of forecasting results.</p>
<hr>
<ul>
<li>series decomposition module</li>
</ul>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e874e19f345e8d0375e261.png" >
</div>

<ul>
<li><p>In the Encoder part, a stepwise elimination of thetrend term is taken to obtain the periodic terms . And based on this periodicity,an <strong>autocorrelation mechanism is used to aggregate similar sub processes of different periods</strong></p>
</li>
<li><p>In the Decoder section, the trend term and the period term are modeled separately. In which, </p>
<ul>
<li>for the period term, the autocorrelation mechanism uses the periodic nature of the sequence to aggregate subsequences with similar processes in different cycles; </li>
<li>for the trend term T, the trend information is gradually extracted from the predicted hidden variables using a cumulative approach.</li>
</ul>
</li>
</ul>
<hr>
<h3 id="3Feature-Extraction-Method-and-Remaining-Useful-Life-Prediction-Algorithm"><a href="#3Feature-Extraction-Method-and-Remaining-Useful-Life-Prediction-Algorithm" class="headerlink" title="3Feature Extraction Method and Remaining Useful Life Prediction Algorithm"></a>3Feature Extraction Method and Remaining Useful Life Prediction Algorithm</h3><h4 id="3-1-ECA-CAE"><a href="#3-1-ECA-CAE" class="headerlink" title="3.1. ECA-CAE"></a>3.1. ECA-CAE</h4><p>In order to verify the effectiveness of the ECA-CAE method proposed in this chapter, the proposed method is compared with the ECA encoder MLP and the convolutional self-encoder CAE</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e70e759f345e8d038faa07.png" >
</div>

<p>why EAC？</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e874e19f345e8d0375e307.png" >
</div>

<h4 id="3-2-Remaining-Useful-Life-Prediction-Based-on-ECA-CAE-and-Autoformer-Model"><a href="#3-2-Remaining-Useful-Life-Prediction-Based-on-ECA-CAE-and-Autoformer-Model" class="headerlink" title="3.2. Remaining Useful Life Prediction Based on ECA-CAE and Autoformer Model"></a>3.2. Remaining Useful Life Prediction Based on ECA-CAE and Autoformer Model</h4><p>During the training of the network model, due to <strong>the difference in the failure criteria adopted by different bearings or the sampling time interval is too large,</strong> there is easily too much <strong>difference in the vibration signals leading to the threshold value cannot be unified</strong>; the trend difference problem is shown in Figure 6, if bearing 1,2 is used as training, the model is unable to learn any degradation process about bearing 3, and the RUL of bearing 3 is predicted on this basis, without deliberate tuning of the parameter optimization, the theoretical accuracy is not high.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e874e19f345e8d0375e3b6.png" >
</div>

<hr>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e70ed99f345e8d0390f5d0.png" >
</div>

<p>part1:</p>
<div align=center >
<img src="https://pic.imgdb.cn/item/65e70eda9f345e8d0390f6dd.png"  width= 750 height=350 >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/65e70ed99f345e8d0390f66b.png" >
</div>

<ul>
<li>in order to preserve the spatial information and minimize the loss of decoding, the global average pooling layer (GAP) is used outside the encoder to obtain 200×1 data. Using GAP instead of fully connected layer (FC) will reduce a lot of training time, reduce the spatial parameters to make the model more robust, and at the same time anti-overﬁtting eﬀect is bett</li>
<li>The 200 dimensional signals obtained by ECA-CAE have similar features and useless features with small variance, which need to do feature processing.</li>
</ul>
<h3 id="4-Experimental-Validation"><a href="#4-Experimental-Validation" class="headerlink" title="4. Experimental Validation"></a>4. Experimental Validation</h3><h4 id="4-1-Data-Sources"><a href="#4-1-Data-Sources" class="headerlink" title="4.1. Data Sources"></a>4.1. Data Sources</h4><p>XJTU-SY bearing dataset</p>
<h4 id="4-2-Data-Processing-and-Partitioning"><a href="#4-2-Data-Processing-and-Partitioning" class="headerlink" title="4.2. Data Processing and Partitioning"></a>4.2. Data Processing and Partitioning</h4><ul>
<li>A and B are the data without CAE processing, and C and D are the data after CAE processing, where B and D are part of the data of A and C respectively taken out separately for T-SNE.<br><strong>T-SNE</strong> processing, the purpose is to <strong>separate the fault stage and normal stage</strong>, the signal of normal stage is smooth, and it has no guiding meaning to the actual prediction.<blockquote>
<p>T-SNE 一种降维方法，在这里用来区分normal and fault stage</p>
</blockquote>
</li>
<li>Since the signal distribution of the normal operation phase of the bearing is irregular, the features related to the life time cannot be extracted, <strong>so the failure phase data is chosen as the training and prediction data.</strong></li>
</ul>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e70ed99f345e8d0390f47d.png" >
</div>

<ul>
<li>After feature screening, five features representing different degradation characteristics were finally obtained, and the correlation between the features and the correlation with time (time) were calculated as shown in Figure 12.</li>
</ul>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e7121d9f345e8d039c916c.png" >
</div>

<ul>
<li>If the resulting features are considered for lifetime prediction at this point, the required sequence length is not sufficient to support the training of the model, which leads to overfitting. At the same time, if the period information is added, the prediction accuracy of the sequences will be greatly increased. Therefore, a <strong>multi-dimensional sequence splicing</strong> approach is proposed as shown in Figure 13 below<br><strong>To achieve a smooth connection between the points of the sequence</strong>, the sequence is uniformly increased to a length of 9600 using <strong>cubic spline interpolation</strong>(三次样条插值).</li>
</ul>
<blockquote>
<p>三次样条插值法是一种常用的数值分析方法，可以通过给定的一组散点数据来拟合出一条光滑的连续函数曲线。其基本思想是用低次多项式逼近一段小区间内的数据，并利用这些多项式的连接处衔接条件来保证整个曲线的光滑性</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e7121d9f345e8d039c9220.png" >
</div>



</blockquote>
<h4 id="4-3-Timing-and-Remaining-Useful-Life-Prediction-The-above-mentioned-training-set-and-predict"><a href="#4-3-Timing-and-Remaining-Useful-Life-Prediction-The-above-mentioned-training-set-and-predict" class="headerlink" title="4.3. Timing and Remaining Useful Life Prediction The above-mentioned training set and predict"></a>4.3. Timing and Remaining Useful Life Prediction The above-mentioned training set and predict</h4><p>If the prediction is taken to the training set by fitting the double exponential model, the error exists between the fitted curve and the actual degradation curve because the degradation trend at the later stage is unknown, and the proposed method in this paper is the prediction of the degradation process, and the prediction results tend to be close to the real life.</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e7121d9f345e8d039c933f.png" >
</div>

<p><strong>对比实验</strong></p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e7121d9f345e8d039c93cc.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/65e7121d9f345e8d039c9417.png" >
</div>

<p><strong>泛用性</strong><br>taking working condition three as an example, the model parameters of bearing 1 signal processing and feature screening are retained for the prediction of bearing 2 and bearing 5. Since the degradation curve of each bearing is different, so each bearing does a separate Autoformer model training, and the prediction results obtained are shown in Table 10,</p>
<div align=center>
<img src="https://pic.imgdb.cn/item/65e961c89f345e8d039b3fb5.png" >
</div>

<div align=center>
<img src="https://pic.imgdb.cn/item/65e874e29f345e8d0375e75d.png" >
</div>
]]></content>
      <categories>
        <category>paper note</category>
      </categories>
  </entry>
</search>
